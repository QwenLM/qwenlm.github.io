[{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In recent months, our focus has been on developing a \u0026ldquo;good\u0026rdquo; model while optimizing the developer experience. As we progress towards \u0026lt;b\u0026gt;Qwen1.5\u0026lt;/b\u0026gt;, the next iteration in our Qwen series, this update arrives just before the Chinese New Year.\nWith Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, we\u0026rsquo;re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models.","title":"Introducing Qwen1.5"},{"summary":"Along with the rapid development of our large language model Qwen, we leveraged Qwenâ€™s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\nSubstantially boost in image-related reasoning capabilities; Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein; Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.","title":"Introducing Qwen-VL"},{"summary":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.\nPAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nAdditionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.","title":"Introducing Qwen"},{"summary":"2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.","title":"OFA: Towards Building a One-For-All Model"},{"summary":"Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.","title":"OFASys: Enabling Multitask Learning with One Line of Code! "},{"summary":"CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.","title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese"}]