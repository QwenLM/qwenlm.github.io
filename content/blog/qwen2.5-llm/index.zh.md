---
title: "Qwen2.5-LLM：扩展大型语言模型的边界"
date: 2024-09-19T00:00:03+08:00
weight: 1
# aliases: ["/first"]
# tags: ["Research"]
# draft: true
# comments: false
# description: "Desc Text."
# disable_share: false
# hide_meta: false
# hide_summary: false # to hide summary in list
# hide_footer: false
math: true
# search_hidden: false # to hide from search page
show_reading_time: true
show_bread_crumbs: true
show_post_nav_links: false # the prev/next after the content
show_code_copy_buttons: true
show_word_count: true
# use_hugo_toc: true
# show_toc: true
# toc_open: true # default expand all
# cover:
#     image: "path"
#     # can also paste direct link from external site
#     # ex. https://i.ibb.co/K0HVPBd/paper-mod-profilemode.png
#     alt: "<alt text>"
#     caption: "<text>"
#     relative: true # To use relative path for cover image, used in hugo Page-bundles
#     responsive_images: true
#     hidden: false
# header:
#   background: "" # background css value
#   background_image: ""
#   gradient: false
#   blur: false
---


{{< button href="https://github.com/QwenLM/Qwen2.5" label="GITHUB" external=true >}}
{{< button href="https://huggingface.co/Qwen" label="HUGGING FACE" external=true >}}
{{< button href="https://modelscope.cn/organization/qwen" label="MODELSCOPE" external=true >}}
{{< button href="https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct" label="DEMO" external=true >}}
{{< button href="https://discord.gg/yPEP2vHTu4" label="DISCORD" external=true >}}

# 简介

我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过[阿里云大模型服务平台](https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm)的API服务进行体验。

相比Qwen2系列，Qwen2.5带来了以下全新升级：

1) **全面开源**：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— **Qwen2.5-14B** 和 **Qwen2.5-32B**，以及一款适合移动端的 **Qwen2.5-3B**。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。

2) **更大规模、更高质量的预数据训练集**：我们的预训练数据集规模从 7T tokens 扩展到了 **18T** tokens。

3) **知识储备升级**：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 **74.2**，和从84.2提升到 **86.1**。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。

4) **代码能力增强**：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 **55.5**、**75.1** 和 **88.2**，优于Qwen2-72B-Instruct的32.2、69.2和80.2。

5) **数学能力提升**：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 **75.5/83.1**。

6) **更符合人类偏好**：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 **48.1** 大幅提升至 **81.2**，MT-Bench得分也从 **9.12** 提升到了 **9.35**，与之前的Qwen2-72B相比提升显著。

7) **其他核心能力提升**：Qwen2.5在 **指令跟随**、生成 **长文本**（从1K升级到 **8K tokens**）、理解 **结构化数据**（如表格），以及生成 **结构化输出**（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 **系统提示**，用户可以给模型设置 **特定角色** 或 **自定义条件**。



# 模型基础信息

本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2.0 开源许可协议，而 Qwen2.5-3B 和 Qwen2.5-72B 分别使用 Qwen Research 许可协议 和 Qwen 许可协议。

|  模型  | 参数量 | 非Embedding参数量 | 层数 | 头数 (KV) | Tie Embedding | 长下文长度 | 生成长度 | 许可协议 |
| :--------| :--------: | :--------: | :------: | :------------: | :-------: | :------------: | :-------: | :-------: |
| Qwen2.5-0.5B |  0.49B |  0.36B | 24 | 14 / 2 | Yes |  32K | 8K | Apache 2.0 |
| Qwen2.5-1.5B | 1.54B |  1.31B | 28 | 12 / 2 | Yes |  32K | 8K | Apache 2.0 |
| Qwen2.5-3B  |  3.09B |  2.77B | 36 | 16 / 2 | Yes |  32K | 8K | Qwen Research |
| Qwen2.5-7B | 7.61B |  6.53B | 28 | 28 / 4 | No |  128K | 8K | Apache 2.0 |
| Qwen2.5-14B | 14.7B |  13.1B | 48 | 40 / 8 | No |  128K  | 8K | Apache 2.0 |
| Qwen2.5-32B  |  32.5B |  31.0B | 64 | 40 / 8 | No |  128K | 8K | Apache 2.0 |
| Qwen2.5-72B  | 72.7B |  70.0B | 80 | 64 / 8 | No |  128K | 8K | Qwen |


# 模型表现

在这一部分，我们将通过大量的基准测试来评估 Qwen2.5 基础语言模型和指令调优模型的表现。

## Qwen2.5 基础语言模型评估

评估主要考察基础模型在自然语言理解、通用问答、代码、数学、科学知识、推理及多语言能力等方面的表现。

涉及的评估数据集包括：

**通用任务**：MMLU (5-shot)、MMLU-Pro (5-shot)、MMLU-redux (5-shot)、BBH (3-shot)、ARC-C (25-shot)、TruthfulQA (0-shot)、Winogrande (5-shot)、HellaSwag (10-shot)

**数学与科学任务**：GPQA (5-shot)、Theorem QA (5-shot)、GSM8K (4-shot)、MATH (4-shot)

**代码任务**：HumanEval (0-shot)、HumanEval+ (0-shot)、MBPP (0-shot)、MBPP+ (0-shot)、MultiPL-E (0-shot) (Python、C++、JAVA、PHP、TypeScript、C#、Bash、JavaScript)
  
**多语言任务**：Multi-Exam (M3Exam 5-shot、IndoMMLU 3-shot、ruMMLU 5-shot、mMMLU 5-shot)、Multi-Understanding (BELEBELE 5-shot、XCOPA 5-shot、XWinograd 5-shot、XStoryCloze 0-shot、PAWS-X 5-shot)、Multi-Mathematics (MGSM 8-shot)、Multi-Translation (Flores-101 5-shot)

### Qwen2.5-72B 表现

|  数据集  |  Llama-3-70B   |  Mixtral-8x22B  |   Llama-3-405B  | Qwen2-72B |  **Qwen2.5-72B** |
| :--------| :------------: | :------------: | :------------: |:---------:|:------------: |
| ***通用任务***  |   |   |    |     	     |		|  
|MMLU | 79.5 | 77.8 | 85.2 |   84.2    | **86.1** | 
|MMLU-Pro | 52.8 | 51.6 | **61.6** |   55.7    | 58.1 | 
|MMLU-redux |75.0 | 72.9 | - |   80.5    | **83.9** |
|BBH |81.0 | 78.9 | 85.9 |   82.4    | **86.3** | 
|ARC-C  |68.8 | 70.7 | - |   68.9    | **72.4** |
|TruthfulQA  |45.6 | 51.0 | - |   54.8    | **60.4** |
|WindoGrande  |85.3 | 85.0 | **86.7** |   85.1    | 83.9 |
|HellaSwag   |88.0 | **88.7** | - |   87.3    | 87.6 |
|  ***数学与科学任务*** |   |   |    |     	     | |
|GPQA |36.3|34.3|-|   37.4    |**45.9**|
|Theoremqa  |32.3|35.9|-| **42.8**  |42.4|
|MATH  | 42.5  | 41.7  | 53.8  |   50.9    | **62.1**  |
|MMLU-stem  | 73.7  | 71.7  | -     |   79.6    | **82.7**  |
|GSM8K | 77.6  | 83.7  | 89.0  |   89.0    | **91.5**  |
|   ***代码任务***  |   |   |    |     	     |	    |
| HumanEval  | 48.2  | 46.3  | **61.0** |   64.6    | 59.1  | 
| HumanEval+ | 42.1  | 40.2  | -    | **56.1**  | 51.2  | 
| MBPP       | 70.4  | 71.7  | 73.0 |   76.9    | **84.7**  |
| MBPP+      | 58.4  | 58.1  | -    |   63.9    | **69.2**  | 
| MultiPL-E  | 46.3  | 46.7  | -    |   59.6    | **60.5**  | 
|   ***多语言任务***  |   |   |    |     	     |	    |	 
| Multi-Exam           | 70.0  | 63.5  | - |   76.6    | **78.7**  | 
| Multi-Understanding | 79.9  | 77.7  | -  |   80.7    | **89.6**  | 
| Multi-Mathematics    | 67.1  | 62.9  |  - |   76.0    | **76.7**  |
| Multi-Translation    | 38.0  | 23.3  |  - |   37.8    | **39.0**  | 

Qwen2.5-72B 基础模型在各类任务上明显超过同类模型，以不到 1/5 的参数达到了与 Llama-3-405B 相当的表现。相比它的前身 Qwen2-72B，Qwen2.5-72B 几乎在所有基准评测上都有显著提升，尤其在通用任务、数学和代码竞赛中。


### Qwen2.5-14/32B 表现

|  数据集  |  Qwen1.5-32B  | Gemma2-27B  | Yi-1.5-34B  | Qwen2-57B-A14B  | **Qwen2.5-14B**  | **Qwen2.5-32B** |
| :--------| :------------: | :------------: | :------------: | :------------: |:------------: |:------------: |
| ***通用任务***  |   |   |    |     |     |   |
| MMLU       | 74.3  | 75.2  | 77.2  | 76.5  | 79.7 | **83.3** |
| MMLU-pro   | 44.1  | 49.1  | 48.3  | 43.0  | 51.2 | **55.1** |
| MMLU-redux | 69.0  | -     | 74.1  | 72.4  | 76.6 | **82.0** |
| BBH        | 66.8  | 74.9  | 76.4  | 67.0  | 78.2 |**84.5** |
| ARC-C      | 63.6  | **71.4**  | 65.6  | 64.1  |  67.3 | 70.4 |
| Truthfulqa | 57.4  | 40.1  | 53.9  | 57.7  | **58.4** | 57.8 |
| Winogrande | 81.5  | 59.7  | **84.9**  | 79.5  | - | 82.0 |
| Hellaswag  | 85.0  | **86.4**  | 85.9  | 85.2  | - | 85.2 |
|  ***数学与科学任务*** |   |   |    |     |     |   |
| GPQA      | 30.8  | 34.9  | 37.4  | 34.3  | 32.8  | **48.0** |
| Theoremqa | 28.8  | 35.8  | 40.0  | 33.5  | 43.0  | **44.1** |
| MATH      | 36.1  | 42.7  | 41.7  | 43.0  | 55.6  | **57.7** |
| MMLU-stem | 66.5  | 71.0  | 72.6  | 69.8  | 76.4  | **80.9** |
| GSM8K     | 78.5  | 81.1  | 81.7  | 80.7  | 90.2  | **92.9** |
|   ***代码任务***  |    |   |    |     |     |   |
| HumanEval  | 43.3  | 54.9  | 46.3  | 53.0  | 56.7  | **58.5** |
| HumanEval+ | 40.2  | 46.3  | 40.2  | 46.3  | 51.2  | **52.4** |
| MBPP       | 64.2  | 75.7  | 65.5  | 71.9  | 76.7  | **84.5** |
| MBPP+      | 53.9  | 60.2  | 55.4  | 57.4  | 63.2  | **67.2** |
| MultiPL-E  | 38.5  | 48.0  | 39.5  | 49.8  | 53.5  |**59.4** |
|   ***多语言任务*** |   |   |    |     |     |  |
| Multi-Exam           | 61.6  | 65.8  | 58.3  | 65.5  | 70.6 | **75.4** |
| Multi-Understanding | 76.5  | 82.2  | 73.9  | 77.0  | 85.9  |**88.4** |
| Multi-Mathematics    | 56.1  | 61.6  | 49.3  | 62.3  | 68.5  | **73.7** |
| Multi-Translation    | 33.5  | 38.7  | 30.0  | 34.5  | 36.2  |**37.3** |

Qwen2.5-14B 模型在多项任务中表现稳健，尤其是在像MMLU和BBH这样的通用任务上，分别取得了 79.7 分和 78.2 分，超越了许多规模更大的竞争对手。Qwen2.5-32B 表现尤为出色，甚至优于参数更大的同类模型。特别是在数学和代码等挑战性任务中，Qwen2.5-32B 大幅领先其前身 Qwen1.5-32B，在 MATH 中获得 57.7分，在MBPP中获得 84.5 分。

<!-- ### Qwen2.5-14B performance

|  数据集  | Qwen2.5-7B  | Qwen1.5-14B  |   Qwen1.5-32B  |  **Qwen2.5-14B**  |
| :--------| :------------: | :------------: | :------------: | :------------: |
| ***通用任务***  |   |   |    |  |
| MMLU       | 74.2 | 67.8 | 74.3 | **79.7** |
| MMLU-pro   | 45.0 | 37.2 | 44.1 | **51.2** |
| MMLU-redux | 71.1 | 65.4 | 69.0 | **76.6** |
| BBH        | 70.4 | 53.7 | 66.8 | **78.2** |
| ARC-C      | 63.7 | 56.3 | 63.6 | **67.3** |
| Trurhfulqa | 56.4 | 52.1 | 57.4 | **58.4** |
|  ***Mathematics & Scinece Tasks*** |   |   |    | |
| GPQA      | **36.4**  | 29.3  | 30.8 | 32.8  |
| Theoremqa | 36.0  | 23.3  | 28.8 | **43.0**  |
| MATH      | 49.8  | 44.3  | 36.1 | **55.6**  |
| MMLU-stem | 72.3  | 59.6  | 66.5 | **76.4**  |
| GSM8K     | 85.4  | 73.8  | 78.5 | **90.2**  |
|   ***代码任务***  |   |   |    |  |
| HumanEval  | **57.9**  | 42.1  | 43.3 | 56.7  |
| HumanEval+ | 50.6  | 37.2  | 40.2 | **51.2**  |
| MBPP       | 74.9  | 58.1  | 64.2 | **76.7**  |
| MBPP+      | 62.9  | 47.9  | 53.9 | **63.2**  |
| MultiPL-E  | 50.3  | 35.4  | 38.5 | **53.5**  |
|   ***多语言任务*** |   |   |    |  |
| Multi-Exam           | 59.4  | 55.7  |61.6  | **70.6**  |
| Multi-Understanding | 79.3  | 73.8  | 76.5  | **85.9**  |
| Multi-Mathematics    | 57.8  | 49.9  |56.1  | **68.5**  |
| Multi-Translation    | 32.4  | 32.4  | 33.5 | **36.2**  | -->



### Qwen2.5-7B 表现

|  数据集  | Mistral-7B  | Llama3-8B  | Gemma2-9B  | Qwen2-7B   | **Qwen2.5-7B** |
| :--------| :------------: | :------------: | :------------: | :------------: | :------------: |
|#Non-emb Params | 7.0B | 7.0B |  8.2B   | 6.5B| 6.5B|
| ***通用任务***  |   |   |    |     |  |
| MMLU       | 64.2  | 66.6  | 71.3  | 70.3  | **74.2** |
| MMLU-pro   | 30.9  | 35.4  | 44.7  | 40.1  | **45.0** |
| MMLU-redux | 58.1  | 61.6  | 67.9  | 68.1  | **71.1** |
| BBH        | 56.1  | 57.7  | 68.2  | 62.3  | **70.4** |
| ARC-C      | 60.0  | 59.3  | **68.2**  | 60.6  | 63.7 |
| Trurhfulqa | 42.2  | 44.0  | 45.3  | 54.2  | **56.4** |
| Winogrande | 78.4  | 77.4  | **79.5**  | 77.0  | 75.9 |
| Hellaswag  | **83.3**  | 82.1  | 81.9  | 80.7  | 80.2 |
|  ***数学与科学任务*** |   |   |    |     |  |
| GPQA      | 24.7  | 25.8  | 32.8  | 30.8  | **36.4** |
| Theoremqa | 19.2  | 22.1  | 28.9  | 29.6  | **36.0** |
| MATH      | 10.2  | 20.5  | 37.7  | 43.5  | **49.8** |
| MMLU-stem | 50.1  | 55.3  | 65.1  | 64.2  | **72.3** |
| GSM8K     | 36.2  | 55.3  | 70.7  | 80.2  | **85.4** |
|   ***代码任务***  |   |   |    |     |    |
| HumanEval  | 29.3  | 33.5  | 37.8  | 51.2  | **57.9** |
| HumanEval+ | 24.4  | 29.3  | 30.5  | 43.3  | **50.6** |
| MBPP       | 51.1  | 53.9  | 62.2  | 64.2  | **74.9** |
| MBPP+      | 40.9  | 44.4  | 50.6  | 51.9  | **62.9** |
| MultiPL-E  | 29.4  | 22.6  | 34.9  | 41.0  | **50.3** |
|   ***多语言任务*** |   |   |    |     |  |
| Multi-Exam           | 47.1  | 52.3  | **61.2**  | 59.2  | 59.4 |
| Multi-Understanding | 63.3  | 68.6  | 78.3  | 72.0  | **79.3** |
| Multi-Mathematics    | 26.3  | 36.3  | 53.0  | 57.5  | **57.8** |
| Multi-Translation    | 23.3  | 31.9  | **36.5**  | 31.5  | 32.4 |


Qwen2.5-7B在多个基准测试中超越了它的前代和同类竞争者。尽管它的非嵌入参数更少，但能够在各类任务中的表现更加出色。例如，Qwen2.5-7B 在 MMLU 通用基准测试中得分 74.2，在数学测试MATH中的得分为 49.8，而在代码任务HumanEval中取得了 57.9 分。



### Qwen2.5-0.5B/1.5B/3B 表现

|  数据集  | Qwen2-0.5B  | **Qwen2.5-0.5B** | Qwen2-1.5B  | **Qwen2.5-1.5B**  | Gemma2-2.6B  | **Qwen2.5-3B** |
| :--------| :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |
| ***通用任务***  |   |   |    |     |  | |
| MMLU       | 44.3  | 47.5  | 55.9  | 60.9  | 52.2  | **65.6** |
| MMLU-pro   | 14.7  | 15.7  | 21.6  | 28.5  | 23.0  | **34.6** |
| MMLU-redux | 40.7  | 45.1  | 51.8  | 58.5  | 50.9  | **63.7** |
| BBH        | 18.2  | 20.3  | 36.5  | 45.1  | 41.9  | **56.3** |
| ARC-C      | 31.0  | 35.6  | 43.7  | 54.7  | 55.7  | **56.5** |
| Trurhfulqa | 39.7  | 40.2  | 45.9  | 46.6  | 36.2  | **48.9** |
| Winogrande | 56.9  | 56.3  | 65.0  | 65.0  | **71.5**  | 71.1 |
| Hellaswag  | 49.1  | 52.1  | 67.0  | 67.9  | 74.6  | **74.6** |
|  ***数学与科学任务*** |   |   |    |     |  | |
| GPQA      | 29.8  | 24.8  | 20.7  | 24.2  | 25.3  | **26.3** |
| Theoremqa | 9.6   | 16.0  | 14.8  | 22.1  | 15.9  | **27.4** |
| MATH      | 11.2  | 19.5  | 21.6  | 35.0  | 18.3  | **42.6** |
| MMLU-stem | 27.5  | 39.8  | 42.7  | 54.8  | 45.8  | **62.5** |
| GSM8K     | 36.4  | 41.6  | 46.9  | 68.5  | 30.3  | **79.1** |
|   ***代码任务***  |   |   |    |     |    |  |
| HumanEval  | 22.6  | 30.5  | 34.8  | 37.2  | 19.5  | **42.1** |
| HumanEval+ | 18.9  | 26.8  | 29.9  | 32.9  | 15.9  | **36.0** |
| MBPP       | 33.1  | 39.3  | 46.9  | **60.2**  | 42.1  | 57.1 |
| MBPP+      | 27.6  | 33.8  | 37.6  | **49.6**  | 33.6  | 49.4 |
| MultiPL-E  | 16.3  | 18.9  | 27.9  | 33.1  | 17.6  | **41.2** |
|   ***多语言任务*** |   |   |    |     |  | |
| Multi-Exam           | 29.4  | 30.8  | 43.1  | 47.9  | 38.1  | **54.6** |
| Multi-Understanding | 40.4  | 41.0 | 50.7  |  65.1  | 46.8  | **76.6** |
| Multi-Mathematics    | 7.8   | 13.5   | 21.3  | 37.5  | 18.2  | **48.9** |
| Multi-Translation    | 14.1  | 15.3   | 23.8  |  25.0  | 26.9  | **29.3** |


对于移动端模型，Qwen2.5-0.5B、1.5B 和 3B 在几乎所有评测中都表现了强劲的性能。值得一提的是，Qwen2.5-0.5B 模型，在一些数学和编程任务中甚至超过了Gemma2-2.6B。


## 指令微调模型评估

评估主要考察指令微调模型在自然语言理解、通用问答、推理、代码、数学、指令遵循及人类对齐等方面的表现。

涉及的评估数据集包括：

**通用任务**：MMLU-Pro、MMLU-redux

**数学与科学任务**：GPQA、GSM8K、MATH

**代码任务**：HumanEval、MBPP、MultiPL-E、LiveCodeBench 2305-2409、LiveBench 0831

**指令和对齐任务**：IFeval strict-prompt、Arena-Hard、AlignBench v1.1、MTbench

### Qwen2.5-72B-Instruct 表现·

| 数据集 | Mistral-Large2 Instruct | Llama-3.1-70B-Instruct | Llama-3.1-405B-Instruct | Qwen2-72B-Instruct | **Qwen2.5-72B-Instruct** |
| --- | --- | --- | --- | --- | --- |
| MMLU-Pro | 69.4 | 66.4 | **73.3** | 64.4 | 71.1 |
| MMLU-redux | 83.0 | 83.0 | 86.2 | 81.6 | **86.8** |
| GPQA | **52.0** | 46.7 | 51.1 | 42.4 | 49.0 |
| MATH | 69.9 | 68.0 | 73.8 | 69.0 | **83.1** |
| GSM8K | 92.7 | 95.1 | **96.8** | 93.2 | 95.8 |
| HumanEval | **92.1** | 80.5 | 89.0 | 86.0 | 86.6 |
| MBPP | 80.0 | 84.2 | 84.5 | 80.2 | **88.2** |
| MultiPL-E | **76.9** | 68.2 | 73.5 | 69.2 | 75.1 |
| LiveCodeBench 2305-2409 | 42.2 | 32.1 | 41.6 | 32.2 | **55.5** |
| LiveBench 0831 | 48.5 | 46.6 | **53.2** | 41.5 | 52.3 |
| IFeval strict-prompt | 64.1 | 83.6 | **86.0** | 77.6 | 84.1 |
| Arena-Hard | 73.1 | 55.7 | 69.3 | 48.1 | **81.2** |
| AlignBench v1.1 | 7.69 | 5.94 | 5.95 | 8.15 | **8.16** |
| MTbench | 8.61 | 8.79 | 9.08 | 9.12 | **9.35** |

Qwen2.5-72B-Instruct 模型展现出了极为优异的表现，甚至在多个核心任务上超越了参数量巨大的 Llama-3.1-405B，在数学（MATH: 83.1）、代码（LiveCodeBench: 55.5）以及对话任务（Arena-Hard: 81.2）中表现尤为突出。与基础模型 Qwen2.5-72B 及前身 Qwen2-72B-Instruct 相比，Qwen2.5-72B-Instruct 在各项任务上的表现都有显著提升。

### Qwen2.5-Turbo & Qwen2.5-14B-Instruct & Qwen2.5-32B-Instruct 表现

| 数据集 | Qwen2-57B-A14B-Instruct | Gemma2-27B-IT | GPT4o-mini | **Qwen-Turbo** | **Qwen2.5-14B-Instruct** | **Qwen2.5-32B-Instruct** |
| --- | --- | --- | --- | --- | --- | --- |
| MMLU-Pro | 52.8 | 55.5 | 63.1 | 64.8 | 63.7 | **69.0** |
| MMLU-redux | 72.6 | 75.7 | 81.5 | 80.4 | 80.0 | **83.9** |
| GPQA | 34.3 | 38.4 | 40.2 | 44.4 | 45.5 | **49.5** |
| MATH | 49.1 | 54.4 | 70.2 | 81.0 | 80.0 | **83.1** |
| GSM8K | 85.3 | 90.4 | 93.2 | 93.6 | 94.8 | **95.9** |
| HumanEval | 79.9 | 78.7 | **88.4** | 86.6 | 83.5 | **88.4** |
| MBPP | 70.9 | 81.0 | **85.7** | 80.2 | 82.0 | 84.0 |
| MultiPL-E | 66.4 | 67.4 | 75.0 | 73.0 | 72.8 | **75.4** |
| LiveCodeBench 2305-2409 | 22.5 | - | 40.7 | 43.1 | 42.6 | **51.2** |
| LiveBench 0831 | 31.1 | 39.6 | 43.3 | 41.6 | 44.4 | **50.7** |
| IFeval strict-prompt | 59.9 | 77.1 | 80.4 | 74.9 | **81.0** | 79.5 |
| Arena-Hard | 17.8 | 57.5 | **74.9** | 68.4 | 68.3 | 74.5 |
| AlignBench v1.1 | 7.02 | 7.22 | 7.81 | **7.99** | 7.94 | 7.93 |
| MTbench | 8.55 | 9.10 | - | 8.86 | 8.88 | **9.20** |

Qwen2.5-32B-Instruct 在大多数任务中表现优于同类规模的模型。与 GPT-4o-mini 相比，我们的开源模型 Qwen2.5-14B-Instruct 与 API 模型 Qwen-Turbo也在所有任务都中展现出了相当的竞争力。


### Qwen2.5-7B-Instruct 表现

| 数据集 | Gemma2-9b-IT | Llama3.1-8B-Instruct | Qwen2-7B-Instruct | **Qwen2.5-7B-Instruct** |
| --- | --- | --- | --- | --- |
| MMLU-Pro | 52.1 | 48.3 | 44.1 | **56.3** |
| MMLU-redux | 72.8 | 67.2 | 67.3 | **75.4** |
| GPQA | 32.8 | 32.8 | 34.3 | **36.4** |
| MATH | 44.3 | 51.9 | 52.9 | **75.5** |
| GSM8K | 76.7 | 84.5 | 85.7 | **91.6** |
| HumanEval | 68.9 | 72.6 | 79.9 | **84.8** |
| MBPP | 74.9 | 69.6 | 67.2 | **79.2** |
| MultiPL-E | 53.4 | 50.7 | 59.1 | **70.4** |
| LiveCodeBench 2305-2409 | 18.9 | 8.3 | 23.9 | **28.7** |
| LiveBench 0831 | 30.6 | 26.7 | 29.2 | **35.9** |
| IFeval strict-prompt | 70.1 | **75.9** | 54.7 | 71.2 |
| Arena-Hard | 41.6 | 27.8 | 25.0 | **52.0** |
| AlignBench v1.1 | 7.05 | 4.75 | 7.13 | **7.33** |
| MTbench | 8.49 | 8.23 | 8.26 | **8.75** |

Qwen2.5-7B-Instruct 在除了 IFeval 的所有任务中表现均优于竞争对手 Gemma2-9b-IT 和 Llama3.1-8B-Instruct，尤其是在数学（MATH: 75.5）和代码（HumanEval: 84.8）任务上优势明显。



### Qwen2.5-3B-Instruct 表现

| 数据集 | Gemma2-2B-IT | Phi3.5-mini-Instruct | MiniCPM3-4B | **Qwen2.5-3B-Instruct** |
| --- |-------------| --- | --- | --- |
| Non-Emb Params | 2.0B        | 3.6B | 4.0B | 2.8B |
| MMLU-Pro | 26.7        | **47.5** | 43.0 | 43.7 |
| MMLU-redux | 51.9        | **67.7** | 59.9 | 64.4 |
| GPQA | 29.3        | 27.2 | **31.3** | 30.3 |
| MATH | 26.6        | 48.5 | 46.6 | **65.9** |
| GSM8K | 63.2        | 86.2 | 81.1 | **86.7** |
| HumanEval | 68.9        | 72.6 | **74.4** | **74.4** |
| MBPP | **74.9**    | 63.2 | 72.5 | 72.7 |
| MultiPL-E | 30.5        | 47.2 | 49.1 | **60.2** |
| LiveCodeBench 2305-2409 | 5.8         | 15.8 | **23.8** | 19.9 |
| LiveBench 0831 | 20.1        | 27.4 | **27.6** | 26.8 |
| IFeval strict-prompt | 51.0        | 52.1 | **68.4** | 58.2 |

在适用移动端的指令模型中，Qwen2.5-3B-Instruct 的参数量虽然少于 Phi3.5-mini-Instruct 和 MiniCPM3-4B，但在数学和编程任务上仍然具有优势，同时在语言理解方面也展现出不错的实力。


### Qwen2.5-0.5B/1.5B-Instruct 表现

| 数据集 | Qwen2-0.5B-Instruct | **Qwen2.5-0.5B-Instruct** | Qwen2-1.5B-Instruct | **Qwen2.5-1.5B-Instruct** |
| --- | --- | --- |--------------------|--------------------------|
| MMLU-Pro | 14.4 | **15.0** | 22.9               | **32.4**                 |
| MMLU-redux | 12.9 | **24.1** | 41.2               | **50.7**                 |
| GPQA | 23.7 | **29.8** | 21.2               | **29.8**                 |
| MATH | 13.9 | **34.4** | 25.3               | **55.2**                 |
| GSM8K | 40.1 | **49.6** | 61.6               | **73.2**                 |
| HumanEval | 31.1 | **35.4** | 42.1               | **61.6**                 |
| MBPP | 39.7 | **49.6** | 44.2               | **63.2**                 |
| MultiPL-E | 20.8 | **28.5** | 38.5               | **50.4**                 |
| LiveCodeBench 2305-2409 | 1.6 | **5.1** | 4.5                | **14.8**                 |
| LiveBench 0831 | 7.4 | **12.6** | 12.4               | **18.8**                 |
| IFeval strict-prompt | 14.6 | **27.9** | 29.0               | **42.5**                 |

Qwen2.5-1.5B-Instruct 及 Qwen2.5-0.5B-Instruct 的性能相比前代大幅提升，使它们尤其适合在资源极度受限的端侧场景下应用。


### 多语言表现

为了更好地评估指令微调模型的多语言表现，我们收集并扩展了以下基准测试：

- **IFEval（多语言）**：我们将IFEval进行翻译，构建了多语言版本的 IFEval。在此过程中，我们移除了语言特定（例如“以字母A开头”）的测试用例。每种语言我们都收集了100个测试用例，包括阿拉伯语（ar）、西班牙语（es）、法语（fr）、印尼语（in）、日语（ja）、韩语（ko）、葡萄牙语（pt）和越南语（vi）。所有用例都由付费标注人员进行检查，并在必要时进行修改。

- **知识能力测试**：我们选用了五个类似 MMLU 的多选题基准测试来验证 Qwen2.5 系列模型的多语言知识掌握情况，包括：AMMLU（阿拉伯语）、JMMLU（日语）、KMMLU（韩语）、IndoMMLU（印尼语）和 TurkishMMLU（土耳其语）。此外，我们还展示了翻译版MMLU（即 okapi_MMLU，将英文MMLU翻译为多种语言）的性能表现。

- **MGSM8K（扩展版）**：在原版 MGSM8K 包含的语言外，我们还增加了阿拉伯语（ar）、韩语（ko）、葡萄牙语（pt）和越南语（vi）的支持。我们将 250 个测试用例翻译成这四种语言，保持与其他 MGSM8K 支持语言测试数量一致。所有示例也由付费标注人员进行了检查和必要的修改。

- **文化差异**：我们还使用了 BLEnD 基准测试，旨在评估大模型对于文化差异的处理能力，以进一步验证 Qwen2.5 系列模型的表现。

| 数据集 | Qwen2-72B-Instruct | Llama3.1-70B-Instruct | Qwen2.5-32B-Instruct | Mistral-Large-Instruct-2407 (123B) | GPT4o-mini | Qwen2.5-72B-Instruct |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ***指令遵循任务*** |    |    |    |    |    |    |
| IFEval（多语言） | 79.69 | 80.47 | 82.68 | 82.69 | 85.03 | **86.98** |
| ***知识任务*** |    |    |    |    |    |    |
| AMMLU（阿拉伯语） | 68.85 | 70.08 | 70.44 | 69.24 | 69.73 | **72.44** |
| JMMLU（日语） | 77.37 | 73.89 | 76.55 | 75.77 | 73.74 | **80.56** |
| KMMLU（韩语） | 57.04 | 53.23 | 60.75 | 56.42 | 56.77 | **61.96** |
| IndoMMLU（印尼语） | 66.31 | 67.50 | 66.42 | 63.21 | 67.75 | **69.25** |
| TurkishMMLU（土耳其语） | 69.22 | 66.89 | 72.41 | 64.78 | 71.19 | **76.12** |
| okapi MMLU（翻译） | 77.84 | 76.49 | 77.16 | 78.37 | 73.44 | **79.97** |
| ***数学任务*** |    |    |    |    |    |    |
| MGSM8K（扩展版） | 82.72 | 73.31 | 87.15 | **89.01** | 87.36 | 88.16 |
| ***文化差异任务*** |    |    |    |    |    |    |
| BLEnD | 25.90 | 30.49 | 27.88 | 33.47 | **35.91** | 32.48 |



| 数据集 | Qwen2-7B-Instruct | Llama3.1-8B-Instruct | Qwen2.5-7B-Instruct | Gemma-2-9B-Instruct | Mistral-Nemo-Instruct-2407 (12B) | Qwen2.5-14B-Instruct |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ***指令遵循任务*** |    |    |    |    |    |    |
| IFEval（多语言） | 51.43 | 60.68 | 74.87 | **77.47** | 64.59 | 77.08 |
| ***知识任务*** |    |    |    |    |    |    |
| AMMLU（阿拉伯语） | 54.87 | 54.28 | 59.78 | 60.26 | 53.92 | **66.81** |
| JMMLU（日语） | 57.71 | 53.26 | 61.88 | 64.59 | 55.17 | **72.78** |
| KMMLU（韩语） | 43.96 | 42.28 | 46.59 | 46.24 | 42.22 | **59.71** |
| IndoMMLU（印尼语） | 54.05 | 53.92 | 56.42 | 61.73 | 50.76 | **65.09** |
| TurkishMMLU（土耳其语） | 49.27 | 45.61 | 54.28 | 55.44 | 34.44 | **66.85** |
| okapi MMLU（翻译） | 60.47 | 55.18 | 66.98 | 46.72 | 59.65 | **72.12** |
| ***数学任务*** |    |    |    |    |    |    |
| MGSM8K（扩展版） | 56.13 | 66.05 | 66.11 | 78.37 | 54.75 | **82.27** |
| ***文化差异任务*** |    |    |    |    |    |    |
| BLEnD | 22.49 | 19.47 | 23.66 | **28.31** | 26.61 | 26.99 |


# 实例演示

我们准备了一些实例演示，来体现 Qwen2.5 的新特性和改进之处，涵盖了生成JSON格式输出、撰写长篇内容以及理解结构化数据等能力。


{{< fullwidth class="example-container" >}}
{{< example data="cases/1_1.json" hide=false next=true >}}
{{< example data="cases/1_2.json" hide=true next=true >}}
{{< example data="cases/1_3.json" hide=true next=true >}}
{{< /fullwidth >}}
