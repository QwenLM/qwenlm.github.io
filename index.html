<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.106.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen</title><meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Qwickly forging AGI, enhancing intelligence."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/json href=https://qwenlm.github.io/index.json><link rel=alternate hreflang=en href=https://qwenlm.github.io/><link rel=alternate hreflang=zh href><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen"><meta property="og:description" content="Qwickly forging AGI, enhancing intelligence."><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen"><meta name=twitter:description content="Qwickly forging AGI, enhancing intelligence."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Qwen","url":"https://qwenlm.github.io/","description":"Qwen","thumbnailUrl":"https://qwenlm.github.io/favicon.png","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><img class=hero-background style=opacity:0 onload="this.style.opacity=1" src=https://cdn.jsdelivr.net/gh/qwenlm/qwenlm.github.io@master/static/img/background.webp width=100%><div class=hero-gradient></div><div class=hero-blur></div><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-light text-fade-in"><div class=hero-header><svg width="280" height="100" style="filter:drop-shadow(0 0 32px #4f2dda)" viewBox="0 0 66.067 20.526" xmlns="http://www.w3.org/2000/svg"><path d="m8.7933 13.184h3.3982l1.4263 1.6743q1.3146-1.0914 2.0216-2.6417.71934-1.5503.71934-3.3238.0-2.8401-1.7487-4.5765-1.7363-1.7487-4.6013-1.7487-1.4387.0-2.6913.5209-1.2526.5085-2.2324 1.5131-1.0914 1.1286-1.6743 2.5549-.58291 1.4263-.58291 2.9766.0 2.8029 1.7115 4.5889 1.7115 1.7859 4.3532 1.7859.5457.0 1.1286-.08682.59531-.08682 1.2402-.27285zm6.2012 7.3422-1.9844-2.2944q-1.0542.40928-2.034.60772-.97978.21084-1.9348.21084-4.1548.0-6.598-2.3812t-2.4433-6.4368q0-2.2076.78135-4.13.78135-1.9348 2.2448-3.3362 1.4139-1.3519 3.237-2.0588 1.8355-.70693 3.9315-.70693 4.0804.0 6.536 2.4061 2.4557 2.4061 2.4557 6.412.0 2.4681-.99219 4.5765t-2.8277 3.5223l3.0262 3.6091zm5.3578-13.692h2.6913l1.91 6.0027q.0124.03721.04961.14883.34726 1.1038.37207 1.6371.13643-.42168.34727-.89297.22324-.48369.5457-1.0294l3.9439-6.7345 1.9224 6.9329q.11162.38447.18604.79375.07441.40928.12402.94258.18604-.5209.37207-.94258.19844-.42168.38447-.73174l3.5595-6.1268h3.0262l-7.9747 12.489-1.9844-6.5732q-.11162-.34727-.19844-.74414-.07441-.40928-.13643-.90537-.27285.57051-.48369.99219t-.32246.59531l-3.9936 6.6353zm22.758 4.4648h6.0647q-.03721-1.1906-.74414-1.8728-.69453-.69453-1.8728-.69453-1.3395.0-2.2696.69453t-1.1782 1.8728zm5.9159 3.6835 1.9224 1.5007q-1.0294 1.3519-2.2324 1.972-1.203.62012-2.7657.62012-2.6169.0-4.2292-1.5751-1.6123-1.5875-1.6123-4.1672.0-3.0262 1.8479-4.9609 1.8604-1.9472 4.7253-1.9472 2.3937.0 3.8075 1.4883 1.4263 1.4759 1.4263 3.9688.0.21084-.0248.5457-.0124.32246-.04961.78135h-8.9669q0 1.5999.80615 2.5549.80615.94258 2.1456.94258.93018.0 1.7611-.44648.84336-.45889 1.4387-1.2774zm13.357 3.6091.89297-6.7593q.0248-.18604.03721-.38447.0124-.21084.0124-.60772.0-1.1534-.5581-1.7611-.55811-.62012-1.6247-.62012-1.6619.0-2.6045 1.079-.94258 1.0666-1.2278 3.2866l-.74414 5.7671h-2.6665l1.5503-11.757h2.5673l-.19844 1.4139q1.017-.93018 2.096-1.3767 1.0914-.44648 2.3192-.44648 1.8107.0 2.8153.95498 1.017.94258 1.017 2.6541.0.43408-.04961 1.017-.04961.57051-.14883 1.3395l-.81855 6.2012z" fill="#fff" stroke-linejoin="round" stroke-opacity=".7352" stroke-width="1.6"/></svg></div><div class=hero-content>Qwickly forging AGI, enhancing intelligence.</div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Time to Speak Some Dialects, Qwen-TTS!</h2></header><div class=entry-content><p>API DISCORD
Introduction Here we introduce the latest update of Qwen-TTS (qwen-tts-latest or qwen-tts-2025-05-22) through Qwen API . Trained on a large-scale dataset encompassing over millions of hours of speech, Qwen-TTS achieves human-level naturalness and expressiveness. Notably, Qwen-TTS automatically adjusts prosody, pacing, and emotional inflections in response to the input text. Notably, Qwen-TTS supports the generation of 3 Chinese dialects, including Pekingese, Shanghainese, and Sichuanese.
As of now, Qwen-TTS supports 7 Chinese-English bilingual voices, including Cherry, Ethan, Chelsie, Serena, Dylan (Pekingese), Jada (Shanghainese) and Sunny (Sichuanese)....</p></div><footer class=entry-footer><span title='2025-06-27 15:01:30 +0800 +0800'>June 27, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;537 words&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Time to Speak Some Dialects, Qwen-TTS!" href=https://qwenlm.github.io/blog/qwen-tts/></a></article><article class=post-entry><header class=entry-header><h2>Qwen VLo: From "Understanding" the World to "Depicting" It</h2></header><div class=entry-content><p>QWEN CHAT DISCORD
Introduction The evolution of multimodal large models is continually pushing the boundaries of what we believe technology can achieve. From the initial QwenVL to the latest Qwen2.5 VL, we have made progress in enhancing the model’s ability to understand image content. Today, we are excited to introduce a new model, Qwen VLo, a unified multimodal understanding and generation model. This newly upgraded model not only “understands” the world but also generates high-quality recreations based on that understanding, truly bridging the gap between perception and creation....</p></div><footer class=entry-footer><span title='2025-06-26 22:00:04 +0800 +0800'>June 26, 2025</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;3250 words&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label='post link to Qwen VLo: From "Understanding" the World to "Depicting" It' href=https://qwenlm.github.io/blog/qwen-vlo/></a></article><article class=post-entry><header class=entry-header><h2>Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
We release Qwen3 Embedding series, a new proprietary model of the Qwen model family. These models are specifically designed for text embedding, retrieval, and reranking tasks, built on the Qwen3 foundation model. Leveraging Qwen3’s robust multilingual text understanding capabilities, the series achieves state-of-the-art performance across multiple benchmarks for text embedding and reranking tasks. We have open-sourced this series of text embedding and reranking models under the Apache 2....</p></div><footer class=entry-footer><span title='2025-06-05 21:00:00 +0800 +0800'>June 5, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;798 words&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models" href=https://qwenlm.github.io/blog/qwen3-embedding/></a></article><article class=post-entry><header class=entry-header><h2>Qwen3: Think Deeper, Act Faster</h2></header><div class=entry-content><p>QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD
Introduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2....</p></div><footer class=entry-footer><span title='2025-04-29 04:00:00 +0800 +0800'>April 29, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2036 words&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen3: Think Deeper, Act Faster" href=https://qwenlm.github.io/blog/qwen3/></a></article><article class=post-entry><header class=entry-header><h2>QVQ-Max: Think with Evidence</h2></header><div class=entry-content><p>QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only “understand” the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities....</p></div><footer class=entry-footer><span title='2025-03-28 00:00:04 +0800 +0800'>March 28, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;829 words&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to QVQ-Max: Think with Evidence" href=https://qwenlm.github.io/blog/qvq-max-preview/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://qwenlm.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>