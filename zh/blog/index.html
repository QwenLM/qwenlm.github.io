<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.25451dd4678157e0fb2e84a2fba5ad7861ab458e1168319a052575d04324b785.css integrity="sha256-JUUd1GeBV+D7LoSi+6WteGGrRY4RaDGaBSV10EMkt4U=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Qwen2.5-VL-32B: 更聪明、更轻量!</h2></header><div class=entry-content><p>QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
介绍 今年一月底，我们推出了 Qwen2.5-VL 系列模型，获得了社区的广泛关注和积极反馈。在 Qwen2.5-VL 系列的基础上，我们使用强化学习持续优化模型，并使用 Apache 2.0 协议开源 32B 这个备受喜爱的参数规模的新 VL 模型—— Qwen2.5-VL-32B-Instruct。相比此前发布的 Qwen2.5-VL 系列模型，本次推出的 32B 模型的特点如下：
回复更符合人类主观偏好：调整了输出风格，使回答更加详细、格式更规范，并更符合人类偏好。 数学推理能力：复杂数学问题求解的准确性显著提升。 图像细粒度理解与推理：在图像解析、内容识别以及视觉逻辑推导等任务中表现出更强的准确性和细粒度分析能力。 性能表现 我们与业内先进的同规模模型进行比较，包括近期推出的 Mistral-Small-3.1-24B 和 Gemma-3-27B-IT， Qwen2.5-VL-32B-Instruct 展现出了明显的优势，甚至超越了更大规模的 Qwen2-VL-72B-Instruct 模型。尤其是在多模态任务中，例如 MMMU、MMMU-Pro 和 MathVista，这些任务强调复杂的多步骤推理，Qwen2.5-VL-32B-Instruct 表现尤为突出。在注重主观用户体验评估的 MM-MT-Bench 基准测试中，该模型相较于其前代 Qwen2-VL-72B-Instruct 取得了显著进步。
除了在视觉能力上优秀，Qwen2.5-VL-32B-Instruct 在纯文本能力上也达到了同规模的最优表现。
样例 Fine-grained Image Understanding and Reasoning
Next
User
I am driving a large truck on this road, and it is now 12 o’clock. Can I reach a place 110 kilometers away before 13 o’clock?...</p></div><footer class=entry-footer><span title='2025-03-24 00:00:04 +0800 +0800'>2025年3月24日</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;1794 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-VL-32B: 更聪明、更轻量!" href=https://qwenlm.github.io/zh/blog/qwen2.5-vl-32b/></a></article><article class=post-entry><header class=entry-header><h2>QwQ-32B: 领略强化学习之力</h2></header><div class=entry-content><p>QWEN CHAT Hugging Face ModelScope DEMO DISCORD
大规模强化学习（RL）有潜力超越传统的预训练和后训练方法来提升模型性能。近期的研究表明，强化学习可以显著提高模型的推理能力。例如，DeepSeek R1 通过整合冷启动数据和多阶段训练，实现了最先进的性能，使其能够进行深度思考和复杂推理。这一次，我们探讨了大规模强化学习（RL）对大语言模型的智能的提升作用，同时很高兴推出我们最新的推理模型 QwQ-32B。这是一款拥有 320 亿参数的模型，其性能可与具备 6710 亿参数（其中 370 亿被激活）的 DeepSeek-R1 媲美。这一成果突显了将强化学习应用于经过大规模预训练的强大基础模型的有效性。此外，我们还在推理模型中集成了与 Agent 相关的能力，使其能够在使用工具的同时进行批判性思考，并根据环境反馈调整推理过程。我们希望我们的一点努力能够证明强大的基础模型叠加大规模强化学习也许是一条通往通用人工智能的可行之路。
QwQ-32B 已在 Hugging Face 和 ModelScope 开源，采用了 Apache 2.0 开源协议。大家可通过 Qwen Chat 直接进行体验！
模型效果 QwQ-32B 在一系列基准测试中进行了评估，测试了数学推理、编程能力和通用能力。以下结果展示了 QwQ-32B 与其他领先模型的性能对比，包括 DeepSeek-R1-Distilled-Qwen-32B、DeepSeek-R1-Distilled-Llama-70B、o1-mini 以及原始的 DeepSeek-R1。
强化学习 我们在冷启动的基础上开展了大规模强化学习。在初始阶段，我们特别针对数学和编程任务进行了 RL 训练。与依赖传统的奖励模型（reward model）不同，我们通过校验生成答案的正确性来为数学问题提供反馈，并通过代码执行服务器评估生成的代码是否成功通过测试用例来提供代码的反馈。随着训练轮次的推进，这两个领域中的性能均表现出持续的提升。在第一阶段的 RL 过后，我们增加了另一个针对通用能力的 RL。此阶段使用通用奖励模型和一些基于规则的验证器进行训练。我们发现，通过少量步骤的通用 RL，可以提升其他通用能力，同时在数学和编程任务上的性能没有显著下降。
API 以下我们展示了一段简短的示例代码，说明如何通过 API 使用 QwQ-32B。
from openai import OpenAI import os # Initialize OpenAI client client = OpenAI( # If the environment variable is not configured, replace with your API Key: api_key="sk-xxx" # How to get an API Key：https://help....</p></div><footer class=entry-footer><span title='2025-03-06 00:00:04 +0800 +0800'>2025年3月6日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;227 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to QwQ-32B: 领略强化学习之力" href=https://qwenlm.github.io/zh/blog/qwq-32b/></a></article><article class=post-entry><header class=entry-header><h2>&lt;think>...&lt;/think> QwQ-Max-Preview</h2></header><div class=entry-content><p>QWEN CHAT DISCORD
这篇博客出自 QwQ-Max-Preview 之手。希望各位看官喜欢！
介绍 &lt;think>
好的，用户希望我为他们的博客创建一个标题和介绍，宣布 QwQ-Max-Preview 的发布。让我先整理一下他们提到的关键点。 首先，该模型属于通义千问系列，基于 Qwen2.5-Max 构建。这是一个预览版本，因此他们可能希望强调这是正式发布前的抢先体验。主要优势包括深度推理、数学、编程、通用领域以及与 Agent 相关的任务。他们还提到了未来的更新计划，并将开源代码，采用 Apache 2.0 许可证。 对于标题，我需要让它吸引人并包含模型名称。也许可以这样写：“隆重推出 QwQ-Max-Preview ：深度推理与多领域精通的全新飞跃”。这突出了预览状态和关键功能。 介绍部分应简要总结公告内容。以对发布的兴奋之情开头，提到它基于 Qwen2.5-Max ，强调其在推理、数学、编程和 Agent 任务方面的强大能力。同时指出这是一个预览版本，未来会有更新，并最终开源。保持简洁，不展开解释，按照用户要求避免使用 Markdown，段落也要简短。最后检查是否涵盖了所有要点：模型名称、基于 Qwen2.5、预览状态、推理/数学/编程/Agent 任务的优势、未来更新、以及 Apache 2.0 开源许可。是的，这些都包括了。好了，现在开始整合内容。 &lt;/think>
我们很高兴向大家介绍 QwQ-Max-Preview，这是 Qwen 系列的最新成果。这一版本基于 Qwen2.5-Max 构建，在数学、编程以及通用任务中展现了更强的能力，同时在与 Agent 相关的工作流中也有不错的表现。作为即将发布的 QwQ-Max 的预览版，这个版本还在持续优化中。我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max。我们期待与大家一起探索智能推理的更多可能性！ 示例 Coding
Next
QwQ-Max-Preview
Game
Next
QwQ-Max-Preview
Game
Next
QwQ-Max-Preview
Agent
Next
QwQ-Max-Preview
Agent
Next
QwQ-Max-Preview
Math
Next
QwQ-Max-Preview
Creative Writing
Next
QwQ-Max-Preview
Search
Next
QwQ-Max-Preview
接下来的工作 &lt;think>...</p></div><footer class=entry-footer><span title='2025-02-25 02:00:04 +0800 +0800'>2025年2月25日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;133 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to ... QwQ-Max-Preview" href=https://qwenlm.github.io/zh/blog/qwq-max-preview/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Max：探索大规模 MoE 模型的智能</h2></header><div class=entry-content><p>QWEN CHAT API DEMO DISCORD
过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！
性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。
首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。
在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。
在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。
我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2....</p></div><footer class=entry-footer><span title='2025-01-28 23:00:04 +0800 +0800'>2025年1月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;162 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Max：探索大规模 MoE 模型的智能" href=https://qwenlm.github.io/zh/blog/qwen2.5-max/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型</h2></header><div class=entry-content><p>Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：
开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。
推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。
技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。
现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。
另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。
模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。
长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。...</p></div><footer class=entry-footer><span title='2025-01-27 00:00:03 +0800 +0800'>2025年1月27日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;501 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型" href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://qwenlm.github.io/zh/blog/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>