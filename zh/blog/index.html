<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.0ba7b7b31d6f65f4630a254e3d5ed3b35b42e349fc949c2df73c456e864b8350.css integrity="sha256-C6e3sx1vZfRjCiVOPV7Ts1tC40n8lJwt9zxFboZLg1A=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=http://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=http://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.f20a5212619392e989b6d24ad9ce42302014debfad4d3c8c01db030c36d03475.js integrity="sha256-8gpSEmGTkumJttJK2c5CMCAU3r+tTTyMAdsDDDbQNHU="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="http://qwenlm.github.io/zh/blog/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=http://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Qwen1.5 介绍</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。在农历新年即将到来之际，我们满怀欣喜地与大家分享新一代通义千问模型: Qwen1.5。
在此次Qwen1.5版本中，我们开源了包括0.5B、1.8B、4B、7B、14B和72B在内的6个不同规模的Base和Chat模型，并一如既往地放出了各规模对应的量化模型。此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到Hugging Face transformers代码库中，所以现在可以直接使用 transformers>=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46....</p></div><footer class=entry-footer><span title='2024-02-04 13:33:00 +0800 +0800'>2024年2月4日</span>&nbsp;·&nbsp;6 分钟&nbsp;·&nbsp;1231 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen1.5 介绍" href=http://qwenlm.github.io/zh/blog/qwen1.5/></a></article><article class=post-entry><header class=entry-header><h2>Qwen-VL全新升级！</h2></header><div class=entry-content><p>我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79....</p></div><footer class=entry-footer><span title='2024-01-25 13:33:00 +0800 +0800'>2024年1月25日</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;1715 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen-VL全新升级！" href=http://qwenlm.github.io/zh/blog/qwen-vl/></a></article><article class=post-entry><header class=entry-header><h2>Qwen介绍</h2></header><div class=entry-content><p>四个月前，我们首次发布Qwen-7B大型语言模型（LLM），正式开启了我们的开源之旅。今天，我们介绍Qwen开源家族，更全面的展示我们的工作和目标。下面是开源项目和社区的重要链接。
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.
总览 整体上，Qwen不仅仅是一个语言模型，而是一个致力于实现通用人工智能（AGI）的项目，目前包含了大型语言模型（LLM）和大型多模态模型（LMM）。下图展示了Qwen的主要组成部分:
在这里，“Qwen” 指的是基础语言模型，而 “Qwen-Chat” 则指的是通过后训练技术如SFT（有监督微调）和RLHF（强化学习人类反馈）训练的聊天模型。我们还有提供了专门针对特定领域和任务的模型，例如用于编程的 “Code-Qwen” 和用于数学的 “Math-Qwen”。大型语言模型（LLM）可以通过模态对齐扩展到多模态，因此我们有视觉-语言模型 “Qwen-VL” 以及音频-语言模型 “Qwen-Audio” 。值得注意的是，本篇博客仅介绍语言模型，至于多模态模型（LMM），例如Qwen-VL和Qwen-Audio，请参阅其各自的博客。
基础模型：对齐的良好起点 构建助手模型的一般流程包括预训练和后训练，后者主要由SFT（有监督微调）和RLHF（强化学习人类反馈）组成。至于预训练，与之前的大语言模型GPT-3、Llama类似，Qwen是一个基于Transformer的语言模型，通过预测下一个词的任务进行预训练。为了简化和稳定性，我们没有为语言模型引入更多的任务，而是专注于模型规模的扩展和数据的扩展。目前，我们已经开发了5种不同大小的模型，其中4种已开源，包括 1.8B、Qwen-7B、Qwen-14B和Qwen-72B。
Model Release Date Max Length System Prompt Enhancement # of Pretrained Tokens Minimum GPU Memory Usage of Finetuning (Q-Lora) Minimum GPU Usage of Generating 2048 Tokens (Int4) Tool Usage Qwen-1....</p></div><footer class=entry-footer><span title='2024-01-23 22:13:29 +0800 +0800'>2024年1月23日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;154 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen介绍" href=http://qwenlm.github.io/zh/blog/qwen/></a></article><article class=post-entry><header class=entry-header><h2>OFA：走向通用统一模型</h2></header><div class=entry-content><p>2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。
论文 GitHub ModelScope 体验
背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。
方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。
统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。
模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。
多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。
我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。
我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。
实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。
此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。
单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。
另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。
这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。
另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。
总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning. ↩︎
Devlin, J., Chang, M....</p></div><footer class=entry-footer><span title='2022-11-14 16:01:41 +0800 +0800'>2022年11月14日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;729 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to OFA：走向通用统一模型" href=http://qwenlm.github.io/zh/blog/ofa/></a></article><article class=post-entry><header class=entry-header><h2>OFASys：一行代码带你搞定多任务学习！</h2></header><div class=entry-content><p>引言 通用模型非常火！我们现在跟随多模态多任务学习的发展似乎看到了实现一个真正的通用模型的机会。我们此前推出的OFA便是朝着这个目标迈向的重要一步。但是，我们在实际实现过程中遇到了非常多的困难。比如说，把多任务训练的模型搭建起来，组织多任务的训练比如给数据打batch和保证训练稳定等等，都非常困难。因此，我们推出一个AI系统OFASys，它主要解决多模态多任务学习的实现问题。简单来说，它主要通过一个叫做“Instruction”的接口来实现。Instruction 即指定了任务描述和输入信息的模板。因此，用户只需要写一行代码构建好自己的 Instruction，就可以构建一个多模态多任务学习的任务。后续的复杂步骤用户无需关心，包括数据处理、模型构建和训练等。总而言之，OFASys帮你摆脱很多复杂的实现细节，让你专注于设计任务和模态组合等。
论文 GitHub
背景 Transformer和预训练技术的蓬勃发展让我们看到了实现通用AI的机会。在自然语言领域，我们见证了GPT-31以及不可思议的ChatGPT及其背后的GPT-3.5系列，它们都具备了极强的问答、对话、以及文字创作能力等。而在多模态领域，我们也看到通用统一模型的快速崛起，包括我们的OFA2、GATO3、Unified-IO4等。实现这样的模型和系统是困难的，难点往往在于实现中。算法工程师最难受的事情往往是如何实现这样一个模型，并且成功把它训起来，接入不同类型的数据和任务并且对它们实现良好的管理。尽管我们针对深度学习有了PyTorch和TensorFlow这样的基建，以及很多诸如Hugging Face Transformers和fairseq这样的帮助实现Transformer的框架，但当前依然没有一个专门针对多模态多任务学习的提供良好抽象和相关工具的系统。
用户接口 介绍系统设计前，我们先看看如何在OFASys中使用1行代码实现一个多任务学习模型。具体而言，你需要写一个合适的 Instruction 。下面是一些例子。
以上例子中，两个句子用“->”分隔，表示输入和期望输出。“&lt;tt>[IMAGE:img]&lt;/tt>”表示有一个图像输入和 数据集中的&lt;tt>img&lt;/tt>字段关联。Instruction 中的文本则表示这个任务是图像描述。任务输出是文本序列，即对应数据集中的&lt;tt>cap&lt;/tt>字段.
另一个例子是自然语言推理的例子，我们以MNLI为例：
和上述例子类似，我们用模板和指示词来构建 Instruction 。不同的是，输入侧包含2个输入。此外，由于我们发现在解码器重复输入有助于效果提升，我们在输出端用一个&lt;tt>no_loss&lt;/tt>的信号表示不计算这段序列对应的损失函数。而由于标签是一个封闭集，我们用&lt;tt>closed_set&lt;/tt>来表示。
简而言之，使用OFASys可以让一切变得简单。你也许只需要1行代码就可以解决问题！
系统实现 一个良好的系统实现是一个易用接口的基础。系统实现的整体架构如下所示。
OFASys通过解析 Instruction 来将任务定义和任务数据传入任务计划中。每个计划有一个模型的层次结构，其中包括模态特定的前处理和后处理模块以及模态无关的计算引擎。通用模型在这里负责融合多模态输入以及获得输出。由于输入输出都是表示序列，因此通用模型是非常灵活的。通用模型的输出最终传入后处理模块得到最终输出。而诸如损失函数计算以及生成器等都有大量实现方式。
而在多任务学习场景中，有多个上述计划。OFASys共享大部分可训练的参数，这样通用模型能使用尽可能多的数据进行训练。任务调度器在这里负责安排任务优先级以及管理多任务优化，而逻辑调度器则负责安排到多台物理机上。
应用实例: OFA+ 我们训练了一个基于OFA的通用模型OFA+，它首次实现同时处理文本、图像、语音、视频和动作多种模态。具体包括一个通用的OFA+ (Generalist)以及一个基于多模态MoE的升级版本OFA+ (Generalist MoE)。对比对象则为我们此前的OFA，它需要针对每个任务单独微调，我们称之为OFA+ (Specialist)。
从上述结果可以看出，OFA+整体能保留接近95%的OFA+ (Specialist)在各个下游任务的效果，其中涵盖7种模态的23个任务。这也能看出来多任务学习不仅赋予模型实现多任务的基础能力，同时能让它在各项任务上都能达到顶级的表现。
总结 随着通用模型快速发展，特定系统和库的空缺成了一个棘手的问题。OFASys就是为了解决实现多模态多任务学习难的问题而生。我们希望它能推动多模态多任务学习的研究同时帮助实现更加通用的通用模型。
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.J., Child, R., Ramesh, A....</p></div><footer class=entry-footer><span title='2022-12-28 18:01:21 +0800 +0800'>2022年12月28日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;219 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to OFASys：一行代码带你搞定多任务学习！" href=http://qwenlm.github.io/zh/blog/ofasys/></a></article><footer class=page-footer><nav class=pagination><a class=next href=http://qwenlm.github.io/zh/blog/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>