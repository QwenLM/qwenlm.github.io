<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>将上下文长度扩展至百万 Tokens ！ | Qwen</title><meta name=keywords content><meta name=description content="API文档 HuggingFace Demo ModelScope Demo
简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：
更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5-turbo/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-turbo/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-turbo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="将上下文长度扩展至百万 Tokens ！"><meta property="og:description" content="API文档 HuggingFace Demo ModelScope Demo
简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：
更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。"><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5-turbo/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-11-15T00:00:03+08:00"><meta property="article:modified_time" content="2024-11-15T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="将上下文长度扩展至百万 Tokens ！"><meta name=twitter:description content="API文档 HuggingFace Demo ModelScope Demo
简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：
更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"将上下文长度扩展至百万 Tokens ！","item":"https://qwenlm.github.io/zh/blog/qwen2.5-turbo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"将上下文长度扩展至百万 Tokens ！","name":"将上下文长度扩展至百万 Tokens ！","description":"API文档 HuggingFace Demo ModelScope Demo\n简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：\n更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。","keywords":[],"articleBody":"API文档 HuggingFace Demo ModelScope Demo\n简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：\n更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。\n实例演示 我们准备了一些实例演示，来体现 Qwen2.5-Turbo 能带来的新用法，涵盖了长篇小说的深入理解，仓库级别代码助手，多篇论文阅读等能力。\nExample: Understanding Long Novels\rNext\rUser\r[Upload the three Chinese novels of The Three-Body Problem, which contain 690k tokens.]\nProvide a summary of the plots of these three novels in English\nQwen2.5-Turbo\rThe three novels you mentioned from the “Earth’s Past” series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.\nFirst Novel: “Earth’s Past” In the first novel, the narrative focuses on the development of a top-secret project called “Red Coast” (红岸) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.\nSecond Novel: “Dark Forest” The second novel, titled “Dark Forest,” delves into the concept of the “Dark Forest Theory.” According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as “water drops” (水滴). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the “Wall-Bearing Plan” (面壁计划), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.\nThird Novel: “Death’s End” In “Death’s End,” Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (执剑人) brings him immense personal struggle and psychological turmoil. As Luo Ji’s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (黑域) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth. The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.\nExample: 长篇小说理解\rNext\rUser\r[上传《三体》小说三部曲，共计69万tokens]\n给出这三本小说的剧情概要\nQwen2.5-Turbo\r《三体》、《三体 II·黑暗森林》和《三体 III·死神永生》三部小说的剧情概要如下：\n《三体》\n背景：文化大革命期间，军方启动了名为“红岸工程”的绝密计划，以探寻外星文明。 情节：叶文洁在红岸基地见证了人类向宇宙发出的第一声啼鸣，这一举动彻底改变了人类的命运。三体文明在接收到地球的信息后，开始向地球进发。人类面对三体文明的入侵，采取了一系列的防御措施。 《三体 II·黑暗森林》\n背景：人类与三体文明的战争使得双方都面临前所未有的威胁，人类社会陷入极度的绝望与恐惧。 情节：罗辑被选为面壁者，执行一项高度机密的任务。他通过太阳向宇宙发送咒语，导致三体世界中的一个恒星被摧毁。随后，罗辑被指定为执剑人，负责人类的威慑计划。最终，罗辑的威慑成功地阻止了三体文明的进攻。 《三体 III·死神永生》\n背景：三体舰队和人类文明之间的战争导致了双方的毁灭。罗辑和程心在两个世纪后醒来，发现世界已经发生了巨大变化。 情节：程心和罗辑在宇宙中发现了一个被命名为“647号宇宙”的小宇宙，其中蕴藏了三体文明的秘密。他们了解到，人类需要借助三体文明的技术来生存。程心最终成为新的执剑人，罗辑则在精神上得到了新的寄托。 三部小说展现了人类文明在面对三体文明威胁时的生存斗争，从探寻外星文明到最终的战争与和平，描绘了一个宏大的宇宙史诗。 Example: Understanding Repository\rNext\rUser\r[Upload a text file containing all code files from the Qwen-Agent repository, which has a length of 133k tokens.]\nPlease read the codes carefully and ensure your response is comprehensive. Introduce this repository\nQwen2.5-Turbo\rQwen-Agent is a framework designed for developing LLM (Large Language Model) applications. It is based on the instruction-following, tool usage, planning, and memory capabilities of Qwen. Here are some key points and functionalities:\nAgent Class: Provides a high-level interface for integrating tool calls and LLM interactions. It supports role-playing, automatic planning, and tool invocation. DocQA Agent: Specialized for document question answering, capable of handling long documents and providing context-aware responses. Function Calling: Supports function calling, enabling interaction with tools or APIs directly from the model’s responses. RAG (Retrieval-Augmented Generation): Integrates document retrieval with generation, enhancing the model’s ability to recall and use information from documents. Tool Integration: Includes various built-in and custom tools, such as code interpreters, web extractors, and image generators, which can be used to perform tasks like code execution, web scraping, and image manipulation. Multi-Agent Systems: Supports the creation of multi-agent systems where multiple agents can interact, simulate conversations, and collaborate on tasks. Customization: Highly customizable, allowing developers to tailor the framework to specific needs, such as creating custom agents, defining custom workflows, and integrating additional tools. Examples and Demos: Comes with several examples and demos, such as a Chrome browser extension called BrowserQwen, which demonstrates the practical application of Qwen-Agent in a real-world scenario. Testing and Debugging: Includes testing and debugging utilities, such as test cases and logging mechanisms, to ensure the reliability and effectiveness of the developed applications. Dependencies: Requires certain dependencies to be installed for specific functionalities, such as qwen-agent[gui] for GUI support and qwen-agent[rag] for RAG support. This repository is valuable for researchers, developers, and anyone interested in leveraging large language models for complex tasks and applications. User\rWhat are the subclasses of Agent in this repository, and provide their file paths\rQwen2.5-Turbo\rHere are the subclasses of the Agent class in the Qwen-Agent repository along with their file paths:\nAssistant File Path: qwen_agent/agents/assistant.py ArticleAgent File Path: qwen_agent/agents/article_agent.py BasicDocQA File Path: qwen_agent/agents/doc_qa/basic_doc_qa.py DialogueRetrievalAgent File Path: qwen_agent/agents/dialogue_retrieval_agent.py DialogueSimulator File Path: qwen_agent/agents/dialogue_simulator.py DocQAAgent (Note: The actual implementation of DocQAAgent may vary with each release) File Path: qwen_agent/agents/doc_qa/basic_doc_qa.py GroupChat File Path: qwen_agent/agents/group_chat.py GroupChatAutoRouter File Path: qwen_agent/agents/group_chat_auto_router.py GroupChatCreator File Path: qwen_agent/agents/group_chat_creator.py HumanSimulator File Path: qwen_agent/agents/human_simulator.py ReActChat File Path: qwen_agent/agents/react_chat.py Router File Path: qwen_agent/agents/router.py TIRMathAgent File Path: qwen_agent/agents/tir_agent.py UserAgent File Path: qwen_agent/agents/user_agent.py VisualStorytelling File Path: qwen_agent/agents/visual_storytelling.py MemoAssistant File Path: qwen_agent/agents/memo_assistant.py WriteFromScratch File Path: qwen_agent/agents/writing/write_from_scratch.py ExpandWriting File Path: qwen_agent/agents/writing/expand_writing.py OutlineWriting File Path: qwen_agent/agents/writing/outline_writing.py These subclasses extend the base Agent class to provide specialized functionalities tailored for different use cases within the Qwen-Agent framework.\nExample: Understanding Multiple Papers\rNext\rUser\r[Upload 7 papers about long-context LLM, which have a length of 171k tokens. Sources: 1 2 3 4 5 6 7]\nclassify the above papers into two categories\nQwen2.5-Turbo\rThe papers provided can be classified into two categories based on their focus:\nCategory 1: Long-Context Scaling Techniques\nDCA (Dual Chunk Attention): This paper introduces a new training-free framework to allow large language models to handle long-context sequences by breaking down the attention computation into smaller chunks. (File: 1. DCA.pdf) MInference: This paper proposes a method to accelerate pre-filling for long-context language models by using a dynamic sparse attention mechanism with a context memory module. (File: 2. minference.pdf) InfLLM: This paper presents a training-free memory-based approach to enable large language models to understand extremely long sequences by incorporating an efficient context memory mechanism. (File: 4. infLLM.pdf) SnapKV: This paper introduces a fine-tuning-free approach to efficiently minimize the key-value (KV) cache size of large language models during generation while maintaining comparable performance. (File: 7. SnapKV.pdf) Category 2: Benchmarking Long-Context Capabilities\nLV-Eval: This paper introduces a challenging long-context benchmark designed to evaluate the capabilities of large language models across various lengths and tasks, with a focus on mitigating knowledge leakage and providing a balanced evaluation. (File: 3. lveval.pdf) NeedleBench: This paper presents a benchmark for evaluating the retrieval and reasoning capabilities of large language models in long-context scenarios, including a needle-in-a-haystack task. (File: 5. needlebench.pdf) RULER: This paper proposes a synthetic benchmark for evaluating long-context language models with diverse task categories, including retrieval, multi-hop tracing, aggregation, and question answering. (File: 6. ruler.pdf) User\rOutput a JSON array where each element sequentially contains the paper title and a one-sentence summary\rQwen2.5-Turbo\r[ { \"title\": \"Dual Chunk Attention\", \"summary\": \"DCA introduces a novel training-free framework to enable large language models to handle long-context sequences bydecomposing attention computation into smaller chunks.\" }, { \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\", \"summary\": \"MInference uses a dynamic sparse attention mechanism with a context memory module to significantlyreduce the latency of long-context LLM inference.\" }, { \"title\": \"InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory\", \"summary\": \"InfLLM leverages an efficient context memory to enable large language models to process extremelylong sequences without additional training.\" }, { \"title\": \"SnapKV: LLM Knows What You are Looking for Before Generation\", \"summary\": \"SnapKV is a fine-tuning-free approach that efficiently compresses the KV cache to minimize memory usage while maintaining model performance.\" }, { \"title\": \"LV-EVAL: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K\", \"summary\": \"LV-EVAL is a benchmark designed to evaluate large language models' long-context capabilities across multiple lengths and tasks, focusing on mitigating knowledge leakage.\" }, { \"title\": \"NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?\", \"summary\": \"NeedleBench assesses the retrieval and reasoning capabilities of large language models in long-context scenarios, including tasks like needle-in-a-haystack.\" }, { \"title\": \"RULER: What's the Real Context Size of Your Long-Context Language Models?\", \"summary\": \"RULER is a synthetic benchmark that evaluates long-context language models through tasks like retrieval, multi-hop tracing, aggregation, and question answering.\" } ] 如何使用 API 最新的支持 1M tokens 的 Qwen2.5-Turbo 的使用方法和 Qwen API 的标准用法一致，并且与 OpenAI API 兼容。下面是一个简单的 Python 调用示例 (注意：你需要将环境变量 YOUR_API_KEY 设置为您的 API Key，欲了解更多细节请访问阿里云大模型服务快速入门)：\nimport os from openai import OpenAI # 读取长文本文件 with open(\"example.txt\", \"r\", encoding=\"utf-8\") as f: text = f.read() user_input = text + \"\\n\\nSummarize the above text.\" client = OpenAI( api_key=os.getenv(\"YOUR_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", ) completion = client.chat.completions.create( model=\"qwen-turbo-latest\", messages=[ {'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': user_input}, ], ) print(completion.choices[0].message) 模型表现 在这一部分，我们通过各类基准测试来评估 Qwen2.5-Turbo 的性能表现以及推理速度上的进步。\n大海捞针 我们首先在 1M 大海捞针 (Passkey Retrieval) 任务上进行了实验。结果显示，Qwen2.5-Turbo 能在1M长度的无关文本中完美捕捉所有隐藏的数字，证明模型在超长上下文中捕捉细节信息的能力。\n更复杂的长文本任务 我们选取了若干长文本任务数据集对模型进行了测试，包括：\nRULER：在大海捞针基础上的扩展基准，任务包括在无关上下文中查找多“针”或回答多个问题，或找到上下文中出现最多或最少的词。数据的上下文长度最长为 128K。 LV-Eval：要求同时理解众多证据片段的基准测试。我们对 LV-Eval 原始版本中的评估指标进行了调整，避免因为过于严苛的匹配规则所导致的假阴性结果。数据的上下文长度最长为 256K。 LongbenchChat：一个评价长文本任务中人类偏好对齐的数据集。数据的上下文长度最长为 100K。 从结果中可以看出，Qwen2.5-Turbo 在各类长文本任务中均有优势：\n在 RULER 基准测试中，Qwen2.5-Turbo 取得了 93.1 分，不仅超越了 GPT-4o-mini，甚至超越了 GPT-4，证明了其处理长文本任务的卓越能力。 在 LV-Eval、LongBench-Chat 等更加接近真实情况的长文本任务中，Qwen2.5-Turbo 在多数维度超越了 GPT-4o-mini，并且能够进一步扩展到超过 128K tokens 的上下文的问题上。 短文本任务 除了长序列任务的性能提升外，我们也同样关心模型在短文本任务上的性能。现有的上下文长度扩展方案经常会导致模型在处理短文本时出现比较大的性能下降，因此我们在构建 Qwen2.5-Turbo 的过程中特别关注了该问题，保证了在扩展上下文长度的同时，几乎不会影响短文本任务的能力。\n在传统短文本基准上的结果显示，Qwen2.5-Turbo 在大部分任务上显著超越之前上下文长度为 1M tokens 的开源模型；和 GPT-4o-mini 以及 Qwen2.5-14B-Instruct 模型相比，Qwen2.5-Turbo 在短文本任务性能接近的同时，却能支持其 8 倍长度的上下文。\n推理速度 我们对不同长度的输入在多种模型架构下的首包延时时间进行了测试。在 1M tokens 的序列上，我们利用稀疏注意力机制将注意力部分的计算量压缩了约 12.5 倍，在不同硬件配置下实现了 3.2 至 4.3 倍的加速比。\n下一步目标 虽然我们很高兴终于能够将 Qwen2.5-Turbo 的上下文扩展到 1M tokens，但我们也清楚地认识到，当前模型在解决实际长序列任务上并不总是令人满意。这里存在很多未解决的挑战，例如长序列任务中模型的表现更加不稳定，推理的成本使得使用更大规模的模型变得困难等。不过，我们将积极进一步探索长序列人类偏好对齐，优化推理效率以减少运算时间，尝试推出更大、更强的长序列模型。我们期望能够很快向大家分享在长序列模型上的新进展，敬请关注！\n","wordCount":"1687","inLanguage":"zh","datePublished":"2024-11-15T00:00:03+08:00","dateModified":"2024-11-15T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5-turbo/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>将上下文长度扩展至百万 Tokens ！</h1><div class=post-meta><span title='2024-11-15 00:00:03 +0800 +0800'>2024年11月15日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1687 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5-turbo/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen class="btn external" target=_blank>API文档</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>HuggingFace Demo</a>
<a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>ModelScope Demo</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：</p><ol><li><strong>更长的上下文支持</strong>: 我们首次将模型的上下文长度从 128k 扩展到 <strong>1M</strong>，该长度约为 <strong>100 万个英文单词或 150 万个汉字</strong>，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 <strong>100%</strong> 的准确率，在长文本评测集 RULER 上获得 <strong>93.1</strong> 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。</li><li><strong>更快的推理速度</strong>: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，<strong>实现 4.3 倍加速比</strong>。</li><li><strong>更低的价格</strong>：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 <strong>3.6 倍的 Token</strong>。</li></ol><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/cover_cn.png#center width=100%></figure><p>现在，你可以通过<a href=https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm>阿里云大模型服务平台</a>的 API 服务，或者通过 <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo>HuggingFace Demo</a> 或者 <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo>ModelScope Demo</a> 进行体验。</p><h1 id=实例演示>实例演示<a hidden class=anchor aria-hidden=true href=#实例演示>#</a></h1><p>我们准备了一些实例演示，来体现 Qwen2.5-Turbo 能带来的新用法，涵盖了长篇小说的深入理解，仓库级别代码助手，多篇论文阅读等能力。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Understanding Long Novels</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case1_en.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload the three Chinese novels of <code>The Three-Body Problem</code>, which contain 690k tokens.]</p><p>Provide a summary of the plots of these three novels in English</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The three novels you mentioned from the &ldquo;Earth&rsquo;s Past&rdquo; series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.</p><p><strong>First Novel: &ldquo;Earth&rsquo;s Past&rdquo;</strong>
In the first novel, the narrative focuses on the development of a top-secret project called &ldquo;Red Coast&rdquo; (红岸) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.</p><p><strong>Second Novel: &ldquo;Dark Forest&rdquo;</strong>
The second novel, titled &ldquo;Dark Forest,&rdquo; delves into the concept of the &ldquo;Dark Forest Theory.&rdquo; According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as &ldquo;water drops&rdquo; (水滴). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the &ldquo;Wall-Bearing Plan&rdquo; (面壁计划), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.</p><p><strong>Third Novel: &ldquo;Death&rsquo;s End&rdquo;</strong>
In &ldquo;Death&rsquo;s End,&rdquo; Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (执剑人) brings him immense personal struggle and psychological turmoil. As Luo Ji&rsquo;s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (黑域) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth.
The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: 长篇小说理解</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case1_zh.mp4></video></figure></div><div class=role>User</div><div class=content><p>[上传《三体》小说三部曲，共计69万tokens]</p><p>给出这三本小说的剧情概要</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>《三体》、《三体 II·黑暗森林》和《三体 III·死神永生》三部小说的剧情概要如下：</p><p><strong>《三体》</strong></p><ul><li><strong>背景</strong>：文化大革命期间，军方启动了名为“红岸工程”的绝密计划，以探寻外星文明。</li><li><strong>情节</strong>：叶文洁在红岸基地见证了人类向宇宙发出的第一声啼鸣，这一举动彻底改变了人类的命运。三体文明在接收到地球的信息后，开始向地球进发。人类面对三体文明的入侵，采取了一系列的防御措施。</li></ul><p><strong>《三体 II·黑暗森林》</strong></p><ul><li><strong>背景</strong>：人类与三体文明的战争使得双方都面临前所未有的威胁，人类社会陷入极度的绝望与恐惧。</li><li><strong>情节</strong>：罗辑被选为面壁者，执行一项高度机密的任务。他通过太阳向宇宙发送咒语，导致三体世界中的一个恒星被摧毁。随后，罗辑被指定为执剑人，负责人类的威慑计划。最终，罗辑的威慑成功地阻止了三体文明的进攻。</li></ul><p><strong>《三体 III·死神永生》</strong></p><ul><li><strong>背景</strong>：三体舰队和人类文明之间的战争导致了双方的毁灭。罗辑和程心在两个世纪后醒来，发现世界已经发生了巨大变化。</li><li><strong>情节</strong>：程心和罗辑在宇宙中发现了一个被命名为“647号宇宙”的小宇宙，其中蕴藏了三体文明的秘密。他们了解到，人类需要借助三体文明的技术来生存。程心最终成为新的执剑人，罗辑则在精神上得到了新的寄托。
三部小说展现了人类文明在面对三体文明威胁时的生存斗争，从探寻外星文明到最终的战争与和平，描绘了一个宏大的宇宙史诗。</li></ul></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Repository</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case2.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload a text file containing all code files from the <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> repository, which has a length of 133k tokens.]</p><p>Please read the codes carefully and ensure your response is comprehensive. Introduce this repository</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Qwen-Agent is a framework designed for developing LLM (Large Language Model) applications. It is based on the instruction-following, tool usage, planning, and memory capabilities of Qwen. Here are some key points and functionalities:</p><ol><li><strong>Agent Class</strong>: Provides a high-level interface for integrating tool calls and LLM interactions. It supports role-playing, automatic planning, and tool invocation.</li><li><strong>DocQA Agent</strong>: Specialized for document question answering, capable of handling long documents and providing context-aware responses.</li><li><strong>Function Calling</strong>: Supports function calling, enabling interaction with tools or APIs directly from the model&rsquo;s responses.</li><li><strong>RAG (Retrieval-Augmented Generation)</strong>: Integrates document retrieval with generation, enhancing the model&rsquo;s ability to recall and use information from documents.</li><li><strong>Tool Integration</strong>: Includes various built-in and custom tools, such as code interpreters, web extractors, and image generators, which can be used to perform tasks like code execution, web scraping, and image manipulation.</li><li><strong>Multi-Agent Systems</strong>: Supports the creation of multi-agent systems where multiple agents can interact, simulate conversations, and collaborate on tasks.</li><li><strong>Customization</strong>: Highly customizable, allowing developers to tailor the framework to specific needs, such as creating custom agents, defining custom workflows, and integrating additional tools.</li><li><strong>Examples and Demos</strong>: Comes with several examples and demos, such as a Chrome browser extension called BrowserQwen, which demonstrates the practical application of Qwen-Agent in a real-world scenario.</li><li><strong>Testing and Debugging</strong>: Includes testing and debugging utilities, such as test cases and logging mechanisms, to ensure the reliability and effectiveness of the developed applications.</li><li><strong>Dependencies</strong>: Requires certain dependencies to be installed for specific functionalities, such as <code>qwen-agent[gui]</code> for GUI support and <code>qwen-agent[rag]</code> for RAG support.
This repository is valuable for researchers, developers, and anyone interested in leveraging large language models for complex tasks and applications.</li></ol></div><div class=role>User</div><div class=content>What are the subclasses of Agent in this repository, and provide their file paths</div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Here are the subclasses of the <code>Agent</code> class in the Qwen-Agent repository along with their file paths:</p><ol><li><strong>Assistant</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/assistant.py</code></li></ul><ol start=2><li><strong>ArticleAgent</strong><ul><li>File Path: <code>qwen_agent/agents/article_agent.py</code></li></ul></li><li><strong>BasicDocQA</strong><ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>DialogueRetrievalAgent</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_retrieval_agent.py</code></li></ul></li><li><strong>DialogueSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_simulator.py</code></li></ul></li><li><strong>DocQAAgent</strong> (Note: The actual implementation of DocQAAgent may vary with each release)<ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>GroupChat</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat.py</code></li></ul></li><li><strong>GroupChatAutoRouter</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_auto_router.py</code></li></ul></li><li><strong>GroupChatCreator</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_creator.py</code></li></ul></li><li><strong>HumanSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/human_simulator.py</code></li></ul></li><li><strong>ReActChat</strong><ul><li>File Path: <code>qwen_agent/agents/react_chat.py</code></li></ul></li><li><strong>Router</strong><ul><li>File Path: <code>qwen_agent/agents/router.py</code></li></ul></li><li><strong>TIRMathAgent</strong><ul><li>File Path: <code>qwen_agent/agents/tir_agent.py</code></li></ul></li><li><strong>UserAgent</strong><ul><li>File Path: <code>qwen_agent/agents/user_agent.py</code></li></ul></li><li><strong>VisualStorytelling</strong><ul><li>File Path: <code>qwen_agent/agents/visual_storytelling.py</code></li></ul></li><li><strong>MemoAssistant</strong><ul><li>File Path: <code>qwen_agent/agents/memo_assistant.py</code></li></ul></li><li><strong>WriteFromScratch</strong><ul><li>File Path: <code>qwen_agent/agents/writing/write_from_scratch.py</code></li></ul></li><li><strong>ExpandWriting</strong><ul><li>File Path: <code>qwen_agent/agents/writing/expand_writing.py</code></li></ul></li><li><strong>OutlineWriting</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/writing/outline_writing.py</code></li></ul><p>These subclasses extend the base <code>Agent</code> class to provide specialized functionalities tailored for different use cases within the Qwen-Agent framework.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Multiple Papers</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case3.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload 7 papers about long-context LLM, which have a length of 171k tokens. Sources: <a href=https://arxiv.org/abs/2402.17463>1</a> <a href=https://arxiv.org/abs/2407.02490>2</a> <a href=https://arxiv.org/abs/2402.05136>3</a> <a href=https://arxiv.org/abs/2402.04617>4</a> <a href=https://arxiv.org/abs/2407.11963>5</a> <a href=https://arxiv.org/abs/2404.06654>6</a> <a href=https://arxiv.org/abs/2404.14469>7</a>]</p><p>classify the above papers into two categories</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The papers provided can be classified into two categories based on their focus:</p><p><strong>Category 1: Long-Context Scaling Techniques</strong></p><ul><li><strong>DCA (Dual Chunk Attention)</strong>: This paper introduces a new training-free framework to allow large language models to handle long-context sequences by breaking down the attention computation into smaller chunks. (File: 1. DCA.pdf)</li><li><strong>MInference</strong>: This paper proposes a method to accelerate pre-filling for long-context language models by using a dynamic sparse attention mechanism with a context memory module. (File: 2. minference.pdf)</li><li><strong>InfLLM</strong>: This paper presents a training-free memory-based approach to enable large language models to understand extremely long sequences by incorporating an efficient context memory mechanism. (File: 4. infLLM.pdf)</li><li><strong>SnapKV</strong>: This paper introduces a fine-tuning-free approach to efficiently minimize the key-value (KV) cache size of large language models during generation while maintaining comparable performance. (File: 7. SnapKV.pdf)</li></ul><p><strong>Category 2: Benchmarking Long-Context Capabilities</strong></p><ul><li><strong>LV-Eval</strong>: This paper introduces a challenging long-context benchmark designed to evaluate the capabilities of large language models across various lengths and tasks, with a focus on mitigating knowledge leakage and providing a balanced evaluation. (File: 3. lveval.pdf)</li><li><strong>NeedleBench</strong>: This paper presents a benchmark for evaluating the retrieval and reasoning capabilities of large language models in long-context scenarios, including a needle-in-a-haystack task. (File: 5. needlebench.pdf)</li><li><strong>RULER</strong>: This paper proposes a synthetic benchmark for evaluating long-context language models with diverse task categories, including retrieval, multi-hop tracing, aggregation, and question answering. (File: 6. ruler.pdf)</li></ul></div><div class=role>User</div><div class=content>Output a JSON array where each element sequentially contains the paper title and a one-sentence summary</div><div class=role>Qwen2.5-Turbo</div><div class=content><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Dual Chunk Attention&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;DCA introduces a novel training-free framework to enable large language models to handle 
</span></span></span><span class=line><span class=cl><span class=s2>long-context sequences bydecomposing attention computation into smaller chunks.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;MInference uses a dynamic sparse attention mechanism with a context memory module to
</span></span></span><span class=line><span class=cl><span class=s2>significantlyreduce the latency of long-context LLM inference.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;InfLLM leverages an efficient context memory to enable large language models to process
</span></span></span><span class=line><span class=cl><span class=s2>extremelylong sequences without additional training.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;SnapKV: LLM Knows What You are Looking for Before Generation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;SnapKV is a fine-tuning-free approach that efficiently compresses the KV cache to minimize
</span></span></span><span class=line><span class=cl><span class=s2>memory usage while maintaining model performance.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;LV-EVAL: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;LV-EVAL is a benchmark designed to evaluate large language models&#39; long-context capabilities
</span></span></span><span class=line><span class=cl><span class=s2>across multiple lengths and tasks, focusing on mitigating knowledge leakage.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;NeedleBench assesses the retrieval and reasoning capabilities of large language models in
</span></span></span><span class=line><span class=cl><span class=s2>long-context scenarios, including tasks like needle-in-a-haystack.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;RULER: What&#39;s the Real Context Size of Your Long-Context Language Models?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;RULER is a synthetic benchmark that evaluates long-context language models through tasks
</span></span></span><span class=line><span class=cl><span class=s2>like retrieval, multi-hop tracing, aggregation, and question answering.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></div></div></div></div></div><h1 id=如何使用-api>如何使用 API<a hidden class=anchor aria-hidden=true href=#如何使用-api>#</a></h1><p>最新的支持 1M tokens 的 Qwen2.5-Turbo 的使用方法和 Qwen API 的标准用法一致，并且与 OpenAI API 兼容。下面是一个简单的 Python 调用示例 (注意：你需要将环境变量 <code>YOUR_API_KEY</code> 设置为您的 API Key，欲了解更多细节请访问<a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen>阿里云大模型服务快速入门</a>)：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 读取长文本文件</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;example.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>user_input</span> <span class=o>=</span> <span class=n>text</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>Summarize the above text.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;YOUR_API_KEY&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-turbo-latest&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;system&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;You are a helpful assistant.&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=n>user_input</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=模型表现>模型表现<a hidden class=anchor aria-hidden=true href=#模型表现>#</a></h1><p>在这一部分，我们通过各类基准测试来评估 Qwen2.5-Turbo 的性能表现以及推理速度上的进步。</p><h2 id=大海捞针>大海捞针<a hidden class=anchor aria-hidden=true href=#大海捞针>#</a></h2><p>我们首先在 1M 大海捞针 (Passkey Retrieval) 任务上进行了实验。结果显示，Qwen2.5-Turbo 能在1M长度的无关文本中完美捕捉所有隐藏的数字，证明模型在超长上下文中捕捉细节信息的能力。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/passkey_retrieval.png#center width=100%></figure><h2 id=更复杂的长文本任务>更复杂的长文本任务<a hidden class=anchor aria-hidden=true href=#更复杂的长文本任务>#</a></h2><p>我们选取了若干长文本任务数据集对模型进行了测试，包括：</p><ul><li><a href=https://github.com/hsiehjackson/RULER>RULER</a>：在大海捞针基础上的扩展基准，任务包括在无关上下文中查找多“针”或回答多个问题，或找到上下文中出现最多或最少的词。数据的上下文长度最长为 128K。</li><li><a href=https://github.com/infinigence/LVEval>LV-Eval</a>：要求同时理解众多证据片段的基准测试。我们对 LV-Eval 原始版本中的评估指标进行了调整，避免因为过于严苛的匹配规则所导致的假阴性结果。数据的上下文长度最长为 256K。</li><li><a href=https://github.com/THUDM/LongAlign>LongbenchChat</a>：一个评价长文本任务中人类偏好对齐的数据集。数据的上下文长度最长为 100K。</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/long_result.png#center width=100%></figure><p>从结果中可以看出，Qwen2.5-Turbo 在各类长文本任务中均有优势：</p><ul><li>在 RULER 基准测试中，Qwen2.5-Turbo 取得了 93.1 分，不仅超越了 GPT-4o-mini，甚至超越了 GPT-4，证明了其处理长文本任务的卓越能力。</li><li>在 LV-Eval、LongBench-Chat 等更加接近真实情况的长文本任务中，Qwen2.5-Turbo 在多数维度超越了 GPT-4o-mini，并且能够进一步扩展到超过 128K tokens 的上下文的问题上。</li></ul><h2 id=短文本任务>短文本任务<a hidden class=anchor aria-hidden=true href=#短文本任务>#</a></h2><p>除了长序列任务的性能提升外，我们也同样关心模型在短文本任务上的性能。现有的上下文长度扩展方案经常会导致模型在处理短文本时出现比较大的性能下降，因此我们在构建 Qwen2.5-Turbo 的过程中特别关注了该问题，保证了在扩展上下文长度的同时，几乎不会影响短文本任务的能力。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/short_result.png#center width=100%></figure><p>在传统短文本基准上的结果显示，Qwen2.5-Turbo 在大部分任务上显著超越之前上下文长度为 1M tokens 的开源模型；和 GPT-4o-mini 以及 Qwen2.5-14B-Instruct 模型相比，Qwen2.5-Turbo 在短文本任务性能接近的同时，却能支持其 8 倍长度的上下文。</p><h2 id=推理速度>推理速度<a hidden class=anchor aria-hidden=true href=#推理速度>#</a></h2><p>我们对不同长度的输入在多种模型架构下的首包延时时间进行了测试。在 1M tokens 的序列上，我们利用稀疏注意力机制将注意力部分的计算量压缩了约 12.5 倍，在不同硬件配置下实现了 3.2 至 4.3 倍的加速比。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/inference_speed.png#center width=70%></figure><h1 id=下一步目标>下一步目标<a hidden class=anchor aria-hidden=true href=#下一步目标>#</a></h1><p>虽然我们很高兴终于能够将 Qwen2.5-Turbo 的上下文扩展到 1M tokens，但我们也清楚地认识到，当前模型在解决实际长序列任务上并不总是令人满意。这里存在很多未解决的挑战，例如长序列任务中模型的表现更加不稳定，推理的成本使得使用更大规模的模型变得困难等。不过，我们将积极进一步探索长序列人类偏好对齐，优化推理效率以减少运算时间，尝试推出更大、更强的长序列模型。我们期望能够很快向大家分享在长序列模型上的新进展，敬请关注！</p></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>