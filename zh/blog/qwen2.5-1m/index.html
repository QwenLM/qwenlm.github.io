<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型 | Qwen</title><meta name=keywords content><meta name=description content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：
开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。
推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。
技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。
现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。
另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。
模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。
长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-1m/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型"><meta property="og:description" content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：
开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。
推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。
技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。
现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。
另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。
模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。
长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。"><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5-1m/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-27T00:00:03+08:00"><meta property="article:modified_time" content="2025-01-27T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型"><meta name=twitter:description content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：
开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。
推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。
技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。
现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。
另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。
模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。
长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型","item":"https://qwenlm.github.io/zh/blog/qwen2.5-1m/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型","name":"Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型","description":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\n简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：\n开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。\n推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。\n技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。\n现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。\n另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。\n模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。\n长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。","keywords":[],"articleBody":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\n简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：\n开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。\n推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。\n技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。\n现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。\n另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。\n模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。\n长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。\n对于更复杂的长上下文理解任务，我们选择了RULER、LV-Eval 和 LongbenchChat，这些测试集也在此博客中进行了介绍。\n从这些结果中，我们可以得出以下几点关键结论：\n显著超越128K版本：Qwen2.5-1M 系列模型在大多数长上下文任务中显著优于之前的128K版本，特别是在处理超过64K长度的任务时表现出色。 性能优势明显：Qwen2.5-14B-Instruct-1M 模型不仅击败了 Qwen2.5-Turbo，还在多个数据集上稳定超越 GPT-4o-mini，因此可以作为现有长上下文模型的优秀开源替代。 短序列任务 除了长序列任务的性能外，我们同样关注这些模型在短序列上的表现。我们在广泛使用的学术基准测试中比较了 Qwen2.5-1M 系列模型及之前的128K版本，并加入了 GPT-4o-mini 进行对比。\n我们可以发现：\nQwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M 在短文本任务上的表现与其128K版本相当，确保了基本能力没有因为增加了长序列处理能力而受到影响。\n与 GPT-4o-mini 相比，Qwen2.5-14B-Instruct-1M 和 Qwen2.5-Turbo 在短文本任务上实现了相近的性能，同时上下文长度是 GPT-4o-mini 的八倍。\n关键技术 在这里，我们将简要介绍构建 Qwen2.5-1M 背后的关键技术。更多内容可参阅我们的技术报告。\n长上下文训练 长序列的训练需要大量的计算资源，因此我们采用了逐步扩展长度的方法，在多个阶段将 Qwen2.5-1M 的上下文长度从 4K 扩展到 256K：\n我们从预训练的Qwen2.5的一个中间检查点开始，此时上下文长度为4K。 在预训练阶段，我们逐步将上下文长度从 4K 增加到 256K，同时使用Adjusted Base Frequency的方案，将 RoPE 基础频率从 10,000 提高到 10,000,000。 在监督微调阶段，我们分两个阶段进行以保持短序列上的性能： 第一阶段： 仅在短指令（最多 32K 长度）上进行微调，这里我们使用与 Qwen2.5 的 128K 版本相同的数据和步骤数，以获得类似的短任务性能。 第二阶段： 混合短指令（最多 32K）和长指令（最多 256K）进行训练，以实现在增强长任务的性能的同时，保持短任务上的准确率。 在强化学习阶段，我们在短文本（最多 8K 长度）上训练模型。我们发现，即使在短文本上进行训练，也能很好地将人类偏好对齐性能泛化到长上下文任务中。 通过以上训练，我们最终获得了 256K 上下文长度的指令微调模型。\n长度外推 在上述训练过程中，模型的上下文长度仅为 256K 个 Tokens。为了将其扩展到 1M ，我们采用了长度外推的技术。\n当前，基于旋转位置编码的大型语言模型会在长上下文任务中产生性能下降，这主要是由于在计算注意力权重时，Query 和 Key 之间的相对位置距离过大，在训练过程中未曾见过。为了解决这一问题，我们引入了Dual Chunk Attention (DCA)，该方法通过将过大的相对位置，重新映射为较小的值，从而解决了这一难题。\n我们对 Qwen2.5-1M 模型及之前 128K 的版本进行了评估，分别测试了使用和不使用长度外推方法的情况。\n结果表明：即使是仅在 32K 长度上训练的 Qwen2.5-7B-Instruct，在处理 1M 上下文的 Passkey Retrieval 任务中也能达到近乎完美的准确率。这充分展示了 DCA 在无需额外训练的情况下，也可显著扩展支持的上下文长度的强大能力。\n稀疏注意力机制 对于长上下文的语言模型，推理速度对用户体验至关重要。为了加速预填充阶段，我们引入了基于 MInference 的稀疏注意力优化。在此基础上，我们还提出了一系列改进：\n分块预填充： 如果直接使用模型处理长度100万的序列，其中 MLP 层的激活权重会产生巨大的显存开销。以Qwen2.5-7B 为例，这部分开销高达 71GB。通过将分块预填充（Chunked Prefill）与稀疏注意力适配，可以将输入序列以 32768 长度分块，逐块进行预填充，MLP 层激活权重的显存使用量可减少 96.7%，因而显著降低了设备的显存需求。\n集成长度外推方案： 我们在稀疏注意力机制中进一步集成了基于 DCA 的长度外推方案，这使我们的推理框架能够同时享受更高的推理效率和长序列任务的准确性。\n稀疏性优化： 原始的 MInference 方法需要进行离线搜索以确定每个注意力头的最佳稀疏化配置。由于全注意力权重对内存的要求太大，这种搜索通常在短序列上进行，不一定能在更长序列下起到很好的效果。我们提出了一种能够在100万长度的序列上优化稀疏化配置的方法，从而显著减少了稀疏注意力带来的精度损失。\n其他优化： 我们还引入了其他优化措施，如优化算子效率和动态分块流水线并行，以充分发挥整个框架的潜力。\n通过这些改进，我们的推理框架在不同模型大小和 GPU 设备上，处理 1M 长度输入序列的预填充速度提升了 3.2 倍到 6.7 倍。\n本地部署 Qwen2.5-1M 模型 接下来，我们将逐步介绍如何将 Qwen2.5-1M 模型部署到本地设备。\n1. 系统准备 为了获得最佳性能，我们建议使用支持优化内核的 Ampere 或 Hopper 架构的 GPU。\n请确保满足以下要求：\nCUDA 版本：12.1 或 12.3 Python 版本：\u003e=3.9 且 \u003c=3.12 显存要求，对于处理 1M 长度的序列：\nQwen2.5-7B-Instruct-1M：至少需要 120GB 显存（多 GPU 总和）。 Qwen2.5-14B-Instruct-1M：至少需要 320GB 显存（多 GPU 总和）。 如果 GPU 显存不满足以上要求，你仍然可以使用 Qwen2.5-1M 进行较短任务的处理。\n2. 安装依赖项 暂时，你需要从我们的自定义分支克隆 vLLM 仓库，并手动安装。我们正在努力将我们的分支提交到 vLLM 项目中。\ngit clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git cd vllm pip install -e . -v 3. 启动 OpenAI 兼容的 API 服务 使用以下命令启动服务，根据你的硬件配置进行设置：\nvllm serve Qwen/Qwen2.5-7B-Instruct-1M \\ --tensor-parallel-size 4 \\ --max-model-len 1010000 \\ --enable-chunked-prefill --max-num-batched-tokens 131072 \\ --enforce-eager \\ --max-num-seqs 1 # --quantization fp8 # you can use fp8 quantization for model weights to reduce memory usage 参数说明：\n--tensor-parallel-size\n设置为您使用的 GPU 数量。7B 模型最多支持 4 个 GPU，14B 模型最多支持 8 个 GPU。 --max-model-len\n定义最大输入序列长度。如果遇到内存不足问题，请减少此值。 --max-num-batched-tokens\n设置 Chunked Prefill 的块大小。较小的值可以减少激活内存使用，但可能会减慢推理速度。 推荐值为 131072，以获得最佳性能。 --max-num-seqs\n限制并发处理的序列数量。 如果遇到问题，请参考 Troubleshooting 相关内容。\n4. 与模型交互 你可以使用以下方法与部署的模型进行交互：\n选项 1. 使用 Curl\ncurl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen/Qwen2.5-7B-Instruct-1M\", \"messages\": [ {\"role\": \"user\", \"content\": \"告诉我一些关于大型语言模型的事情。\"} ], \"temperature\": 0.7, \"top_p\": 0.8, \"repetition_penalty\": 1.05, \"max_tokens\": 512 }' 选项 2. 使用 Python\nfrom openai import OpenAI openai_api_key = \"EMPTY\" openai_api_base = \"http://localhost:8000/v1\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) prompt = ( \"There is an important info hidden inside a lot of irrelevant text. \" \"Find it and memorize it. I will quiz you about the important information there.\\n\\n\" \"The pass key is 28884. Remember it. 28884 is the pass key.\\n\" \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. \" * 800 \"\\nWhat is the pass key?\" # The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier. ) chat_response = client.chat.completions.create( model=\"Qwen/Qwen2.5-7B-Instruct-1M\", messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0, ) print(\"Chat response:\", chat_response.choices[0].message.content) 其他选项\n对于更高级的使用方式，可以探索如 Qwen-Agent 之类的框架。Qwen-Agent 使模型能够读取 PDF 文件或获得更多功能。\n下一步 虽然 Qwen2.5-1M 系列为长序列处理任务带来了优秀的开源选择，我们也充分认识到长上下文模型仍有很大的提升空间。我们的目标是打造在长短任务中均能表现卓越的模型，确保它们在实际应用场景中真正发挥作用。为此，我们正深入研究更高效的训练方式、模型架构和推理方法，力求使这些模型即使在资源有限的环境中也能高效部署并且获得最佳的性能效果。 我们坚信，这些努力将为长上下文模型开启全新的可能性，大幅拓展其应用范围。我们将持续突破这一领域的边界，敬请期待！\n","wordCount":"501","inLanguage":"zh","datePublished":"2025-01-27T00:00:03+08:00","dateModified":"2025-01-27T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5-1m/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型</h1><div class=post-meta><span title='2025-01-27 00:00:03 +0800 +0800'>2025年1月27日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;501 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5-1m/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf class="btn external" target=_blank>Tech Report</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HuggingFace</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>ModelScope</a>
<a href=https://chat.qwenlm.ai/ class="btn external" target=_blank>Qwen Chat</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo class="btn external" target=_blank>HuggingFace Demo</a>
<a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo class="btn external" target=_blank>ModelScope Demo</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>两个月前，我们升级了 <a href=../qwen2.5-turbo>Qwen2.5-Turbo</a>，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：</p><ol><li><p><strong>开源模型：</strong> 我们发布了两个新的开源模型，分别是 <strong>Qwen2.5-7B-Instruct-1M</strong> 和 <strong>Qwen2.5-14B-Instruct-1M</strong>，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。</p></li><li><p><strong>推理框架：</strong> 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 <a href=https://github.com/vllm-project/vllm>vLLM</a> 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 <strong>3倍到7倍</strong>。</p></li><li><p><strong><a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf>技术报告</a>：</strong> 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。</p></li></ol><p>现在，你可以访问我们在 <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo>Huggingface</a> 和 <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo>Modelscope</a> 上的在线演示来体验 Qwen2.5-1M 模型。</p><p>另外，我们最近也推出了 <strong><a href=https://chat.qwenlm.ai/>Qwen Chat</a></strong> ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。</p><h1 id=模型性能>模型性能<a hidden class=anchor aria-hidden=true href=#模型性能>#</a></h1><p>首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。</p><h2 id=长上下文任务>长上下文任务<a hidden class=anchor aria-hidden=true href=#长上下文任务>#</a></h2><p>在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/passkey_retrieval.png#center width=100%></figure><p>对于更复杂的长上下文理解任务，我们选择了<a href=https://github.com/hsiehjackson/RULER>RULER</a>、<a href=https://github.com/infinigence/LVEval>LV-Eval</a> 和 <a href=https://github.com/THUDM/LongAlign>LongbenchChat</a>，这些测试集也在<a href=../qwen2.5-turbo/#more-complex-long-text-tasks>此博客</a>中进行了介绍。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/ruler.png#center width=80%></figure><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/lv-eval.png#center width=80%></figure><p>从这些结果中，我们可以得出以下几点关键结论：</p><ol><li><strong>显著超越128K版本</strong>：Qwen2.5-1M 系列模型在大多数长上下文任务中显著优于之前的128K版本，特别是在处理超过64K长度的任务时表现出色。</li><li><strong>性能优势明显</strong>：Qwen2.5-14B-Instruct-1M 模型不仅击败了 Qwen2.5-Turbo，还在多个数据集上稳定超越 GPT-4o-mini，因此可以作为现有长上下文模型的优秀开源替代。</li></ol><h2 id=短序列任务>短序列任务<a hidden class=anchor aria-hidden=true href=#短序列任务>#</a></h2><p>除了长序列任务的性能外，我们同样关注这些模型在短序列上的表现。我们在广泛使用的学术基准测试中比较了 Qwen2.5-1M 系列模型及之前的128K版本，并加入了 GPT-4o-mini 进行对比。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/short_result.png#center width=80%></figure><p>我们可以发现：</p><ul><li><p>Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M 在短文本任务上的表现与其128K版本相当，确保了基本能力没有因为增加了长序列处理能力而受到影响。</p></li><li><p>与 GPT-4o-mini 相比，Qwen2.5-14B-Instruct-1M 和 Qwen2.5-Turbo 在短文本任务上实现了相近的性能，同时上下文长度是 GPT-4o-mini 的八倍。</p></li></ul><h1 id=关键技术>关键技术<a hidden class=anchor aria-hidden=true href=#关键技术>#</a></h1><p>在这里，我们将简要介绍构建 Qwen2.5-1M 背后的关键技术。更多内容可参阅我们的<a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf>技术报告</a>。</p><h2 id=长上下文训练>长上下文训练<a hidden class=anchor aria-hidden=true href=#长上下文训练>#</a></h2><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/training_stages.png#center width=70%></figure><p>长序列的训练需要大量的计算资源，因此我们采用了逐步扩展长度的方法，在多个阶段将 Qwen2.5-1M 的上下文长度从 4K 扩展到 256K：</p><ul><li>我们从预训练的Qwen2.5的一个中间检查点开始，此时上下文长度为4K。</li><li><strong>在预训练阶段</strong>，我们逐步将上下文长度从 4K 增加到 256K，同时使用<a href=https://arxiv.org/abs/2309.16039>Adjusted Base Frequency</a>的方案，将 RoPE 基础频率从 10,000 提高到 10,000,000。</li><li><strong>在监督微调阶段</strong>，我们分两个阶段进行以保持短序列上的性能：<ul><li><strong>第一阶段：</strong> 仅在短指令（最多 32K 长度）上进行微调，这里我们使用与 Qwen2.5 的 128K 版本相同的数据和步骤数，以获得类似的短任务性能。</li><li><strong>第二阶段：</strong> 混合短指令（最多 32K）和长指令（最多 256K）进行训练，以实现在增强长任务的性能的同时，保持短任务上的准确率。</li></ul></li><li><strong>在强化学习阶段</strong>，我们在短文本（最多 8K 长度）上训练模型。我们发现，即使在短文本上进行训练，也能很好地将人类偏好对齐性能泛化到长上下文任务中。</li></ul><p><p>通过以上训练，我们最终获得了 256K 上下文长度的指令微调模型。</p><h2 id=长度外推>长度外推<a hidden class=anchor aria-hidden=true href=#长度外推>#</a></h2><p>在上述训练过程中，模型的上下文长度仅为 256K 个 Tokens。为了将其扩展到 1M ，我们采用了长度外推的技术。</p><p>当前，基于旋转位置编码的大型语言模型会在长上下文任务中产生性能下降，这主要是由于在计算注意力权重时，Query 和 Key 之间的相对位置距离过大，在训练过程中未曾见过。为了解决这一问题，我们引入了<a href=https://arxiv.org/abs/2402.17463><strong>Dual Chunk Attention</strong></a> (DCA)，该方法通过将过大的相对位置，重新映射为较小的值，从而解决了这一难题。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca.png#center width=70%></figure><p>我们对 Qwen2.5-1M 模型及之前 128K 的版本进行了评估，分别测试了使用和不使用长度外推方法的情况。</p><p>结果表明：即使是仅在 32K 长度上训练的 Qwen2.5-7B-Instruct，在处理 1M 上下文的 Passkey Retrieval 任务中也能达到近乎完美的准确率。这充分展示了 DCA 在无需额外训练的情况下，也可显著扩展支持的上下文长度的强大能力。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca_ablation.png#center width=40%></figure><h2 id=稀疏注意力机制>稀疏注意力机制<a hidden class=anchor aria-hidden=true href=#稀疏注意力机制>#</a></h2><p>对于长上下文的语言模型，推理速度对用户体验至关重要。为了加速预填充阶段，我们引入了基于 <a href=https://arxiv.org/abs/2407.02490><strong>MInference</strong></a> 的稀疏注意力优化。在此基础上，我们还提出了一系列改进：</p><ul><li><p><strong>分块预填充：</strong> 如果直接使用模型处理长度100万的序列，其中 MLP 层的激活权重会产生巨大的显存开销。以Qwen2.5-7B 为例，这部分开销高达 71GB。通过将分块预填充（Chunked Prefill）与稀疏注意力适配，可以将输入序列以 32768 长度分块，逐块进行预填充，MLP 层激活权重的显存使用量可减少 96.7%，因而显著降低了设备的显存需求。</p></li><li><p><strong>集成长度外推方案：</strong> 我们在稀疏注意力机制中进一步集成了基于 DCA 的长度外推方案，这使我们的推理框架能够同时享受更高的推理效率和长序列任务的准确性。</p></li><li><p><strong>稀疏性优化：</strong> 原始的 MInference 方法需要进行离线搜索以确定每个注意力头的最佳稀疏化配置。由于全注意力权重对内存的要求太大，这种搜索通常在短序列上进行，不一定能在更长序列下起到很好的效果。我们提出了一种能够在100万长度的序列上优化稀疏化配置的方法，从而显著减少了稀疏注意力带来的精度损失。</p></li><li><p><strong>其他优化：</strong> 我们还引入了其他优化措施，如优化算子效率和动态分块流水线并行，以充分发挥整个框架的潜力。</p></li></ul><p><p>通过这些改进，我们的推理框架在不同模型大小和 GPU 设备上，处理 1M 长度输入序列的预填充速度提升了 3.2 倍到 6.7 倍。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/speed.png#center width=85%></figure><h1 id=本地部署-qwen25-1m-模型>本地部署 Qwen2.5-1M 模型<a hidden class=anchor aria-hidden=true href=#本地部署-qwen25-1m-模型>#</a></h1><p>接下来，我们将逐步介绍如何将 Qwen2.5-1M 模型部署到本地设备。</p><h3 id=1-系统准备>1. 系统准备<a hidden class=anchor aria-hidden=true href=#1-系统准备>#</a></h3><p>为了获得最佳性能，我们建议使用支持优化内核的 Ampere 或 Hopper 架构的 GPU。</p><p>请确保满足以下要求：</p><ul><li><strong>CUDA 版本</strong>：12.1 或 12.3</li><li><strong>Python 版本</strong>：>=3.9 且 &lt;=3.12</li></ul><p><p>显存要求，对于处理 1M 长度的序列：</p><ul><li><strong>Qwen2.5-7B-Instruct-1M</strong>：至少需要 120GB 显存（多 GPU 总和）。</li><li><strong>Qwen2.5-14B-Instruct-1M</strong>：至少需要 320GB 显存（多 GPU 总和）。</li></ul><p>如果 GPU 显存不满足以上要求，你仍然可以使用 Qwen2.5-1M 进行较短任务的处理。</p><h3 id=2-安装依赖项>2. 安装依赖项<a hidden class=anchor aria-hidden=true href=#2-安装依赖项>#</a></h3><p>暂时，你需要从我们的自定义分支克隆 vLLM 仓库，并手动安装。我们正在努力将我们的分支提交到 vLLM 项目中。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> vllm
</span></span><span class=line><span class=cl>pip install -e . -v
</span></span></code></pre></div><h3 id=3-启动-openai-兼容的-api-服务>3. 启动 OpenAI 兼容的 API 服务<a hidden class=anchor aria-hidden=true href=#3-启动-openai-兼容的-api-服务>#</a></h3><p>使用以下命令启动服务，根据你的硬件配置进行设置：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vllm serve Qwen/Qwen2.5-7B-Instruct-1M <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --tensor-parallel-size <span class=m>4</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --max-model-len <span class=m>1010000</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --enable-chunked-prefill --max-num-batched-tokens <span class=m>131072</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --enforce-eager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --max-num-seqs <span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --quantization fp8 # you can use fp8 quantization for model weights to reduce memory usage</span>
</span></span></code></pre></div><p><p><strong>参数说明：</strong></p><ul><li><p><strong><code>--tensor-parallel-size</code></strong></p><ul><li>设置为您使用的 GPU 数量。7B 模型最多支持 4 个 GPU，14B 模型最多支持 8 个 GPU。</li></ul></li><li><p><strong><code>--max-model-len</code></strong></p><ul><li>定义最大输入序列长度。如果遇到内存不足问题，请减少此值。</li></ul></li><li><p><strong><code>--max-num-batched-tokens</code></strong></p><ul><li>设置 Chunked Prefill 的块大小。较小的值可以减少激活内存使用，但可能会减慢推理速度。</li><li>推荐值为 131072，以获得最佳性能。</li></ul></li><li><p><strong><code>--max-num-seqs</code></strong></p><ul><li>限制并发处理的序列数量。</li></ul></li></ul><p><p>如果遇到问题，请参考 <a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M#troubleshooting>Troubleshooting</a> 相关内容。</p><h3 id=4-与模型交互>4. 与模型交互<a hidden class=anchor aria-hidden=true href=#4-与模型交互>#</a></h3><p>你可以使用以下方法与部署的模型进行交互：</p><p><strong>选项 1. 使用 Curl</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;告诉我一些关于大型语言模型的事情。&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>    ],
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;temperature&#34;: 0.7,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;top_p&#34;: 0.8,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;repetition_penalty&#34;: 1.05,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;max_tokens&#34;: 512
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span>
</span></span></code></pre></div><p><strong>选项 2. 使用 Python</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>openai_api_key</span> <span class=o>=</span> <span class=s2>&#34;EMPTY&#34;</span>
</span></span><span class=line><span class=cl><span class=n>openai_api_base</span> <span class=o>=</span> <span class=s2>&#34;http://localhost:8000/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>openai_api_key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=n>openai_api_base</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;There is an important info hidden inside a lot of irrelevant text. &#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Find it and memorize it. I will quiz you about the important information there.</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The pass key is 28884. Remember it. 28884 is the pass key.</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. &#34;</span> <span class=o>*</span> <span class=mi>800</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>What is the pass key?&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier.</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chat_response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}],</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Chat response:&#34;</span><span class=p>,</span> <span class=n>chat_response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span></code></pre></div><p><p><strong>其他选项</strong></p><p>对于更高级的使用方式，可以探索如 <a href=https://github.com/QwenLM/Qwen-Agent/tree/main>Qwen-Agent</a> 之类的框架。Qwen-Agent 使模型能够读取 PDF 文件或获得更多功能。</p><h1 id=下一步>下一步<a hidden class=anchor aria-hidden=true href=#下一步>#</a></h1><p>虽然 Qwen2.5-1M 系列为长序列处理任务带来了优秀的开源选择，我们也充分认识到长上下文模型仍有很大的提升空间。我们的目标是打造在长短任务中均能表现卓越的模型，确保它们在实际应用场景中真正发挥作用。为此，我们正深入研究更高效的训练方式、模型架构和推理方法，力求使这些模型即使在资源有限的环境中也能高效部署并且获得最佳的性能效果。
我们坚信，这些努力将为长上下文模型开启全新的可能性，大幅拓展其应用范围。我们将持续突破这一领域的边界，敬请期待！</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>