<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OFA：走向通用统一模型 | Qwen</title><meta name=keywords content><meta name=description content="2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。
论文 GitHub ModelScope 体验
背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。
方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。
统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。
模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。
多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。
我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。
我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。
实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。
此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。
单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。
另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。
这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。
另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。
总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning.&#160;&#8617;&#xfe0e;
Devlin, J., Chang, M."><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/zh/blog/ofa/><link crossorigin=anonymous href=/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://qwenlm.github.io/blog/ofa/><link rel=alternate hreflang=zh href=http://qwenlm.github.io/zh/blog/ofa/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="OFA：走向通用统一模型"><meta property="og:description" content="2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。
论文 GitHub ModelScope 体验
背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。
方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。
统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。
模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。
多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。
我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。
我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。
实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。
此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。
单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。
另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。
这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。
另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。
总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning.&#160;&#8617;&#xfe0e;
Devlin, J., Chang, M."><meta property="og:type" content="article"><meta property="og:url" content="http://qwenlm.github.io/zh/blog/ofa/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-11-14T16:01:41+08:00"><meta property="article:modified_time" content="2022-11-14T16:01:41+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="OFA：走向通用统一模型"><meta name=twitter:description content="2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。
论文 GitHub ModelScope 体验
背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。
方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。
统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。
模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。
多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。
我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。
我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。
实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。
此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。
单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。
另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。
这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。
另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。
总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning.&#160;&#8617;&#xfe0e;
Devlin, J., Chang, M."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"OFA：走向通用统一模型","item":"http://qwenlm.github.io/zh/blog/ofa/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OFA：走向通用统一模型","name":"OFA：走向通用统一模型","description":"2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。\n论文 GitHub ModelScope 体验\n背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。\n方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。\n统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。\n模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。\n多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。\n我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。\n我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。\n实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。\n此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。\n单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。\n另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。\n这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。\n另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。\n总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026amp; Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDevlin, J., Chang, M.","keywords":[],"articleBody":"\r2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA1，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。\n论文 GitHub ModelScope 体验\n背景 自BERT2成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER3、VilBERT4等。这些工作将基于Transformer的BERT2结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT5的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN6的流程，比如最简单的使用patch映射ViLT7、使用CLIP8的CLIP-ViL9，等等。而SimVLM10作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO11、Flamingo12、BeiT-313等。\n方法 OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。\n统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型14来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展1516以及基于Transformer的文本生成图像模型1718，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。\n模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T519。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer20的方法提升模型训练稳定性和最终迁移效果。\n多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。\n我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。\n我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。\n实验 我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。\n此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。\n单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT21和MAE22的效果。\n另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。\n这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。\n另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。\n总结 这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-323！\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026 Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. International Conference on Machine Learning. ↩︎\nDevlin, J., Chang, M., Lee, K., \u0026 Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv, abs/1810.04805. ↩︎ ↩︎\nChen, Y., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., \u0026 Liu, J. (2019). UNITER: UNiversal Image-TExt Representation Learning. European Conference on Computer Vision. ↩︎\nLu, J., Batra, D., Parikh, D., \u0026 Lee, S. (2019). ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. Neural Information Processing Systems. ↩︎\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., \u0026 Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv, abs/2010.11929. ↩︎\nRen, S., He, K., Girshick, R.B., \u0026 Sun, J. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149. ↩︎\nKim, W., Son, B., \u0026 Kim, I. (2021). ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision. International Conference on Machine Learning. ↩︎\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., \u0026 Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning. ↩︎\nShen, S., Li, L.H., Tan, H., Bansal, M., Rohrbach, A., Chang, K., Yao, Z., \u0026 Keutzer, K. (2021). How Much Can CLIP Benefit Vision-and-Language Tasks? arXiv, abs/2107.06383. ↩︎\nWang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., \u0026 Cao, Y. (2021). SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. arXiv, abs/2108.10904. ↩︎\nLu, J., Clark, C., Zellers, R., Mottaghi, R., \u0026 Kembhavi, A. (2022). Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. arXiv, abs/2206.08916. ↩︎\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., \u0026 Simonyan, K. (2022). Flamingo: a Visual Language Model for Few-Shot Learning. arXiv, abs/2204.14198. ↩︎\nWang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O., Singhal, S., Som, S., \u0026 Wei, F. (2022). Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks. arXiv, abs/2208.10442. ↩︎\nHo, J., Jain, A., \u0026 Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. arXiv, abs/2006.11239. ↩︎\nRazavi, A., Oord, A.V., \u0026 Vinyals, O. (2019). Generating Diverse High-Fidelity Images with VQ-VAE-2. arXiv, abs/1906.00446. ↩︎\nEsser, P., Rombach, R., \u0026 Ommer, B. (2020). Taming Transformers for High-Resolution Image Synthesis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12868-12878. ↩︎\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., \u0026 Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. arXiv, abs/2102.12092. ↩︎\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., \u0026 Tang, J. (2021). CogView: Mastering Text-to-Image Generation via Transformers. Neural Information Processing Systems. ↩︎\nRaffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., \u0026 Liu, P.J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv, abs/1910.10683. ↩︎\nShleifer, S., Weston, J., \u0026 Ott, M. (2021). NormFormer: Improved Transformer Pretraining with Extra Normalization. arXiv, abs/2110.09456. ↩︎\nBao, H., Dong, L., \u0026 Wei, F. (2021). BEiT: BERT Pre-Training of Image Transformers. arXiv, abs/2106.08254. ↩︎\nHe, K., Chen, X., Xie, S., Li, Y., Doll’ar, P., \u0026 Girshick, R.B. (2021). Masked Autoencoders Are Scalable Vision Learners. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15979-15988. ↩︎\nBrown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.J., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., \u0026 Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv, abs/2005.14165. ↩︎\n","wordCount":"729","inLanguage":"zh","datePublished":"2022-11-14T16:01:41+08:00","dateModified":"2022-11-14T16:01:41+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://qwenlm.github.io/zh/blog/ofa/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>OFA：走向通用统一模型</h1><div class=post-meta><span title='2022-11-14 16:01:41 +0800 +0800'>2022年11月14日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;729 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=http://qwenlm.github.io/blog/ofa/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure class=gallery><video loop src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/demo.mp4 autoplay></video></figure><p>2022年可以说是属于通用模型的一年！随着多模态预训练的蓬勃发展，尤其是通用模型，我们看到实现一个具有处理多种模态的多种任务的能力的通用模型的机会。因此我们提出OFA<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，即One-For-All。它是一个统一的多模态预训练模型，以统一的模型架构和任务形式兼容多模态和单模态的理解与生成任务。我们使用多模态多任务的方式预训练OFA，使其成为一个接近全能的模型。我们将OFA的模型和代码全部开源到社区，希望能推动通用模型的发展。</p><p><a href=https://arxiv.org/abs/2202.03052 class="btn external" target=_blank>论文</a>
<a href=https://github.com/OFA-Sys/OFA class="btn external" target=_blank>GitHub</a>
<a href="https://www.modelscope.cn/models?name=ofa" class="btn external" target=_blank>ModelScope</a>
<a href=https://huggingface.co/spaces/OFA-Sys/OFA-Generic_Interface class="btn external" target=_blank>体验</a></p><h2 id=背景>背景<a hidden class=anchor aria-hidden=true href=#背景>#</a></h2><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/uniter.jpg#center width=80%></figure><p>自BERT<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>成功迁移到多模态领域，多模态预训练蓬勃发展，代表性的工作包括UNITER<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>、VilBERT<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>等。这些工作将基于Transformer的BERT<sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>结合到单流或双流的架构中，并将图像处理成物体特征接入Transformer中。而到了2021年，随着ViT<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>的兴起，越来越多放弃物体特征的工作出现，不再依赖复杂的如Faster-RCNN<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>的流程，比如最简单的使用patch映射ViLT<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>、使用CLIP<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>的CLIP-ViL<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>，等等。而SimVLM<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>作为这个领域一个代表工作，利用了T5/BART的特性将理解和生成任务兼容并实现多项最优表现。这些进展都奠定了通用统一模型的发展基础，2022年涌现了一批工作，包括我们的OFA、Unified-IO<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>、Flamingo<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>、BeiT-3<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>等。</p><h2 id=方法>方法<a hidden class=anchor aria-hidden=true href=#方法>#</a></h2><p>OFA希望实现的是任务、模态和架构的统一。我们提出统一模型应当具备三大特性，即任务无关、模态无关和任务全面性。任务无关即统一模型应当能接受多种任务形式而无需针对性做模型结构和训练方法的改变，模态无关即不需针对模态做特定模型结构和训练方法修改，任务全面性即模型应当尽可能学习多的任务从而让自己能力更全面更能融会贯通迁移到没学习过的新任务上。因此我们提出了三大统一，即模态、架构和任务的统一。下面我们逐一解释。</p><p>统一模态最大的难题是不同模态的离散化表示，不然我们就需要使用diffusion模型<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>来解决问题。文本表示无需改动，主要的变化在于图像和物体框。得益于近年来vector quantization的快速发展<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup><sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>以及基于Transformer的文本生成图像模型<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup><sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>，图像可以用VQ token来进行表示。而针对物体框，则可以采用分桶的方式对连续的坐标值实现离散化。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/io.jpg#center width=80%></figure><p>模型架构上我们选择的是基于Transformer的编码-解码器。它已经在NLP领域取得巨大成功，如T5<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>。而对于图像输入，我们使用ResNet的前3个stage。而对于Transformer，我们加入Normformer<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup>的方法提升模型训练稳定性和最终迁移效果。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/arc.jpg#center width=80%></figure><p>多任务学习是OFA的一大特点。我们使用了8个任务做预训练，其中包括5个图文任务、2个视觉任务和1个自然语言任务。图文任务包括视觉定位、定位物体描述、视觉问答、图文匹配和图像描述，视觉任务包括目标检测和图像还原，语言任务则是文本还原。为了让模型识别不同任务，我们为每个任务增加相应的文本提示，说明任务的内容。我们希望模型遇到新的提示能实现零样本学习。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/task.jpg#center width=80%></figure><p>我们使用了公开数据集进行预训练。我们希望研究人员能够利用我们的开源复现相应的结果。</p><p>我们一共开源了5个规模的模型，分别是OFA-Tiny (33M)，OFA-Medium (93M)，OFA-Base (180M)，OFA-Large(470M)和OFA-Huge (930M)。具体数据查看下表。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/variants.jpg#center width=80%></figure><h2 id=实验>实验<a hidden class=anchor aria-hidden=true href=#实验>#</a></h2><p>我们在多模态和单模态任务上都做了实验。在视觉-语言理解上，我们在视觉问答和视觉推理上做了实验。在VQA上，OFA的效果和800亿参数的Flamingo以及基于50亿图文数据训练的20亿参数的CoCa效果相当，并且在视觉推理上取得最优成绩。而在视觉-语言生成上，OFA在两个阶段的图像描述评测均取得最优效果。而在视觉定位任务上，base规模的模型就可以超出此前最好的模型，也反映了生成式的方法和多任务训练的有效性，并且随着规模的增加，模型效果也能实现稳定增长。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/vqa.jpg#center width=60%></figure><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/caption.jpg#center width=80%></figure><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/vg.jpg#center width=80%></figure><p>此外，我们还在文本图像生成任务上做了测试，因为预训练中我们设计了图像还原任务，模型应当具备一定的图像生成能力。可以看到OFA可以达到非常低的FID分数，并且在更大的数据上微调能明显提升它的生成效果。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/t2i.jpg#center width=80%></figure><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/t2i_cases.jpg#center width=80%></figure><p>单模态方面，我们在GLUE上验证OFA的自然语言理解能力，Gigaword上验证自然语言生成能力，以及ImageNet上验证视觉理解能力。可以看到OFA在GLUE上可以取得匹敌RoBERTa和DeBERTa的效果，而此前的多模态模型距离最优的自然语言模型差距都很大。在摘要生成上，OFA也取得最优效果。而视觉分类上，OFA也能取得匹敌BeiT<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>和MAE<sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>的效果。</p><p>另外，我们观察到OFA具备一定的新任务和新领域的迁移能力。如下图所示。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/unseen_task.jpg#center width=80%></figure><p>这个例子说明OFA对提示的理解和组合多项技能的能力。我们设计了一个基于定位的视觉问答的任务，相当于视觉问答和定位描述的组合。具体实现方式就是修改任务的提示。包含了问题和定位信息的提示引导模型做出了正确的回答。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/ofa/unseen_domain.jpg#center width=80%></figure><p>另外，OFA迁移到新领域的能力也比较不错。在动画领域数据的定位上，模型能实现较为精准的物体定位。这是因为模型学过这个任务而预训练又见过该领域数据。这也体现了模型的组合能力。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>这是我们通用统一模型研究工作的起点。我们认为这个方向很有前景，因为Transformer有着一统天下的趋势，同时它能很好地兼容多种模态和任务。我们相信，多模态领域很快也会迎来属于它的GPT-3<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>！</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022).
Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.
International Conference on Machine Learning.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv, abs/1810.04805.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Chen, Y., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., & Liu, J. (2019).
UNITER: UNiversal Image-TExt Representation Learning.
European Conference on Computer Vision.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Lu, J., Batra, D., Parikh, D., & Lee, S. (2019).
ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.
Neural Information Processing Systems.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020).
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
arXiv, abs/2010.11929.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Ren, S., He, K., Girshick, R.B., & Sun, J. (2015).
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Kim, W., Son, B., & Kim, I. (2021).
ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.
International Conference on Machine Learning.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021).
Learning Transferable Visual Models From Natural Language Supervision.
International Conference on Machine Learning.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Shen, S., Li, L.H., Tan, H., Bansal, M., Rohrbach, A., Chang, K., Yao, Z., & Keutzer, K. (2021).
How Much Can CLIP Benefit Vision-and-Language Tasks?
arXiv, abs/2107.06383.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Wang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2021).
SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.
arXiv, abs/2108.10904.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Lu, J., Clark, C., Zellers, R., Mottaghi, R., & Kembhavi, A. (2022).
Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks.
arXiv, abs/2206.08916.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., & Simonyan, K. (2022).
Flamingo: a Visual Language Model for Few-Shot Learning.
arXiv, abs/2204.14198.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O., Singhal, S., Som, S., & Wei, F. (2022).
Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks.
arXiv, abs/2208.10442.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Ho, J., Jain, A., & Abbeel, P. (2020).
Denoising Diffusion Probabilistic Models.
arXiv, abs/2006.11239.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Razavi, A., Oord, A.V., & Vinyals, O. (2019).
Generating Diverse High-Fidelity Images with VQ-VAE-2.
arXiv, abs/1906.00446.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Esser, P., Rombach, R., & Ommer, B. (2020).
Taming Transformers for High-Resolution Image Synthesis.
2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12868-12878.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I. (2021).
Zero-Shot Text-to-Image Generation.
arXiv, abs/2102.12092.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021).
CogView: Mastering Text-to-Image Generation via Transformers.
Neural Information Processing Systems.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2019).
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
arXiv, abs/1910.10683.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Shleifer, S., Weston, J., & Ott, M. (2021).
NormFormer: Improved Transformer Pretraining with Extra Normalization.
arXiv, abs/2110.09456.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Bao, H., Dong, L., & Wei, F. (2021).
BEiT: BERT Pre-Training of Image Transformers.
arXiv, abs/2106.08254.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>He, K., Chen, X., Xie, S., Li, Y., Doll&rsquo;ar, P., & Girshick, R.B. (2021).
Masked Autoencoders Are Scalable Vision Learners.
2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15979-15988.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.J., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020).
Language Models are Few-Shot Learners.
arXiv, abs/2005.14165.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>