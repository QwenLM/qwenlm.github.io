<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5: 基础模型大派对！ | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。
我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！
我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：
Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。
如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：
Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math
准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！
要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5: 基础模型大派对！"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。
我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！
我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：
Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。
如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：
Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math
准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！
要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-09-19T00:00:04+08:00"><meta property="article:modified_time" content="2024-09-19T00:00:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5: 基础模型大派对！"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。
我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！
我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：
Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。
如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：
Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math
准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！
要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5: 基础模型大派对！","item":"https://qwenlm.github.io/zh/blog/qwen2.5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5: 基础模型大派对！","name":"Qwen2.5: 基础模型大派对！","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。\n我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！\n我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：\nQwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。\n如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：\nQwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math\n准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！\n要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2.","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。\n我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！\n我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：\nQwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。\n如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：\nQwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math\n准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！\n要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2.5 获得了显著更多的知识（MMLU：85+），并在编程能力（HumanEval 85+）和数学能力（MATH 80+）方面有了大幅提升。此外，新模型在指令执行、生成长文本（超过 8K 标记）、理解结构化数据（例如表格）以及生成结构化输出特别是 JSON 方面取得了显著改进。 Qwen2.5 模型总体上对各种system prompt更具适应性，增强了角色扮演实现和聊天机器人的条件设置功能。与 Qwen2 类似，Qwen2.5 语言模型支持高达 128K tokens，并能生成最多 8K tokens的内容。它们同样保持了对包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等 29 种以上语言的支持。 我们在下表中提供了有关模型的基本信息。\n专业领域的专家语言模型，即用于编程的 Qwen2.5-Coder 和用于数学的 Qwen2.5-Math，相比其前身 CodeQwen1.5 和 Qwen2-Math 有了实质性的改进。 具体来说，Qwen2.5-Coder 在包含 5.5 T tokens 编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。 同时，Qwen2.5-Math 支持 中文 和 英文，并整合了多种推理方法，包括CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）。\nAPI 除了我们的开源模型之外，我们还通过 API 服务提供了更多的模型。您可以访问 阿里云百炼平台 获取更多详情，包括定价信息。关于 API 模型的性能，请参阅接下来的模型性能部分。\n模型 模型名称 描述 定价 Qwen-Plus qwen-plus-latest, qwen-plus-0919 Qwen2.5 的旗舰模型，在广泛的任务中实现了顶级性能，适用于需要高级推理和深刻理解的复杂任务。 0.0008 /0.002 Qwen-Turbo qwen-turbo-latest, qwen-turbo-0919 Qwen2.5 的均衡模型，提供快速且准确度高的响应，非常适合实时应用。 0.0003 /0.0006 Qwen-VL-Max qwen-vl-max-latest, qwen-vl-max-0919 Qwen VL 的旗舰模型， 具有优秀的图像理解和视频推理能力，可以更好地识别图片中的多语言文字和手写体的文字。 0.02 / 0.02 对于每个模型，我们提供两个不同的模型名称。第一个名称通常是带有最新模型的相对稳定的服务。然而，为了确保服务的稳定性，我们通常首先在以 -latest 结尾的模型名称上部署我们的最新模型，并且还有一个以日期（如 -0919）结尾的快照版本。在定价方面，我们提供输入和输出标记的价格（¥ / 千 tokens）\n这些模型针对不同的应用场景进行了优化，允许您选择最适合您特定需求的模型。无论您需要顶级性能、快速响应时间，还是两者之间的平衡，我们的 API 服务都能满足您的需求。\n模型性能 Qwen2.5 为了展示 Qwen2.5 的能力，我们用我们最大的开源模型 Qwen2.5-72B —— 一个拥有 720 亿参数的稠密 decoder-only 语言模型——与领先的开源模型如 Llama-3.1-70B 和 Mistral-Large-V2进行了基准测试。我们在多个基准测试中展示了经过指令调优的版本的综合结果，评估了模型的能力和人类偏好。\n除了指令微调的模型之外，我们还发现，我们的旗舰开源模型 Qwen2.5-72B 的基础语言模型性能达到了顶级水准，即便是在与 Llama-3-405B 这样更大的模型对比时也是如此。\n此外，我们将基于 API 的模型 Qwen-Plus 与领先的专有和开源模型进行了对比，包括 GPT4-o、Claude-3.5-Sonnet、Llama-3.1-405B 和 DeepSeek-V2.5。这一对比展示了 Qwen-Plus 在当前大型语言模型领域中的竞争地位。结果显示，Qwen-Plus 显著优于 DeepSeek-V2.5，并且在与 Llama-3.1-405B 的竞争中表现出了竞争力，尽管在某些方面仍不及 GPT4-o 和 Claude-3.5-Sonnet。 这次基准测试不仅突显了 Qwen-Plus 的优势，也指出了未来需要改进的地方，进一步强化了我们在大型语言模型领域持续改进和创新的承诺。\nQwen2.5 的一个重要更新是重新引入了我们的 140 亿参数和 320 亿参数模型，即 Qwen2.5-14B 和 Qwen2.5-32B。这些模型在多样化的任务中超越了同等规模或更大规模的基线模型，例如 Phi-3.5-MoE-Instruct 和 Gemma2-27B-IT。 它们在模型大小和能力之间达到了最佳平衡，提供了匹配甚至超过一些较大模型的性能。此外，我们的基于 API 的模型 Qwen2.5-Turbo 相比这两个开源模型提供了极具竞争力的性能，同时提供了成本效益高且快速的服务。\n近来也出现了明显的转向小型语言模型（SLMs）的趋势。尽管历史上小型语言模型（SLMs）的表现一直落后于大型语言模型（LLMs），但二者之间的性能差距正在迅速缩小。值得注意的是，即使是只有大约 30 亿参数的模型现在也能取得高度竞争力的结果。附带的图表显示了一个重要的趋势：在 MMLU 中得分超过 65 的新型模型正变得越来越小，这凸显了语言模型的知识密度增长速度加快。特别值得一提的是，我们的 Qwen2.5-3B 成为这一趋势的一个典型例子，它仅凭约 30 亿参数就实现了令人印象深刻的性能，展示了其相对于前辈模型的高效性和能力。\n除了在基准评估中取得的显著增强外，我们还改进了我们的后训练方法。我们的四个主要更新包括支持最长可达 8K 标记的长文本生成，大幅提升了对结构化数据的理解能力，生成结构化输出（尤其是 JSON 格式）更加可靠，并且在多样化的系统提示下的表现得到了加强，这有助于有效进行角色扮演。请查阅 LLM 博客了解如何利用这些功能的详细信息。\nQwen2.5-Coder 自从推出 CodeQwen1.5 以来，我们吸引了大量依赖该模型完成各种编程任务的用户，这些任务包括调试、回答编程相关的问题以及提供代码建议。我们最新的迭代版本 Qwen2.5-Coder 特别为编程应用而设计。在本节中，我们展示了 Qwen2.5-Coder-7B-Instruct 的性能结果，并将其与领先的开源模型进行了基准测试，其中包括那些参数量大得多的模型。\n我们认为 Qwen2.5-Coder 是您个人编程助手的优秀选择。尽管它的体积较小，但在多种编程语言和任务中，它的表现超过了众多大型语言模型，展现了其卓越的编程能力。\nQwen2.5-Math 在数学专用语言模型方面，我们上个月发布了首批模型 Qwen2-Math，而这一次，相比于 Qwen2-Math，Qwen2.5-Math 在更大规模的数学相关数据上进行了预训练，包括由 Qwen2-Math 生成的合成数据。 此外，这一次我们增加了对中文的支持，并通过赋予其进行 CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）的能力来加强其推理能力。 Qwen2.5-Math-72B-Instruct 的整体性能超越了 Qwen2-Math-72B-Instruct 和 GPT4-o，甚至是非常小的专业模型如 Qwen2.5-Math-1.5B-Instruct 也能在与大型语言模型的竞争中取得高度竞争力的表现。\n使用 Qwen2.5 最简单的方法使用过阿里云百炼平台提供的通义千问 API 来使用 阿里云百炼平台,百炼平台已经兼容 OpenAI 接口规范。\nfrom openai import OpenAI import os client = OpenAI( api_key=os.getenv(\"YOUR_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", ) completion = client.chat.completions.create( model=\"qwen-plus-latest\", messages=[ {'role': 'user', 'content': 'Tell me something about large language models.'} ] ) print(completion.choices[0].message.content) 通过 Hugging Face Transformers 库来使用，正如在model card中演示的那样:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"Qwen/Qwen2.5-7B-Instruct\" model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) tokenizer = AutoTokenizer.from_pretrained(model_name) prompt = \"Give me a short introduction to large language model.\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) generated_ids = model.generate( **model_inputs, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 要使用 vLLM 运行 Qwen2.5 并部署一个与 OpenAI API 兼容的服务，可以运行如下命令：\npython -m vllm.entrypoints.openai.api_server \\ --model Qwen/Qwen2.5-7B-Instruct 如果你使用的是vllm\u003e=0.5.3，可以使用 vllm serve 命令。然后你就可以通过 curl 来与 Qwen2.5 进行对话了：\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"Qwen/Qwen2.5-7B-Instruct\", \"messages\": [ {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"} ], \"temperature\": 0.7, \"top_p\": 0.8, \"repetition_penalty\": 1.05, \"max_tokens\": 512 }' 此外，Qwen2.5 支持 vLLM 内置的工具调用功能。此功能要求 vllm\u003e=0.6。如果您想启用此功能，请使用以下命令启动 vLLM 的 OpenAI API 兼容服务：\nvllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes 之后你可以像使用 GPT’s tool calling 那样来使用它。\nQwen2.5 同样支持 Ollama’s tool calling.你可以通过启动Ollama的OpenAI兼容服务，并以与使用GPT的工具调用相同的方式来使用它。\nQwen2.5 的聊天模板中也包含了一个工具调用模板，这意味着你可以使用 Hugging Face transformers’ tool calling support.\nvLLM / Ollama / Transformers 的工具调用支持使用受 Nous’ Hermes 的格式启发的工具调用模板。 此前Qwen-Agent 提供了使用Qwen2自己的工具调用模板的工具调用支持（这较难与vllm和Ollama集成），而 Qwen2.5 既保持了与 Qwen2 模板和 Qwen-Agent 的兼容性。\nQwen 的朋友们 💗 Qwen系列取得的成功离不开开源社区的支持！因此，衷心感谢这些老朋友和新朋友们的支持：\nFinetuning: ChatLearn, Llama-Factory, Axolotl, Firefly, Swift, XTuner, Unsloth, Liger Kernel\nQuantization: AutoGPTQ, AutoAWQ, Neural Compressor\nDeployment: vLLM, SGL, SkyPilot, TensorRT-LLM, OpenVino, TGI\nAPI Platforms: Together, Fireworks, OpenRouter\nLocal Run: MLX, Llama.cpp, Ollama, LM Studio, Jan\nAgent and RAG Frameworks: Dify, LlamaIndex, CrewAI\nEvaluation: LMSys, OpenCompass, Open LLM Leaderboard\nModel Training: Arcee AI, Sailor, Dolphin, Openbuddy\n我们要向所有为 Qwen 做出贡献的团队和个人表示衷心的感谢，即使他们没有被特别提及。你们的支持是无价的，我们热忱邀请更多的朋友加入我们这一激动人心的旅程。共同合作，我们可以推动开源 AI 社区的研究与发展，使其比以往任何时候都更加强大和充满创新。\n下一步是什么？ 虽然我们很高兴能够同时推出众多高质量的模型，但我们认识到仍然存在重大的挑战。近期的发布表明了我们在语言、视觉-语言和音频-语言领域开发强大基础模型的决心。然而，将这些不同的模态整合到单一模型中以实现三种信息的无缝端到端处理仍然是至关重要的。此外，尽管我们在通过数据扩展来增强推理能力方面已经取得了进展，但最近在强化学习（例如，o1）方面的进步也激励着我们，我们致力于通过扩展推理计算来进一步提高模型的推理能力。我们期待着很快向大家介绍下一代模型！敬请关注更多精彩进展！\nCitation 我们即将发布 Qwen2.5 的技术报告。在报告发布之前，您可以引用我们的 Qwen2 论文以及这篇博客。\n@misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} } @article{qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } ","wordCount":"621","inLanguage":"zh","datePublished":"2024-09-19T00:00:04+08:00","dateModified":"2024-09-19T00:00:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5: 基础模型大派对！</h1><div class=post-meta><span title='2024-09-19 00:00:04 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;621 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-main.jpg alt="Qwen2.5 Main Image" width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5 class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：<strong>Qwen2.5</strong>。</p><p>我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！</p><p>我们的最新发布包括了语言模型 <strong>Qwen2.5</strong>，以及专门针对编程的 <strong>Qwen2.5-Coder</strong> 和数学的 <strong>Qwen2.5-Math</strong> 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：</p><ul><li>Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B;</li><li>Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B;</li><li>Qwen2.5-Math: 1.5B, 7B, 以及72B。</li></ul><br><p>除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 <strong>Qwen-Plus</strong> 和 <strong>Qwen-Turbo</strong> 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 <strong>Qwen2-VL-72B</strong>。</p><p>如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：</p><p><a href=https://qwenlm.github.io/blog/qwen2.5-llm class="btn external" target=_blank>Qwen2.5 LLM</a>
<a href=https://qwenlm.github.io/blog/qwen2.5-coder class="btn external" target=_blank>Qwen2.5-Coder</a>
<a href=https://qwenlm.github.io/blog/qwen2.5-math class="btn external" target=_blank>Qwen2.5-Math</a></p><br><p>准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！</p><h1 id=要点总结>要点总结<a hidden class=anchor aria-hidden=true href=#要点总结>#</a></h1><p>就 <strong>Qwen2.5</strong> 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 <strong>18T</strong> tokens。相较于 Qwen2，Qwen2.5 获得了显著更多的知识（MMLU：85+），并在编程能力（HumanEval 85+）和数学能力（MATH 80+）方面有了大幅提升。此外，新模型在指令执行、生成长文本（超过 8K 标记）、理解结构化数据（例如表格）以及生成结构化输出特别是 JSON 方面取得了显著改进。
Qwen2.5 模型总体上对各种system prompt更具适应性，增强了角色扮演实现和聊天机器人的条件设置功能。与 Qwen2 类似，Qwen2.5 语言模型支持高达 <strong>128K</strong> tokens，并能生成最多 <strong>8K</strong> tokens的内容。它们同样保持了对包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等 29 种以上语言的支持。
我们在下表中提供了有关模型的基本信息。</p><p>专业领域的专家语言模型，即用于编程的 <strong>Qwen2.5-Coder</strong> 和用于数学的 <strong>Qwen2.5-Math</strong>，相比其前身 CodeQwen1.5 和 Qwen2-Math 有了实质性的改进。
具体来说，Qwen2.5-Coder 在包含 <strong>5.5 T</strong> tokens 编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。
同时，Qwen2.5-Math 支持 <strong>中文</strong> 和 <strong>英文</strong>，并整合了多种推理方法，包括CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）。</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5%20modelcard.001.jpeg alt="Qwen2.5 Specification" width=100%></figure><h1 id=api>API<a hidden class=anchor aria-hidden=true href=#api>#</a></h1><p>除了我们的开源模型之外，我们还通过 API 服务提供了更多的模型。您可以访问 <a href=https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm>阿里云百炼平台</a> 获取更多详情，包括定价信息。关于 API 模型的性能，请参阅接下来的模型性能部分。</p><table><thead><tr><th style=text-align:left;width:100px>模型</th><th style=text-align:center;width:200px>模型名称</th><th style=text-align:center;width:400px>描述</th><th style=text-align:center;width:150px>定价</th></tr></thead><tbody><tr><td style=text-align:left>Qwen-Plus</td><td style=text-align:center><code>qwen-plus-latest</code>, <code>qwen-plus-0919</code></td><td style=text-align:center>Qwen2.5 的旗舰模型，在广泛的任务中实现了顶级性能，适用于需要高级推理和深刻理解的复杂任务。</td><td style=text-align:center>0.0008 /0.002</td></tr><tr><td style=text-align:left>Qwen-Turbo</td><td style=text-align:center><code>qwen-turbo-latest</code>, <code>qwen-turbo-0919</code></td><td style=text-align:center>Qwen2.5 的均衡模型，提供快速且准确度高的响应，非常适合实时应用。</td><td style=text-align:center>0.0003 /0.0006</td></tr><tr><td style=text-align:left>Qwen-VL-Max</td><td style=text-align:center><code>qwen-vl-max-latest</code>, <code>qwen-vl-max-0919</code></td><td style=text-align:center>Qwen VL 的旗舰模型， 具有优秀的图像理解和视频推理能力，可以更好地识别图片中的多语言文字和手写体的文字。</td><td style=text-align:center>0.02 / 0.02</td></tr></tbody></table><p>对于每个模型，我们提供两个不同的模型名称。第一个名称通常是带有最新模型的相对稳定的服务。然而，为了确保服务的稳定性，我们通常首先在以 -latest 结尾的模型名称上部署我们的最新模型，并且还有一个以日期（如 -0919）结尾的快照版本。在定价方面，我们提供输入和输出标记的价格（¥ / 千 tokens）</p><p>这些模型针对不同的应用场景进行了优化，允许您选择最适合您特定需求的模型。无论您需要顶级性能、快速响应时间，还是两者之间的平衡，我们的 API 服务都能满足您的需求。</p><h1 id=模型性能>模型性能<a hidden class=anchor aria-hidden=true href=#模型性能>#</a></h1><h2 id=qwen25>Qwen2.5<a hidden class=anchor aria-hidden=true href=#qwen25>#</a></h2><p>为了展示 Qwen2.5 的能力，我们用我们最大的开源模型 <strong>Qwen2.5-72B</strong> —— 一个拥有 720 亿参数的稠密 decoder-only 语言模型——与领先的开源模型如 Llama-3.1-70B 和 Mistral-Large-V2进行了基准测试。我们在多个基准测试中展示了经过指令调优的版本的综合结果，评估了模型的能力和人类偏好。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-Instruct-Score.jpg alt="Qwen2.5-72B Instruct Performance" width=100%></figure><p>除了指令微调的模型之外，我们还发现，我们的旗舰开源模型 <strong>Qwen2.5-72B</strong> 的基础语言模型性能达到了顶级水准，即便是在与 Llama-3-405B 这样更大的模型对比时也是如此。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-base.001.jpeg alt="Qwen2.5-72B Base Model Performance" width=100%></figure><p>此外，我们将基于 API 的模型 <strong>Qwen-Plus</strong> 与领先的专有和开源模型进行了对比，包括 GPT4-o、Claude-3.5-Sonnet、Llama-3.1-405B 和 DeepSeek-V2.5。这一对比展示了 <strong>Qwen-Plus</strong> 在当前大型语言模型领域中的竞争地位。结果显示，<strong>Qwen-Plus</strong> 显著优于 DeepSeek-V2.5，并且在与 Llama-3.1-405B 的竞争中表现出了竞争力，尽管在某些方面仍不及 GPT4-o 和 Claude-3.5-Sonnet。
这次基准测试不仅突显了 Qwen-Plus 的优势，也指出了未来需要改进的地方，进一步强化了我们在大型语言模型领域持续改进和创新的承诺。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen-plus-instruct.001.jpeg alt="Qwen2.5-32B Instruct Performance" width=100%></figure><p>Qwen2.5 的一个重要更新是重新引入了我们的 140 亿参数和 320 亿参数模型，即 <strong>Qwen2.5-14B</strong> 和 <strong>Qwen2.5-32B</strong>。这些模型在多样化的任务中超越了同等规模或更大规模的基线模型，例如 Phi-3.5-MoE-Instruct 和 Gemma2-27B-IT。
它们在模型大小和能力之间达到了最佳平衡，提供了匹配甚至超过一些较大模型的性能。此外，我们的基于 API 的模型 <strong>Qwen2.5-Turbo</strong> 相比这两个开源模型提供了极具竞争力的性能，同时提供了成本效益高且快速的服务。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-32B-instruct_wturbo.001.jpeg alt="Qwen2.5-32B Instruct Performance" width=100%></figure><p>近来也出现了明显的转向小型语言模型（SLMs）的趋势。尽管历史上小型语言模型（SLMs）的表现一直落后于大型语言模型（LLMs），但二者之间的性能差距正在迅速缩小。值得注意的是，即使是只有大约 30 亿参数的模型现在也能取得高度竞争力的结果。附带的图表显示了一个重要的趋势：在 MMLU 中得分超过 65 的新型模型正变得越来越小，这凸显了语言模型的知识密度增长速度加快。特别值得一提的是，我们的 <strong>Qwen2.5-3B</strong> 成为这一趋势的一个典型例子，它仅凭约 30 亿参数就实现了令人印象深刻的性能，展示了其相对于前辈模型的高效性和能力。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-small.jpg alt="Qwen2.5 Small Model" width=100%></figure><p>除了在基准评估中取得的显著增强外，我们还改进了我们的后训练方法。我们的四个主要更新包括支持最长可达 8K 标记的长文本生成，大幅提升了对结构化数据的理解能力，生成结构化输出（尤其是 JSON 格式）更加可靠，并且在多样化的系统提示下的表现得到了加强，这有助于有效进行角色扮演。请查阅 LLM 博客了解如何利用这些功能的详细信息。</p><h2 id=qwen25-coder>Qwen2.5-Coder<a hidden class=anchor aria-hidden=true href=#qwen25-coder>#</a></h2><p>自从推出 CodeQwen1.5 以来，我们吸引了大量依赖该模型完成各种编程任务的用户，这些任务包括调试、回答编程相关的问题以及提供代码建议。我们最新的迭代版本 Qwen2.5-Coder 特别为编程应用而设计。在本节中，我们展示了 Qwen2.5-Coder-7B-Instruct 的性能结果，并将其与领先的开源模型进行了基准测试，其中包括那些参数量大得多的模型。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/coder-main.png alt="Qwen2.5-Coder Instruct Performance" width=100%></figure><p>我们认为 Qwen2.5-Coder 是您个人编程助手的优秀选择。尽管它的体积较小，但在多种编程语言和任务中，它的表现超过了众多大型语言模型，展现了其卓越的编程能力。</p><h2 id=qwen25-math>Qwen2.5-Math<a hidden class=anchor aria-hidden=true href=#qwen25-math>#</a></h2><p>在数学专用语言模型方面，我们上个月发布了首批模型 Qwen2-Math，而这一次，相比于 Qwen2-Math，Qwen2.5-Math 在更大规模的数学相关数据上进行了预训练，包括由 Qwen2-Math 生成的合成数据。
此外，这一次我们增加了对中文的支持，并通过赋予其进行 CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）的能力来加强其推理能力。
Qwen2.5-Math-72B-Instruct 的整体性能超越了 Qwen2-Math-72B-Instruct 和 GPT4-o，甚至是非常小的专业模型如 Qwen2.5-Math-1.5B-Instruct 也能在与大型语言模型的竞争中取得高度竞争力的表现。</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-allsize.png alt="Qwen2.5 Math Performance Across All Sizes" width=100%></figure><h1 id=使用-qwen25>使用 Qwen2.5<a hidden class=anchor aria-hidden=true href=#使用-qwen25>#</a></h1><p>最简单的方法使用过阿里云百炼平台提供的通义千问 API 来使用 <a href=https://www.aliyun.com/product/bailian>阿里云百炼平台</a>,百炼平台已经兼容 OpenAI 接口规范。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;YOUR_API_KEY&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-plus-latest&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;Tell me something about large language models.&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span></code></pre></div><p>通过 Hugging Face Transformers 库来使用，正如在<a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct>model card</a>中演示的那样:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language model.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>要使用 vLLM 运行 Qwen2.5 并部署一个与 OpenAI API 兼容的服务，可以运行如下命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>python</span> <span class=o>-</span><span class=n>m</span> <span class=n>vllm</span><span class=o>.</span><span class=n>entrypoints</span><span class=o>.</span><span class=n>openai</span><span class=o>.</span><span class=n>api_server</span> \
</span></span><span class=line><span class=cl>    <span class=o>--</span><span class=n>model</span> <span class=n>Qwen</span><span class=o>/</span><span class=n>Qwen2</span><span class=mf>.5</span><span class=o>-</span><span class=mi>7</span><span class=n>B</span><span class=o>-</span><span class=n>Instruct</span>
</span></span></code></pre></div><p>如果你使用的是<code>vllm>=0.5.3</code>，可以使用 <code>vllm serve</code> 命令。然后你就可以通过 <code>curl</code> 来与 Qwen2.5 进行对话了：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions -H <span class=s2>&#34;Content-Type: application/json&#34;</span> -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>  ],
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;temperature&#34;: 0.7,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;top_p&#34;: 0.8,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;repetition_penalty&#34;: 1.05,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;max_tokens&#34;: 512
</span></span></span><span class=line><span class=cl><span class=s1>}&#39;</span>
</span></span></code></pre></div><p>此外，Qwen2.5 支持 vLLM 内置的工具调用功能。此功能要求 <code>vllm>=0.6</code>。如果您想启用此功能，请使用以下命令启动 vLLM 的 OpenAI API 兼容服务：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes
</span></span></code></pre></div><p>之后你可以像使用 <a href=https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models>GPT&rsquo;s tool calling</a> 那样来使用它。</p><p>Qwen2.5 同样支持 <a href=https://ollama.com/blog/tool-support>Ollama&rsquo;s tool calling</a>.你可以通过启动Ollama的OpenAI兼容服务，并以与使用GPT的工具调用相同的方式来使用它。</p><p>Qwen2.5 的聊天模板中也包含了一个工具调用模板，这意味着你可以使用 Hugging Face <a href=https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling>transformers&rsquo; tool calling support</a>.</p><p>vLLM / Ollama / Transformers 的工具调用支持使用受 <a href=https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B>Nous&rsquo; Hermes</a> 的格式启发的工具调用模板。
此前<a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> 提供了使用Qwen2自己的工具调用模板的工具调用支持（这较难与vllm和Ollama集成），而 Qwen2.5 既保持了与 Qwen2 模板和 Qwen-Agent 的兼容性。</p><br><h1 id=qwen-的朋友们>Qwen 的朋友们<a hidden class=anchor aria-hidden=true href=#qwen-的朋友们>#</a></h1><p>💗 Qwen系列取得的成功离不开开源社区的支持！因此，衷心感谢这些老朋友和新朋友们的支持：</p><ul><li><p>Finetuning: <a href=https://github.com/alibaba/ChatLearn/>ChatLearn</a>, <a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>, <a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>, <a href=https://github.com/yangjianxin1/Firefly>Firefly</a>, <a href=https://github.com/modelscope/swift>Swift</a>, <a href=https://github.com/InternLM/xtuner>XTuner</a>, <a href=https://unsloth.ai/>Unsloth</a>, <a href=https://github.com/linkedin/Liger-Kernel>Liger Kernel</a></p></li><li><p>Quantization: <a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>, <a href=https://github.com/intel/neural-compressor>Neural Compressor</a></p></li><li><p>Deployment: <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/sgl-project/sglang>SGL</a>, <a href=https://github.com/skypilot-org/skypilot>SkyPilot</a>, <a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>, <a href=https://github.com/openvinotoolkit/openvino>OpenVino</a>, <a href=https://github.com/huggingface/text-generation-inference>TGI</a></p></li><li><p>API Platforms: <a href=https://www.together.ai/>Together</a>, <a href=https://fireworks.ai/>Fireworks</a>, <a href=https://openrouter.ai/>OpenRouter</a></p></li><li><p>Local Run: <a href=https://github.com/ml-explore/mlx>MLX</a>, <a href=https://github.com/ggerganov/llama.cpp>Llama.cpp</a>, <a href=https://ollama.com/>Ollama</a>, <a href=https://lmstudio.ai/>LM Studio</a>, <a href=https://jan.ai/>Jan</a></p></li><li><p>Agent and RAG Frameworks: <a href=https://dify.ai/>Dify</a>, <a href=https://www.llamaindex.ai/>LlamaIndex</a>, <a href=https://www.crewai.com/>CrewAI</a></p></li><li><p>Evaluation: <a href=https://chat.lmsys.org/>LMSys</a>, <a href=https://opencompass.org.cn/home>OpenCompass</a>, <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a></p></li><li><p>Model Training: <a href=https://www.arcee.ai/>Arcee AI</a>, <a href=https://sailorllm.github.io/>Sailor</a>, <a href=https://huggingface.co/cognitivecomputations>Dolphin</a>, <a href=https://github.com/OpenBuddy/OpenBuddy>Openbuddy</a></p></li></ul><p>我们要向所有为 Qwen 做出贡献的团队和个人表示衷心的感谢，即使他们没有被特别提及。你们的支持是无价的，我们热忱邀请更多的朋友加入我们这一激动人心的旅程。共同合作，我们可以推动开源 AI 社区的研究与发展，使其比以往任何时候都更加强大和充满创新。</p><h1 id=下一步是什么>下一步是什么？<a hidden class=anchor aria-hidden=true href=#下一步是什么>#</a></h1><p>虽然我们很高兴能够同时推出众多高质量的模型，但我们认识到仍然存在重大的挑战。近期的发布表明了我们在语言、视觉-语言和音频-语言领域开发强大基础模型的决心。然而，将这些不同的模态整合到单一模型中以实现三种信息的无缝端到端处理仍然是至关重要的。此外，尽管我们在通过数据扩展来增强推理能力方面已经取得了进展，但最近在强化学习（例如，o1）方面的进步也激励着我们，我们致力于通过扩展推理计算来进一步提高模型的推理能力。我们期待着很快向大家介绍下一代模型！敬请关注更多精彩进展！</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><p>我们即将发布 Qwen2.5 的技术报告。在报告发布之前，您可以引用我们的 Qwen2 论文以及这篇博客。</p><pre tabindex=0><code>@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
</code></pre><pre tabindex=0><code>@article{qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>