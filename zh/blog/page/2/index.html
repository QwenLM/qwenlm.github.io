<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=&#34;https://qwen.ai/research&#34;"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog at <a href=https://qwen.ai/research>qwen.ai</a>!</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/research"}</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Time to Speak Some Dialects, Qwen-TTS!</h2></header><div class=entry-content><p>API 简介 我们通过 Qwen API 更新了 Qwen-TTS ( qwen-tts-latest or qwen-tts-2025-05-22 ) 的最新版本。Qwen-TTS 使用了超过 300 万小时的大规模语料库进行训练，合成效果实现了人类级别的自然度和表现力。比较亮眼的是，Qwen-TTS 会根据输入文本自动调整韵律、节奏和情绪变化。此外，Qwen-TTS 支持生成三种中文方言，包括北京话、上海话和四川话。
目前，Qwen-TTS 支持七种中英双语音色，包括 Cherry、Ethan、Chelsie、Serena、Dylan（北京话）、Jada（上海话） 和 Sunny（四川话），更多语言和风格选项即将在近期推出。
中文方言样例 这里有一些样例展示了 Qwen-TTS 在中文方言上的自然生成能力。
音色
方言种类
文本
合成样例
Dylan
北京话
我们家那边后面有一个后山，就护城河那边，完了呢我们就在山上啊就其实也没什么，就是在土坡上跑来跑去，然后谁捡个那个嗯比较威风的棍，完了我们就呃得瞎打呃，要不就是什么掏个洞啊什么的。
得有自己的想法，别净跟着别人瞎起哄，多动动脑子，有点儿结构化的思维啥的。
Jada
上海话
侬只小赤佬，啊呀，数学句子错它八道题，还想吃肯德基啊！夜到麻将队三缺一啊，嘿嘿，叫阿三头来顶嘛！哦，提前上料这样产品，还要卖 300 块硬币啊。
侬来帮伊向暖吧，天光已经暗转亮哉。
Sunny
四川话
胖娃胖嘟嘟，骑马上成都，成都又好耍。胖娃骑白马，白马跳得高。胖娃耍关刀，关刀耍得圆。胖娃吃汤圆。
他一辈子的使命就是不停地爬哟，爬到大海头上去，不管有好多远！
额外结果 Qwen-TTS 生成的效果目前已经达到了人类水平，其在 SeedTTS-Eval 评测集上的指标如下：
音色
词错误率 WER (↓) 音色相似度 SIM (↑) zh
en
hard
zh
en
hard
Chelsie
1.256
2.004
6.171
0.658
0.473
0.662
Serena
1....</p></div><footer class=entry-footer><span title='2025-06-27 15:01:34 +0800 +0800'>2025年6月27日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;375 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Time to Speak Some Dialects, Qwen-TTS!" href=https://qwenlm.github.io/zh/blog/qwen-tts/></a></article><article class=post-entry><header class=entry-header><h2>Qwen VLo: 从“看懂”世界到“描绘”世界</h2></header><div class=entry-content><p>QWEN CHAT DISCORD
介绍 多模态大模型的演进正在不断突破我们对技术边界的认知。从最初的 QwenVL 到如今的 Qwen2.5 VL ，我们在提升模型对图像内容的理解能力方面取得了一些进展。今天，我们正式推出 Qwen VLo ——一个多模态统一理解与生成模型。这一全新升级的模型不仅能够“看懂”世界，更能基于理解进行高质量的再创造，真正实现了从感知到生成的跨越。需要注意的是，这是一款预览版本，您可以通过 Qwen Chat 访问它。您可以直接发送类似“生成一张可爱猫咪的图片”的提示来生成图像，或者上传一张猫咪的图片并要求“给猫咪头上加顶帽子”来修改图像。图像的生成过程如下所示:
生成过程：发挥你想象力，将你的想法变成现实
正如视频中展示的生成过程，Qwen VLo 以一种渐进式生成方式，从左到右、从上到下逐步清晰地构建整幅图片。在生成过程中，模型会对预测的内容不断调整和优化，从而确保最终结果更加和谐一致。这种生成机制不仅提升了视觉效果，还为用户带来了更灵活、更可控的创作体验。
从理解到创造：更精准的多模态生成能力 Qwen VLo在原始多模态理解与生成能力上进行了全面升级，显著增强了对图像内容的理解深度，并在此基础上实现了更加准确和一致的生成效果。以下是 Qwen VLo 的核心亮点：
更精准的内容理解与再创造 以往的多模态模型在生成过程中容易出现语义不一致的问题，例如将汽车误生成其他类型的物体，或者无法保留原图的关键结构特征。而 Qwen VLo 通过更强大的细节捕捉能力，能够在生成过程中保持高度的语义一致性。例如，当用户输入一张汽车的照片并要求“更换颜色”时，Qwen VLo 不仅能准确识别车型，还能保留其原有的结构特征，同时完成色彩风格的自然转换，让生成结果既符合预期又不失真实感。
支持开放指令编辑修改生成 用户可以通过自然语言提出各种创意性指令，如“将这张画风改为梵高风格”、“让这张照片看起来像19世纪的老照片”或“给这张图片添加一个晴朗的天空”。Qwen VLo 能够灵活响应这些开放性指令，并生成符合用户预期的结果。无论是艺术风格迁移、场景重构还是细节修饰，模型都能轻松应对。甚至一些传统的视觉感知人物如预测深度图、分割图、检测图以及边缘信息等也可以通过编辑指令轻松完成。更进一步，像很多更复杂的指令，比如一条指令中同时包含修改物体、修改文字、更换背景，模型也能轻松完成。
多语言指令支持 Qwen VLo 支持包括中文、英文在内的多种语言指令，打破了语言壁垒，为全球用户提供了统一且便捷的交互体验。无论您使用哪种语言，只需简单描述您的需求，模型便能快速理解并输出理想结果。
样例 Qwen VLo 更像一个人类画师, 根据自己的理解再进行创作. 下面是一些具体的例子。
Qwen VLo 能够直接生成图像，并对其进行修改，例如替换背景、添加主体、进行风格迁移，甚至可以完成基于开放指令的大幅修改，包括检测和分割等视觉感知任务。
A cute Shiba Inu
Next
User
生成一个可爱的柴犬
Translation: Generate a cute Shiba Inu
Qwen-VLo
User
背景改成草原
Translation: Change the background to a grassland...</p></div><footer class=entry-footer><span title='2025-06-26 22:00:04 +0800 +0800'>2025年6月26日</span>&nbsp;·&nbsp;11 分钟&nbsp;·&nbsp;2199 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen VLo: 从“看懂”世界到“描绘”世界" href=https://qwenlm.github.io/zh/blog/qwen-vlo/></a></article><article class=post-entry><header class=entry-header><h2>Qwen3 Embedding：新一代文本表征与排序模型</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
我们正式发布 Qwen3 Embedding 系列模型, Qwen 模型家族的新成员。该系列模型专为文本表征、检索与排序任务设计，基于 Qwen3 基础模型进行训练，充分继承了 Qwen3 在多语言文本理解能力方面的优势。在多项基准测试中，Qwen3 Embedding 系列在文本表征和排序任务中展现了卓越的性能。我们使用了 Apache 2.0 协议在 Hugging Face 和 ModelScope 上开源了这一系列的文本表征及排序模型，并在 GitHub 公布了技术报告及相关代码。
排序模型评测结果
Model Param MTEB-R CMTEB-R MMTEB-R MLDR MTEB-Code FollowIR Qwen3-Embedding-0.6B 0.6B 61.82 71.02 64.64 50.26 75.41 5.09 Jina-multilingual-reranker-v2-base 0.3B 58.22 63.37 63.73 39.66 58.98 -0.68 gte-multilingual-reranker-base 0.3B 59.51 74.08 59.44 66.33 54.18 -1.64 BGE-reranker-v2-m3 0.6B 57.03 72.16 58.36 59.51 41.38 -0.01 Qwen3-Reranker-0.6B 0....</p></div><footer class=entry-footer><span title='2025-06-05 21:00:00 +0800 +0800'>2025年6月5日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;251 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen3 Embedding：新一代文本表征与排序模型" href=https://qwenlm.github.io/zh/blog/qwen3-embedding/></a></article><article class=post-entry><header class=entry-header><h2>Qwen3：思深，行速</h2></header><div class=entry-content><p>QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD
引言 今天，我们宣布推出 Qwen3，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型 Qwen3-235B-A22B 在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 Qwen3-30B-A3B 的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。
我们开源了两个 MoE 模型的权重：Qwen3-235B-A22B，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及Qwen3-30B-A3B，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括 Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B 和 Qwen3-0.6B，均在 Apache 2.0 许可下开源。
Models Layers Heads (Q / KV) Tie Embedding Context Length Qwen3-0.6B 28 16 / 8 Yes 32K Qwen3-1....</p></div><footer class=entry-footer><span title='2025-04-29 04:00:00 +0800 +0800'>2025年4月29日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;794 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen3：思深，行速" href=https://qwenlm.github.io/zh/blog/qwen3/></a></article><article class=post-entry><header class=entry-header><h2>QVQ-Max：有依据地思考</h2></header><div class=entry-content><p>QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。
MathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。
接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。
为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。
举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。
我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。
核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。
细致观察：抓住每一个细节
QVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。
深入推理：不只是“看到”，还要“想到”
仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。
灵活应用：从解答问题到创作
除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。
样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。
职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image Recognition
Next
QVQ-Max-Preview
Mathematical Reasoning
Next
QVQ-Max-Preview
Interpreting Palm Readings (For Reference Only)
Next
QVQ-Max-Preview
Video Understanding
Next
QVQ-Max-Preview
Learn to code by watching videos
Next
QVQ-Max-Preview
下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向：...</p></div><footer class=entry-footer><span title='2025-03-28 00:00:04 +0800 +0800'>2025年3月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;92 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to QVQ-Max：有依据地思考" href=https://qwenlm.github.io/zh/blog/qvq-max-preview/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://qwenlm.github.io/zh/blog/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://qwenlm.github.io/zh/blog/page/3/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>