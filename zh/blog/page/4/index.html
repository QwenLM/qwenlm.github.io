<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.25451dd4678157e0fb2e84a2fba5ad7861ab458e1168319a052575d04324b785.css integrity="sha256-JUUd1GeBV+D7LoSi+6WteGGrRY4RaDGaBSV10EMkt4U=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Qwen2-Audio：开启语音对话！</h2></header><div class=entry-content><p>DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
在一个通用的AI系统中，核心模型应该能够理解不同模态的信息。当前的大语言模型现在已经能够理解语言并进行推理，并且已经扩展到了更多的模态，包括视觉和音频。此前我们陆续发布了多个 Qwen 语言模型系列以及 Qwen-VL 和 Qwen-Audio 等多模态模型。今天，我们正式发布 Qwen2-Audio。这是 Qwen-Audio 的下一代版本，它能够接受音频和文本输入，并生成文本输出。Qwen2-Audio 具有以下特点：
语音聊天：用户可以使用语音向音频语言模型发出指令，无需通过自动语音识别（ASR）模块。
音频分析：该模型能够根据文本指令分析音频信息，包括语音、声音、音乐等。
多语言支持：该模型支持超过8种语言和方言，例如中文、英语、粤语、法语、意大利语、西班牙语、德语和日语。
我们在 Hugging Face 和 ModelScope 上开源了 Qwen2-Audio-7B 以及 Qwen2-Audio-7B-Instruct，并且我们搭建了一个 demo 供用户互动。下面是一些具体例子：
语音交互 语音交互：基础交互
Next
User
Your browser does not support the audio element. Qwen2-Audio
这种情况可能会让人感到沮丧和困惑，但是你需要记住的是，你不能强迫任何人去喜欢你。如果你真的喜欢这个女孩，那么最好的办法就是尊重她的感受，不去打扰她，同时也要保持自己的尊严。 以下是一些建议：   1. 尊重她的决定：如果她明确表示不喜欢你，那么你应该尊重她的决定，不要试图改变她的想法。 2. 不要过于干涉：避免过多地干涉她的生活，也不要试图通过各种方式来接近她。 3. 保持距离：尽量避免与她有太多的接触，尤其是在她不希望的情况下。 4. 寻求支持：如果你感到很难过或者困惑，可以向你的朋友或家人寻求支持和建议。   最重要的是，你要保持自己的尊严和自信，不要因为别人的喜好而失去自我。 Your browser does not support the audio element. User
Your browser does not support the audio element....</p></div><footer class=entry-footer><span title='2024-08-09 16:22:39 +0800 +0800'>2024年8月9日</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;1058 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-Audio：开启语音对话！" href=https://qwenlm.github.io/zh/blog/qwen2-audio/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-Math，新一代数学模型</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
🚨 此模型目前主要支持英语。我们将尽快推出中英双语版本。 简介 在过去的一年里，我们非常关注大模型的推理能力的提升，尤其关注其在数学相关的任务上的表现。今天，我们非常高兴地介绍 Qwen2 开源家族的新成员——Qwen2-Math-1.5B/7B/72B 系列。Qwen2-Math 是一系列基于 Qwen2 LLM 构建的专门用于数学解题的语言模型，其数学能力显著超越了开源模型，甚至超过了闭源模型（如 GPT-4o）。我们希望Qwen2-Math能够为科学界解决需要复杂多步逻辑推理的高级数学问题做出贡献。
我们在一系列数学基准评测上评估了我们的数学专用模型 Qwen2-Math。在 Math 上的评测结果表明，我们最大的数学专用模型 Qwen2-Math-72B-Instruct 超越了最先进的模型，包括 GPT-4o、Claude-3.5-Sonnet、Gemini-1.5-Pro 和 Llama-3.1-405B。
Qwen2-Math基础模型 Qwen2-Math 的基础模型使用 Qwen2-1.5B/7B/72B 进行初始化，然后在精心设计的数学专用语料库上进行预训练，该语料库包含大规模高质量的数学网络文本、书籍、代码、考试题目以及由 Qwen2 模型合成的数学预训练数据。
我们在三个广泛使用的英语数学基准 GSM8K、Math 和 MMLU-STEM 上评估了我们的 Qwen2-Math 基模型。此外，我们还评估了三个中国数学基准 CMATH，GaoKao Math Cloze 和 GaoKao Math QA。所有评估均使用 Few-shot CoT 方式。
Qwen2-Math指令微调模型 我们首先基于 Qwen2-Math-72B 训练了一个数学专用的奖励模型。然后，我们将这个密集的奖励信号与一个二元信号结合，该二元信号指示模型是否正确回答了问题。这个组合信号被用作监督来通过拒绝采样构建 SFT 数据，并在此SFT模型的基础上进一步使用 GRPO 来优化模型。
我们对 Qwen2-Math-Instruct 在英语和中文的数学基准评测上进行了评估。除了常用的基准评测，如 GSM8K 和 MATH 之外，我们还加入了更具挑战性的考试以全面检测 Qwen2-Math-Instruct 的能力，例如 OlympiadBench、CollegeMath、高考（GaoKao）、AIME2024 以及 AMC2023。对于中文的数学评测集，我们使用了 CMATH、2024年中国高考数学题以及2024年中国中考数学题。...</p></div><footer class=entry-footer><span title='2024-08-08 00:00:00 +0800 +0800'>2024年8月8日</span>&nbsp;·&nbsp;25 分钟&nbsp;·&nbsp;5159 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-Math，新一代数学模型" href=https://qwenlm.github.io/zh/blog/qwen2-math/></a></article><article class=post-entry><header class=entry-header><h2>你好，Qwen2</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：
5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！
模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:
模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。
上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。
我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：
地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。...</p></div><footer class=entry-footer><span title='2024-06-07 00:00:00 +0800 +0800'>2024年6月7日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1599 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 你好，Qwen2" href=https://qwenlm.github.io/zh/blog/qwen2/></a></article><article class=post-entry><header class=entry-header><h2>使用Qwen-Agent将上下文记忆扩展到百万量级</h2></header><div class=entry-content><p>我们开发了一个智能体用于理解包含百万字词的文档，虽然仅使用Qwen2模型的8k上下文，但效果超过RAG和长序列原生模型。我们还利用此智能体合成长上下文数据，用于训练长上下文的Qwen模型。</p></div><footer class=entry-footer><span title='2024-06-06 11:59:59 +0800 +0800'>2024年6月6日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;106 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 使用Qwen-Agent将上下文记忆扩展到百万量级" href=https://qwenlm.github.io/zh/blog/qwen-agent-2405/></a></article><article class=post-entry><header class=entry-header><h2>Qwen-Max-0428模型介绍</h2></header><div class=entry-content><p>API DEMO DISCORD
此前，我们开源了Qwen1.5系列的模型，参数规模最小至5亿，最大至1100亿。这一次，我们推出更大规模模型Qwen-Max-0428（通义千问网页端及APP产品版本从2.1升级至2.5）。Qwen-Max-0428是经过指令微调的Chat模型。近期该模型登陆了Chatbot Arena，并登榜前十。此外，我们在MT-Bench的评测上也观察到该模型的表现显著优于Qwen1.5-110B-Chat。
Models MT-Bench Arena Qwen1.5-110B-Chat 8.88 1172 Qwen-Max-0428 8.96 1186 我们也在Hugging Face上提供了Demo服务（链接）：
同时我们也提供了DashScope API服务（链接）。目前API服务已经支持OpenAI API格式，示例如下所示：
from openai import OpenAI client = OpenAI( api_key="$your-dashscope-api-key", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1" ) completion = client.chat.completions.create( model="qwen-max", messages=[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Tell me something about large language models.'}] ) print(completion.choices[0].message) 此外，Qwen-Max-0428已上线通义千问网页端及APP。欢迎体验！
引用 @misc{qwen1.5, title = {Introducing Qwen1.5}, url = {https://qwenlm.github.io/blog/qwen1.5/}, author = {Qwen Team}, month = {February}, year = {2024} }</p></div><footer class=entry-footer><span title='2024-05-11 18:10:00 +0800 +0800'>2024年5月11日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;74 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen-Max-0428模型介绍" href=https://qwenlm.github.io/zh/blog/qwen-max-0428/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://qwenlm.github.io/zh/blog/page/3/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://qwenlm.github.io/zh/blog/page/5/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>