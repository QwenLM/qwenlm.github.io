<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>与 CodeQwen1.5 结对编程</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 代码助手，是一种基于 LLMs 的智能化的编程工具，它可以帮助程序员更高效、更准确的编写代码，使得整个软件开发过程更加流畅和高效。然而流行的代码助手，比如 Github Copilot，依赖于闭源的商业模型，不仅昂贵还会引起如隐私、安全、版权等方面的担忧。幸运的是，开源社区正在致力于打造开放代码模型来实现开放的代码助手。近期涌现出了一批优秀的 Open CodeLLMs，比如 StarCoder2、CodeLlama、DeepSeek-Coder 等，提供了一条新的路径，但仍然值得探索。
今天，我们非常激动地和大家介绍来自 Qwen1.5 开源家族的新成员，一个代码专家模型 CodeQwen1.5! CodeQwen1.5 基于 Qwen 语言模型初始化，拥有 7B 参数的模型，其拥有 GQA 架构，经过了 ~3T tokens 代码相关的数据进行预训练，共计支持 92 种编程语言、且最长支持 64K 的上下文输入。效果方面，CodeQwen1.5 展现出了非凡的代码生成、长序列建模、代码修改、SQL 能力等,该模型可以大大提高开发人员的工作效率，并在不同的技术环境中简化软件开发工作流程。
CodeQwen 是基础的 Coder 代码生成是大语言模型的关键能力之一，期待模型将自然语言指令转换为具有精确的、可执行的代码。仅拥有 70 亿参数的 CodeQwen1.5 在基础代码生成能力上已经超过了更尺寸的模型，进一步缩小了开源代码 LLM 和 GPT-4 之间的编码能力差距。我们对 HumanEval 和 MBPP 进行了评估，下面是具体的比较。
Model Size HumanEval 0-shot HumanEval+ 0-shot MBPP 0-shot MBPP+ 0-shot MBPP 3-shot Base Model CodeLlama-Base 7B 33.5 25....</p></div><footer class=entry-footer><span title='2024-04-16 13:33:00 +0800 +0800'>2024年4月16日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;296 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 与 CodeQwen1.5 结对编程" href=https://qwenlm.github.io/zh/blog/codeqwen1.5/></a></article><article class=post-entry><header class=entry-header><h2>Qwen1.5-32B：Qwen1.5语言模型系列的最后一块拼图</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 开源社区长期以来一直在寻求一种能在性能、效率和内存占用之间达到理想平衡的模型。尽管出现了诸如Qwen1.5-72B和DBRX这样的SOTA模型，但这些模型持续面临诸如内存消耗巨大、推理速度缓慢以及显著的微调成本等问题。当前，参数量约30B的模型往往在这方面被看好，得到很多用户的青睐。顺应这一趋势，我们推出Qwen1.5语言模型系列的最新成员：Qwen1.5-32B和Qwen1.5-32B-Chat。
过去数月中，我们精心研发了Qwen1.5-32B基础模型，旨在对标甚至超越当前最先进的30B模型所设定的性能基准。同时，我们在对齐方面取得了进展，特别是在RLHF方面，以提升Qwen1.5-32B-Chat的对话能力。
模型效果 Qwen1.5-32B 是 Qwen1.5 语言模型系列的最新成员，除了模型大小外，其在模型架构上除了GQA几乎无其他差异。GQA能让该模型在模型服务时具有更高的推理效率潜力。
以下我们将对比展示其与参数量约为30B或更大的当前最优（SOTA）模型在基础能力评估、chat评估以及多语言评估方面的性能。以下是对于基础语言模型能力的评估结果：
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU Llama2-34B 62.6 - 42.2 6.2 22.6 33.0 44.1 - Yi-34B 76.3 81.4 67.2 14.4 23.2 41.0 54.3 83.7 Mixtral-8x7B 70.6 - 74.4 28.4 40.2 60.7 - - Qwen1.5-72B 77.5 84.1 79.5 34.1 41.5 53.4 65.5 83.5 Qwen1.5-32B 73.4 83.5 77.4 36.1 37.2 49.4 66.8 82.3 我们的32B模型在多种任务上展现出颇具竞争力的表现，涵盖MMLU、GSM8K、HumanEval以及BBH等。相较于72B参数模型，Qwen1.5-32B虽在性能上有轻微下降，但在多数任务中仍优于其他30B级别模型，如Llama2-34B和Mixtral-8x7B。...</p></div><footer class=entry-footer><span title='2024-04-02 13:33:00 +0800 +0800'>2024年4月2日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;139 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen1.5-32B：Qwen1.5语言模型系列的最后一块拼图" href=https://qwenlm.github.io/zh/blog/qwen1.5-32b/></a></article><article class=post-entry><header class=entry-header><h2>Qwen1.5-MoE: 1/3的激活参数量达到7B模型的性能</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
介绍 今天，我们推出Qwen系列的首个MoE模型，Qwen1.5-MoE-A2.7B。它仅拥有27亿个激活参数，但其性能却能与当前最先进的70亿参数模型，如Mistral 7B和Qwen1.5-7B相媲美。相较于包含65亿个Non-Embedding参数的Qwen1.5-7B，Qwen1.5-MoE-A2.7B只有20亿个Non-Embedding参数，约为原模型大小的三分之一。此外，相比Qwen1.5-7B，Qwen1.5-MoE-A2.7B的训练成本降低了75%，推理速度则提升至1.74倍。
模型结构 我们在Qwen1.5-MoE模型中采用了特别设计的MoE架构。通常情况下，如Mixtral等方法所示，每个transformer block中的MoE层会配备8个expert，并采用top-2门控策略进行routing。这种配置还存在很大的优化空间。我们对这一架构进行了多项改进：
Finegrained experts 初始化 新的routing机制 DeepSeek-MoE和DBRX已经证明了finegrained experts的有效性。从FFN层过渡到MoE层时，我们一般只是简单地复制多次FFN来实现多个expert。而finegrained experts的目标是在不增加参数数量的前提下生成更多expert。为了实现这一点，我们将单个FFN分割成几个部分，每个部分作为一个独立的expert。我们设计了具有总共64个expert的的MoE，对比其他配置，我们认为这个实现能达到效果和效率的最优。
模型初始化阶段至关重要。初步实验表明，从零开始训练MoE模型可能效率低下，且难以提升至预期的最优性能水平。因此，我们首先利用已有的Qwen-1.8B，将其改造为Qwen1.5-MoE-A2.7B。此外，在初始化阶段引入随机性可以显著加快收敛速度，并在整个预训练过程中带来更好的整体性能表现。
目前，一个明显的趋势是在MoE中实现共享expert与routing expert。从更宏观的角度看，这是一种广义的routing方法，因为在没有共享expert的情况下，实际上就退化为传统的MoE路由设置。对于Qwen1.5-MoE-A2.7B模型，我们在其中整合了4个总是被激活的共享expert和每次只激活其中4个的60个routing expert。这种方式非常灵活，同时在我们实验中效率最佳。
性能 为了全面评估和展示Qwen1.5-MoE-A2.7B的能力和优势，我们对base模型和chat模型进行了评估。对于base模型，我们在MMLU、GSM8K和HumanEval评估了其语言理解、数学和代码能力。此外，为了评估其多语言能力，我们按照Qwen1.5的评测方法在数学、理解、考试和翻译等多个领域的多语言基准测试中进行了测试，并在"Multilingual"列中给出了综合得分。对于chat模型，我们没有使用传统的基准测试，而是使用MT-Bench进行了测试。
在这个比较分析中，我们将Qwen1.5-MoE-A2.7B与最好的7B模型，比如Mistral-7B（base模型为v0.1，chat模型为v0.2）、Gemma-7B以及Qwen1.5-7B进行了对比。此外，我们还将其与具有相似参数数量的MoE模型DeepSeekMoE 16B进行了比较。结果如下表所示：
Model MMLU GSM8K HumanEval Multilingual MT-Bench Mistral-7B 64.1 47.5 27.4 40.0 7.60 Gemma-7B 64.6 50.9 32.3 - - Qwen1.5-7B 61.0 62.5 36.0 45.2 7.60 DeepSeekMoE 16B 45.0 18.8 26.8 - 6.93 Qwen1.5-MoE-A2.7B 62.5 61.5 34.2 40.8 7.17 Qwen1.5-MoE-A2.7B在与最佳的7B模型相比取得了非常接近的性能。然而，我们发现在chat模型方面仍有改进的空间。我们将继续研究如何更加有效地微调MoE模型。
训练成本与推理效率 MoE模型的训练成本与dense模型存在显著差异。尽管MoE模型通常拥有更多的参数，但由于其稀疏性，训练开销可以显著降低。我们先对比各个模型的三个关键参数，分别是总参数数量、激活参数数量和Non-embedding参数：
Model #Parameters #(Activated) Parameters #(Activated) Non-embedding parameters Mistral-7B 7....</p></div><footer class=entry-footer><span title='2024-03-28 11:31:44 +0800 +0800'>2024年3月28日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;274 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen1.5-MoE: 1/3的激活参数量达到7B模型的性能" href=https://qwenlm.github.io/zh/blog/qwen-moe/></a></article><article class=post-entry><header class=entry-header><h2>Qwen1.5 介绍</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。
此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers>=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46....</p></div><footer class=entry-footer><span title='2024-02-04 13:33:00 +0800 +0800'>2024年2月4日</span>&nbsp;·&nbsp;7 分钟&nbsp;·&nbsp;1296 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen1.5 介绍" href=https://qwenlm.github.io/zh/blog/qwen1.5/></a></article><article class=post-entry><header class=entry-header><h2>Qwen-VL全新升级！</h2></header><div class=entry-content><p>我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79....</p></div><footer class=entry-footer><span title='2024-01-25 13:33:00 +0800 +0800'>2024年1月25日</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;1715 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen-VL全新升级！" href=https://qwenlm.github.io/zh/blog/qwen-vl/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://qwenlm.github.io/zh/blog/page/3/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://qwenlm.github.io/zh/blog/page/5/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>