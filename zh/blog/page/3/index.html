<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | Qwen</title><meta name=keywords content><meta name=description content="Blog - Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://qwenlm.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型</h2></header><div class=entry-content><p>Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
简介 两个月前，我们升级了 Qwen2.5-Turbo，使其支持最多一百万个Tokens的上下文长度。今天，我们正式推出开源的 Qwen2.5-1M 模型及其对应的推理框架支持。以下是本次发布的亮点：
开源模型： 我们发布了两个新的开源模型，分别是 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M，这是我们首次将开源 Qwen 模型的上下文扩展到 1M 长度。
推理框架： 为了帮助开发者更高效地部署 Qwen2.5-1M 系列模型，我们完全开源了基于 vLLM 的推理框架，并集成了稀疏注意力方法。该框架在处理 1M 长度输入时的速度能够提升 3倍到7倍。
技术报告： 我们还分享了 Qwen2.5-1M 系列背后的技术细节，包括训练和推理框架的设计思路以及消融实验的结果。
现在，你可以访问我们在 Huggingface 和 Modelscope 上的在线演示来体验 Qwen2.5-1M 模型。
另外，我们最近也推出了 Qwen Chat ，一个基于 Qwen 系列的 AI 助手。你可以与他对话、编程、生成图像与视频，使用搜索以及调用工具等功能。你也可以在 Qwen Chat 中与使用上下文长度同样为 1M 的 Qwen2.5-Turbo 模型进行长序列处理。
模型性能 首先，让我们来看看 Qwen2.5-1M 系列模型在长上下文任务和短文本任务中的性能表现。
长上下文任务 在上下文长度为100万 Tokens 的大海捞针（Passkey Retrieval）任务中，Qwen2.5-1M 系列模型能够准确地从 1M 长度的文档中检索出隐藏信息，其中仅有7B模型出现了少量错误。...</p></div><footer class=entry-footer><span title='2025-01-27 00:00:03 +0800 +0800'>2025年1月27日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;501 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-1M: 支持100万Token上下文的开源Qwen模型" href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5 VL！Qwen2.5 VL！Qwen2.5 VL！</h2></header><div class=entry-content><p>QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
我们发布了 Qwen2.5-VL，Qwen 模型家族的旗舰视觉语言模型，对比此前发布的 Qwen2-VL 实现了巨大的飞跃。欢迎访问 Qwen Chat 并选择 Qwen2.5-VL-72B-Instruct 进行体验。此外，我们在 Hugging Face 和 ModelScope 上开源了 Qwen2.5-VL 的 Base 和 Instruct 模型，包含 3B、7B 和 72B 在内的 3 个模型尺寸。
Qwen2.5-VL 的主要特点如下所示：
感知更丰富的世界：Qwen2.5-VL 不仅擅长识别常见物体，如花、鸟、鱼和昆虫，还能够分析图像中的文本、图表、图标、图形和布局。
Agent：Qwen2.5-VL 直接作为一个视觉 Agent，可以推理并动态地使用工具，初步具备了使用电脑和使用手机的能力。
理解长视频和捕捉事件：Qwen2.5-VL 能够理解超过 1 小时的视频，并且这次它具备了通过精准定位相关视频片段来捕捉事件的新能力。
视觉定位：Qwen2.5-VL 可以通过生成 bounding boxes 或者 points 来准确定位图像中的物体，并能够为坐标和属性提供稳定的 JSON 输出。
结构化输出：对于发票、表单、表格等数据，Qwen2.5-VL 支持其内容的结构化输出，惠及金融、商业等领域的应用。
模型性能 我们对视觉语言模型进行了全面的评估，比较了 SOTA 模型以及同尺寸规模模型中表现最好的模型。在旗舰模型 Qwen2.5-VL-72B-Instruct 的测试中，它在一系列涵盖多个领域和任务的基准测试中表现出色，包括大学水平的问题、数学、文档理解、视觉问答、视频理解和视觉 Agent。值得注意的是，Qwen2.5-VL 在理解文档和图表方面具有显著优势，并且能够作为视觉 Agent 进行操作，而无需特定任务的微调。
在较小的模型方面，Qwen2.5-VL-7B-Instruct 在多个任务中超越了 GPT-4o-mini，而 Qwen2....</p></div><footer class=entry-footer><span title='2025-01-26 19:08:41 +0800 +0800'>2025年1月26日</span>&nbsp;·&nbsp;17 分钟&nbsp;·&nbsp;3608 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5 VL！Qwen2.5 VL！Qwen2.5 VL！" href=https://qwenlm.github.io/zh/blog/qwen2.5-vl/></a></article><article class=post-entry><header class=entry-header><h2>通过全局负载均衡提升混合专家模型的性能和特异化程度</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
引言 混合专家模型（MoEs）通过路由机制动态并稀疏地激活模型参数，使得能高效地增大模型参数规模。基于 TopK 机制的稀疏激活会在训练中会遇到专家激活不均衡的问题：少数被频繁选择的专家会被优化得更多，进一步使得这些专家被更频繁地选择，最终导致只选择少数专家，造成剩余专家的冗余。因此，MoE 在训练中需要引入额外的辅助损失（load balance loss，LBL）来鼓励专家的选择趋于均衡。
目前主流 MoE 训练框架中 LBL 实现其实是局部的负载均衡，这使得模型需要将局部的输入都均匀分配给不同的专家。然而，局部的输入往往只来自个别领域，局部负载均衡会让模型将所有领域的输入都均匀分配。这种均匀分配会阻碍某些专家更多处理特定领域的数据，也即阻碍专家出现领域层次的分化特征。我们发现，将局部的负载均衡放松到全局的负载均衡，能显著增强专家的特异化并提高模型性能。
从局部均衡到全局均衡 LBL 的计算公式为 $N_E \sum_{i=1}^{N_E} f_ip_i$ ，其中 $N_E$ 为专家数， $f_i$ 为专家 i 被选择的频率， $p_i$ 为路由赋予专家 i 的平均分数。目前 Megatron-mcore 等主流框架中的 LBL 都是在每一张卡上统计计算后再全局平均，这使得 $f_i$ 也是在局部统计，而优化 LBL 也鼓励模型将每个局部的输入都均匀分配给所有的专家。这也解释了为什么目前大部分 MoE 工作没有观察到领域层面的专家分化。
得益于 LBL 计算的格式，我们可以通过通信不同节点的 $f_i$ 来将局部的 LBL 转化为全局的 LBL。因为 $f_i$ 只是一个专家数大小的向量，即使是在全局通信的情况下也不会带来明显的开销。此外由于 LBL 的计算与模型其它部分的计算相对独立，还可以用计算掩盖等策略进一步消除同步 $f_i$ 的通信开销。
扩大均衡的范围带来稳定的提升 我们在三种参数规模（3.4B 激活 0.6B, 15B 激活 2.54B，43B 激活 6.6B）下分别训练了 120B 和 400B tokens，对比了不同的均衡范围（Balance BSZ）对模型性能的影响。所有模型都使用了细粒度专家、共享专家及 dropless 策略（专家不会抛弃超过容量的tokens）。可以看到，将均衡范围从一般框架实现的 4，8 或者 16 增大到 128 以上后模型在 Benchmark 指标和 PPL 都有明显提升。...</p></div><footer class=entry-footer><span title='2025-01-21 00:00:03 +0800 +0800'>2025年1月21日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;181 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 通过全局负载均衡提升混合专家模型的性能和特异化程度" href=https://qwenlm.github.io/zh/blog/global-load-balance/></a></article><article class=post-entry><header class=entry-header><h2>面向有效的数学推理过程监督</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。
过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。
今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。
ProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。
Process Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。
Best-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。
如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。
ProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。
结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。
引用 如果你觉得我们的工作有用，欢迎引用！
@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412....</p></div><footer class=entry-footer><span title='2025-01-14 00:00:03 +0800 +0800'>2025年1月14日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;143 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 面向有效的数学推理过程监督" href=https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/></a></article><article class=post-entry><header class=entry-header><h2>QVQ: 更睿智地看世界</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD
在人类的思维中，语言和视觉紧密交织，塑造着我们感知和理解世界的方式。我们的推理能力深深植根于语言思维和视觉记忆之中。那么，当我们将这些能力赋予人工智能时，会发生什么呢？如今的大语言模型已经展现出卓越的推理能力，但我们不禁思考：它们能否通过掌握视觉理解的力量，攀登认知能力的新高峰？
设想一下，一个人工智能能够像物理学大师一样，面对复杂的物理问题，沉着冷静地通过逻辑推理找到解决方案。正是这样的愿景激发我们创造了 QVQ —— 一个基于 Qwen2-VL-72B 构建的开源多模态推理模型。QVQ 在人工智能的视觉理解和复杂问题解决能力方面实现了重大突破。在 MMMU 评测中，QVQ 取得了 70.3 的优异成绩，并且在各项数学相关基准测试中相比 Qwen2-VL-72B-Instruct 都有显著提升。通过细致的逐步推理，QVQ 在视觉推理任务中展现出增强的能力，尤其在需要复杂分析思维的领域表现出色。
局限性 QVQ-72B-Preview 是由 Qwen 团队开发的实验性研究模型，专注于增强视觉推理能力。尽管它的表现超出了预期，但仍有几个限制需要注意：
语言混合与切换：模型可能会意外地混合语言或在语言之间切换，从而影响响应的清晰度。 递归推理：模型可能会陷入循环逻辑模式，产生冗长的响应而无法得出结论。 安全和伦理考虑：模型需要增强安全措施，以确保可靠和安全的性能，用户在部署时应保持谨慎。 性能和基准限制：尽管模型在视觉推理方面有所改善，但它无法完全替代 Qwen2-VL-72B 的能力。此外，在多步骤视觉推理过程中，模型可能会逐渐失去对图像内容的关注，导致幻觉。 模型表现 我们在 4 个数据集上评估 QVQ-72B-Preview，包括：
MMMU：一个大学级别的多学科多模态评测集，旨在考察模型视觉相关的综合理解和推理能力。 MathVista：一个数学相关的视觉推理测试集，评估拼图测试图形的逻辑推理、函数图的代数推理和学术论文图形的科学推理等能力。 MathVision：一个高质量多模态数学推理测试集，来自于真实的数学竞赛，相比于MathVista具有更多的问题多样性和学科广度。 OlympiadBench：一个奥林匹克竞赛级别的双语多模态科学基准测试集，包含来自奥林匹克数学和物理竞赛的8,476个问题，包括中国高考。每个问题都附有专家级别的注释，详细说明了逐步推理的过程。 QVQ-72B-Preview 在 MMMU 基准测试中取得了 70.3 的分数，显著超越了 Qwen2-VL-72B-Instruct。此外，在剩下的三个专注于数学和科学问题的基准测试中，该模型表现出色，有效缩小了与领先的最先进的 o1 模型之间的差距。
示例 以下，我们将展示几个示例，以说明该新模型在视觉推理任务中的应用。
Example: Math
Next
User
Given $h(x) = f(x) \cdot g(x)$, find $h’(2)$ using the table below: QVQ-72B-Preview
Let’s tackle this problem step by step....</p></div><footer class=entry-footer><span title='2024-12-25 00:00:03 +0800 +0800'>2024年12月25日</span>&nbsp;·&nbsp;16 分钟&nbsp;·&nbsp;3372 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to QVQ: 更睿智地看世界" href=https://qwenlm.github.io/zh/blog/qvq-72b-preview/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://qwenlm.github.io/zh/blog/page/2/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://qwenlm.github.io/zh/blog/page/4/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>