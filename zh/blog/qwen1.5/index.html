<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen1.5 介绍 | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。
此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers>=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen1.5/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen1.5/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen1.5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen1.5 介绍"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。
此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers>=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen1.5/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-02-04T13:33:00+08:00"><meta property="article:modified_time" content="2024-02-04T13:33:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen1.5 介绍"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO WeChat
简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。
此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers>=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。
我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。
相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。
模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。
基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。
Model MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen1.5 介绍","item":"https://qwenlm.github.io/zh/blog/qwen1.5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen1.5 介绍","name":"Qwen1.5 介绍","description":"GITHUB HUGGING FACE MODELSCOPE DEMO WeChat\n简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。\n此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers\u0026gt;=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。\n我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。\n相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。\n模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。\n基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。\nModel MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46.","keywords":[],"articleBody":"GITHUB HUGGING FACE MODELSCOPE DEMO WeChat\n简介 最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: Qwen1.5。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击博客 了解详情），并同步放出了各尺寸模型对应的量化模型。\n此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 transformers\u003e=4.37.0 原生代码，而无需指定 trust_remote_code 选项即可进行开发。\n我们已经与vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 Ollama 和 LMStudio 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 together.ai 上提供，全球都可访问。请访问here开始使用，我们建议您试用Qwen1.5-72B-chat。\n相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。\n模型效果 为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。\n基础能力 关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。\nModel MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU GPT-4 86.4 69.9 92.0 45.8 67.0 61.8 86.7 71.0 Llama2-7B 46.8 32.5 16.7 3.3 12.8 20.8 38.2 31.8 Llama2-13B 55.0 41.4 29.6 5.0 18.9 30.3 45.6 38.4 Llama2-34B 62.6 - 42.2 6.2 22.6 33.0 44.1 - Llama2-70B 69.8 50.1 54.4 10.6 23.7 37.7 58.4 53.6 Mistral-7B 64.1 47.4 47.5 11.3 27.4 38.6 56.7 44.7 Mixtral-8x7B 70.6 - 74.4 28.4 40.2 60.7 - - Qwen1.5-7B 61.0 74.1 62.5 20.3 36.0 37.4 40.2 73.1 Qwen1.5-14B 67.6 78.7 70.1 29.2 37.8 44.0 53.7 77.6 Qwen1.5-32B 73.4 83.5 77.4 36.1 37.2 49.4 66.8 82.3 Qwen1.5-72B 77.5 84.1 79.5 34.1 41.5 53.4 65.5 83.5 在不同模型尺寸下，Qwen1.5 都在评估基准中表现出强劲的性能。特别是，Qwen1.5-72B 在所有基准测试中都远远超越了Llama2-70B，展示了其在语言理解、推理和数学方面的卓越能力。\n最近小型模型的构建也成为了热点之一，我们将模型参数小于 70 亿的 Qwen1.5 模型与社区中最杰出的小型模型进行了比较。结果如下：\nModel Non-Emb Params MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU Tinyllama-1.1B 1.1B 24.3 25.0 2.3 0.7 6.7 19.9 28.8 24.0 Gemini-Nano-3B - - - 22.8 - - 27.2 42.4 - StableLM-Zephyr-3B 2.7B 45.9 30.3 52.5 12.5 35.4 31.9 37.7 30.9 Phi-2 2.5B 52.7 23.4 57.2 3.5 47.6 55.0 43.4 24.2 MiniCPM-2B 2.4B 53.5 51.1 53.8 10.2 50.0 47.3 36.9 51.1 Gemma-2B 2.0B 42.3 - 17.7 11.8 22.0 29.2 35.2 - Qwen1.5-0.5B 0.3B 39.2 50.5 22.0 3.1 12.2 6.8 18.3 46.6 Qwen1.5-1.8B 1.2B 46.8 59.7 38.4 10.1 20.1 18.0 24.2 57.8 Qwen1.5-4B 3.1B 56.1 67.6 57.0 10.0 25.6 29.2 32.5 66.7 Qwen1.5-MoE-A2.7B 2.0B 62.5 79.2 61.5 21.9 34.2 36.6 39.1 79.2 我们可以自信地说，参数规模低于 70 亿的 Qwen1.5 base 模型，与业界领先的小型模型相比具有很强的竞争力。未来，我们将继续提高小模型的整体效果，并探索如何将大模型的能力有效迁移到小模型之中。\n人类偏好对齐 对齐的目的是增强语言的指令跟随能力，生成和人类偏好相近的回复。我们认识到将人类偏好融入学习过程的重要性，因此在对齐最新的 Qwen1.5 系列时有效地采用了直接策略优化（DPO）和近端策略优化（PPO）等技术。\n但是，评估此类聊天模型的质量是一项重大挑战。虽然全面的人工评估是最佳方法，但它在可扩展性和可重复性方面面临着巨大挑战。因此我们借助更先进的大模型作为评委，在两个广泛使用的基准上对 Qwen1.5 进行了初步评估： MT-Bench 和 Alpaca-Eval。评估结果如下：\n我们在评测MT-Bench榜单时发现在这个榜单上模型分数有较大的方差，因此我们进行了三轮评测并汇报平均分数和标准差。\nModels MT-Bench AlpacaEval 2.0 Avg. ScoreWin RateLength Qwen1.5-72B-Chat 8.610.04 (8.67/8.61/8.56) 27.181.30 1600 Qwen1.5-14B-Chat 7.910.11 (7.99/7.99/7.77) 19.71.12 1608 Qwen1.5-7B-Chat 7.600.05 (7.58/7.55/7.66) 13.201.43 1606 尽管落后于 GPT-4-Turbo，但最大的 Qwen1.5 模型 Qwen1.5-72B-Chat 在 MT-Bench 和 Alpaca-Eval v2 上都表现出不俗的效果，超过了 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与 Mistral Medium 不相上下。\n此外，虽然大模型裁判的评分似乎与回答的长度有关，但我们的观察结果表明 Qwen1.5 并没有产生过长的回答来操纵大模型裁判的偏差。AlpacaEval 2.0 上 Qwen1.5-Chat 的平均长度仅为 1618，与 GPT-4 的长度一致，比 GPT-4-Turbo 短。从通义千问网页端和APP的反馈看，用户更加喜爱新版本模型的回复。\n多语言能力 我们挑选了来自欧洲、东亚和东南亚的12种不同语言，全面评估Base模型的多语言能力。从开源社区的公开数据集中，我们构建了如下表所示的评测集合，共涵盖四个不同的维度：考试、理解、翻译、数学。下表提供了每个测试集的详细信息，包括其评测配置、评价指标以及所涉及的具体语言种类。\nDataset Category Method/Metric Languages MMLU-multi Exams 5-shot/Acc ar, es, fr, pt, de, it, ru, ja, ko, id M3Exams Exams 5-shot/Acc pt, it, vi, th BELEBELE Understanding 5-shot/Acc ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id XWinograd Understanding 5-shot/Acc fr, pt, ru, ja XCOPA Understanding 5-shot/Acc vi, id, th PAWS-X Understanding 5-shot/Acc es, fr, de, ja, ko XStoryCloze Understanding 0-shot/Acc ar, es, ru, id Flores(zh/en↔xx) Translation 5-shot/BLEU ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id MGSM Math 8-shot/Acc es, fr, ru, de, ja, th 详细的结果如下：\nModels Exams Understanding Math Translation GPT-3.5 52.24 71.84 32.80 31.85 GPT-4 71.64 83.82 80.13 34.37 Llama2-7B 34.03 50.13 9.40 22.19 Llama2-13B 39.55 57.26 16.80 25.89 Llama2-70B 55.88 73.19 40.20 31.56 Mistral-7B 47.12 63.30 26.33 23.33 Mixtral-8x7B 56.08 70.70 45.00 29.78 Qwen1.5-0.5B 26.98 44.08 3.13 9.17 Qwen1.5-1.8B 33.57 48.37 6.47 16.19 Qwen1.5-4B 41.43 59.76 21.33 23.34 Qwen1.5-MoE-A2.7B 44.54 61.08 30.20 27.35 Qwen1.5-7B 47.70 67.63 37.27 28.36 Qwen1.5-14B 55.72 74.10 49.93 31.69 Qwen1.5-72B 66.35 78.16 61.67 35.57 上述结果表明，Qwen1.5 Base模型在12种不同语言的多语言能力方面表现出色，在考试、理解、翻译和数学等各个维度的评估中，均展现优异结果。不论阿拉伯语、西班牙语、法语、日语，还是韩语、泰语，Qwen1.5均展示了在不同语言环境中理解和生成高质量内容的能力。更进一步地，我们也评估了Chat模型的多语言能力，结果如下所示：\n上述结果展示了Qwen1.5 Chat模型强大的多语言能力，可用于翻译、语言理解和多语言聊天等下游应用。我们相信多语言能力的提升，对于其整体通用能力也具有正向的作用。\n长序列 随着长序列理解的需求不断增加，我们这次推出的 Qwen1.5 模型全系列支持 32K tokens 的上下文。我们在L-Eval 基准上评估了 Qwen1.5 模型的性能，该基准衡量了模型根据长输入生成答案的能力。结果如下：\nModels Coursera GSM QuALITY TOEFL SFiction Avg. GPT3.5-turbo-16k 63.51 84.00 61.38 78.43 64.84 70.43 Claude1.3-100k 60.03 88.00 73.76 83.64 72.65 75.62 GPT4-32k 75.58 96.00 82.17 84.38 74.99 82.62 Qwen-72B-Chat 58.13 76.00 77.22 86.24 69.53 73.42 Qwen1.5-0.5B-Chat 30.81 6.00 34.16 40.52 49.22 32.14 Qwen1.5-1.8B-Chat 39.24 37.00 42.08 55.76 44.53 43.72 Qwen1.5-4B-Chat 54.94 47.00 57.92 69.15 56.25 57.05 Qwen1.5-7B-Chat 59.74 60.00 64.36 79.18 62.50 65.16 Qwen1.5-14B-Chat 69.04 79.00 74.75 83.64 75.78 76.44 Qwen1.5-72B-Chat 71.95 82.00 77.72 85.50 73.44 78.12 从结果来看，即使像 Qwen1.5-7B-Chat 这样的小规模模型，在上面大5个任务中的4个表现出与 GPT3.5-turbo-16k 类似的性能。而我们最好的模型 Qwen1.5-72B-Chat，仅略微落后于 GPT4-32k。尽管上述结果仅突显了我们在处理 32K tokens 长度时所展现的卓越性能，但这并不代表模型的最大支持长度仅限于 32K。您可以在 config.json 中，将 max_position_embedding 和 sliding_window 尝试修改为更大的值，观察模型在更长上下文理解场景下，是否可以达到您满意的效果。\n链接外部系统 如今，通用语言模型的一大魅力在于其与外部系统对接的潜能。具体而言，RAG作为一种在社区中快速兴起并广受青睐的任务，有效应对了大语言模型面临的一些典型挑战，比如幻觉、无法获取实时更新或私有数据等问题。此外，语言模型在使用API和根据指令及示例编写代码方面，展现出强大的能力。这使得LLM能够作为代码解释器或AI智能体，发挥更广阔的价值。\n我们首先对 Qwen1.5 系列 Chat 模型，在 RAG 任务上的端到端效果进行了评估。评测基于 RGB 测试集，是一个用于中英文 RAG 评估的集合：\nRGB English Benchmark for Retrieval-Augmented Generation Models Noise 0.8 (Acc.↑) Rejection 1.0 (Acc.↑) Integration 0.4 (Acc.↑) Counterfactual (Acc.↑) GPT4-Turbo 85.67 47.33 60.00 90.00 GPT3.5-Turbo 74.33 27.67 47.00 21.00 Llama2-70B-Chat 82.00 31.00 56.00 15.00 Mistral-7B-Instruct-v0.2 82.00 31.00 56.00 15.00 Mixtral-8x7B-Instruct-v0.1 82.67 37.00 67.00 8.00 Qwen1.5-7B-Chat 77.67 25.00 52.00 9.00 Qwen1.5-14B-Chat 80.67 24.00 60.00 8.00 Qwen1.5-72B-Chat 81.67 48.67 61.00 28.00 RGB Chinese Benchmark for Retrieval-Augmented Generation Models Noise 0.8 (Acc.↑) Rejection 1.0 (Acc.↑) Integration 0.4 (Acc.↑) Counterfactual (Acc.↑) GPT4-Turbo 75.00 38.67 63.00 90.00 GPT3.5-Turbo 69.00 13.00 55.00 25.00 Llama2-70B-Chat 28.00 17.00 32.00 8.00 Mistral-7B-Instruct-v0.2 54.67 28.67 37.00 4.00 Mixtral-8x7B-Instruct-v0.1 27.33 4.00 24.00 4.00 Qwen1.5-7B-Chat 71.00 10.33 54.00 20.00 Qwen1.5-14B-Chat 75.00 16.67 55.00 22.00 Qwen1.5-72B-Chat 76.00 51.00 66.00 44.00 然后，我们在T-Eval 基准测试中评估了 Qwen1.5 作为通用代理运行的能力。所有 Qwen1.5 模型都没有经过专门针对该基准的优化：\nAgent Performance on T-Eval English Models Overall Instruct Plan Reason Retrieve Understand Review GPT4-Turbo 86.4 96.3 87.8 65.3 88.9 85.8 94.5 Llama-2-70B-Chat 58.59 77.80 63.75 39.07 51.35 50.34 69.20 Mistral-7B-Instruct-v0.2 46.68 63.57 60.88 32.59 17.58 38.08 67.35 Mixtral-8x7B-Instruct-v0.1 62.15 42.39 46.48 60.35 76.69 73.70 73.31 Qwen1.5-7B-Chat 59.67 71.12 62.95 37.60 61.17 53.75 71.46 Qwen1.5-14B-Chat 71.77 86.16 73.09 49.51 72.07 66.03 83.78 Qwen1.5-72B-Chat 76.69 80.96 83.12 56.89 80.17 76.68 82.34 Agent Performance on T-Eval Chinese Models Overall Instruct Plan Reason Retrieve Understand Review GPT4-Turbo 85.9 97.6 87.0 68.4 89.2 86.8 86.0 Llama-2-70B-Chat 51.15 53.78 56.65 34.27 48.24 50.49 63.45 Mistral-7B-Instruct-v0.2 46.26 49.64 61.82 36.17 20.26 47.25 62.42 Mixtral-8x7B-Instruct-v0.1 62.77 26.38 60.79 62.02 76.60 77.74 73.10 Qwen1.5-7B-Chat 53.15 60.56 62.31 42.07 55.28 55.76 42.92 Qwen1.5-14B-Chat 64.85 84.25 64.77 54.68 72.35 68.88 44.15 Qwen1.5-72B-Chat 72.88 97.50 80.83 58.11 76.14 71.94 52.77 为了测试工具调用能力，我们遵循之前做法，使用我们自己开源的 评估基准 ，测试模型正确选择、调用工具的能力，结果如下：\nTool-Use Benchmark ModelsTool Selection (Acc.↑)Tool Input (Rouge-L↑)False Positive (Acc.↑) GPT-4 98.0 95.3 76.1 GPT-3.5 74.5 80.7 19.4 Llama-2-70B-Chat 88.54 70.36 0.37 Mistral-7B-Instruct-v0.2 94.79 82.81 6.34 Mixtral-8x7B-Instruct-v0.1 99.31 94.46 31.34 Qwen1.5-7B-Chat 95.83 89.48 92.54 Qwen1.5-14B-Chat 93.06 88.74 92.91 Qwen1.5-72B-Chat 95.14 91.14 98.51 最后，由于 Python 代码解释器已成为高级 LLM 越来越强大的工具，我们还在之前开源的 评估基准 上评估了我们的模型利用这一工具的能力：\nCode Interpreter Benchmark Models Accuracy of Code Execution Results (%) Executable Rate of Code (%) Math↑Visualization-Hard↑Visualization-Easy↑General↑ GPT-4 82.8 66.7 60.8 82.8 GPT-3.5 47.3 33.3 55.7 74.1 Mistral-7B-Instruct-v0.2 25.5 19.1 44.3 62.1 Mixtral-8x7B-Instruct-v0.1 47.8 33.3 54.4 60.3 Qwen1.5-7B-Chat 54.0 35.7 36.7 65.5 Qwen1.5-14B-Chat 62.1 46.4 48.1 70.6 Qwen1.5-72B-Chat 73.1 52.3 50.6 87.9 较大的 Qwen1.5-Chat 模型通常优于较小的模型，接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 Qwen1.5-72B-Chat 模型，也会因编码能力而明显落后于 GPT-4。我们的目标是在未来的版本中，在预训练和对齐过程中提高所有 Qwen 模型的编码能力。\n使用Qwen1.5开发 Qwen1.5 最大的不同之处，在于 Qwen1.5 与 HuggingFace transformers 代码库的集成。从 4.37.0 版本开始，您可以直接使用 transformers 库原生代码，而不加载任何自定义代码（指定trust_remote_code选项）来使用 Qwen1.5，像下面这样加载模型：\nfrom transformers import AutoModelForCausalLM # This is what we previously used model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True) # This is what you can use now model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", device_map=\"auto\") 与以前版本相比，使用 Qwen1.5-Chat 模型进行聊天的方式有所不同。您可以使用以下代码与 Qwen1.5 进行聊天：\nfrom transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" # 加载模型的设备 model = AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen1.5-14B-Chat-AWQ\", device_map=\"auto\" ) tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-14B-Chat-AWQ\") prompt = \"给我介绍一下大型语言模型。\" messages = [ {\"role\": \"system\", \"content\": \"你是一个有用的助手。\"}, {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(device) generated_ids = model.generate( model_inputs.input_ids, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 对于Chat模型，我们不再使用额外的 model.chat() 方法，而是直接调用 model.generate()。具体来说，基于 tokenizer_config.json 中编写的聊天模板，您可使用tokenizer.apply_chat_template() 来拼接输入文本，继而分词并调用 model.generate() 执行生成。您可根据 eos_token 来控制何时终止生成。\n我们还提供了AWQ和GPTQ量化模型（包括Int4和Int8模型）供您在低资源和部署场景中，使用Qwen1.5。由于Hugging Face transformers支持 AWQ 和 GPTQ，您可以直接像上面所示的方式加载模型并调用，只需更换相应的模型名称即可。\n此外，我们已将我们的代码集成到常用的推理框架中，以便您可以轻松部署模型。目前 vLLM\u003e=0.3.0 和 SGLang\u003e=0.1.11 已经正式支持 Qwen1.5。请查看他们的官方 github 仓库和文档，了解详细用法。以下示例展示如何使用vLLM，为模型构建一个与OpenAI-API兼容的接口：\npython -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat curl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen/Qwen1.5-7B-Chat\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"} ] }' 对于希望本地运行 LLM 的用户，llama.cpp 也提供了对 Qwen1.5 的支持，我们在 huggingface 模型中心官方提供了GGUF格式的量化模型。您可以使用以下代码在 llama.cpp 中运行 Qwen1.5：\n./main -m qwen1.5-7b-chat-q2_k.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt 此外，您也可以将GGUF模型与Ollama一起使用。基于 Ollama的支持，现在可以直接使用一行命令：\nollama run qwen 或者您可以将 GGUF 模型与 llamafile 一起使用，以单个文件运行我们的模型。\n为了在本地启动网页版 demo，我们建议您使用 Text generation web UI，非常易用。\n对于希望训练自己定制模型的高级开发者，目前 Qwen1.5 已经实现了 Hugging Face trainer 和 Peft 支持。目前，社区中也提供了易用的监督式微调（SFT）和人类反馈对齐（PPO、DPO等）训练框架，其中 LLaMA-Factory 和 Axolotl 已经支持了 Qwen1.5 的训练。我们建议您查看其官方 github 仓库和文档，了解更高级的用法。\n如果您希望将Qwen1.5用于下游应用，如RAG、工具使用、智能体等，可以考虑如 LlamaIndex、LangChain、CrewAI 等社区常用框架，构建与OpenAI-API兼容的API或本地运行模型。\n总之，我们始终将关注点放在优化您的开发体验上，不仅致力于为社区打造卓越的模型，还力求让一切操作更为简单易用。希望您在使用 Qwen1.5 的过程中能满意，也希望模型能在您的研究或应用项目中发挥作用。\n小结 我们发布了 Qwen1.5 —— Qwen 系列的新一代版本。在这次发布中，我们开源了包括 0.5B、1.8B、4B、7B、14B 和 72B 在内的 6 种大小的 Base 和 Chat 模型，并且我们还提供了量化模型。我们已将 Qwen1.5 的代码合并到 Hugging Face transformers 中，您现在可以直接使用 transformers\u003e=4.37.0 而无需指定 trust_remote_code。此外，我们支持了例如vLLM、SGLang、AutoGPTQ等框架支持Qwen1.5。从现在开始，我们的模型将会更加易用。我们相信这次发布虽然在模型质量上是一小步，但在开发者体验上却是一大步。欢迎加入我们的 Discord 或 微信 分享您的体验、评论或任何您喜欢的内容，向我们提出宝贵的意见和建议！\n引用 @misc{qwen1.5, title = {Introducing Qwen1.5}, url = {https://qwenlm.github.io/blog/qwen1.5/}, author = {Qwen Team}, month = {February}, year = {2024} } ","wordCount":"1296","inLanguage":"zh","datePublished":"2024-02-04T13:33:00+08:00","dateModified":"2024-02-04T13:33:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen1.5/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen1.5 介绍</h1><div class=post-meta><span title='2024-02-04 13:33:00 +0800 +0800'>2024年2月4日</span>&nbsp;·&nbsp;7 分钟&nbsp;·&nbsp;1296 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen1.5/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/QwenLM/Qwen1.5 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat class="btn external" target=_blank>DEMO</a>
<a href=https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png class="btn external" target=_blank>WeChat</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>最近几个月，我们专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。农历新年到来之际，我们推出通义千问开源模型1.5版本: <strong>Qwen1.5</strong>。我们开源了包括0.5B、1.8B、4B、7B、14B、32B、72B和110B共计8个不同规模的Base和Chat模型，, 以及一个MoE模型（点击<a href=https://qwenlm.github.io/blog/qwen-moe/>博客</a> 了解详情），并同步放出了各尺寸模型对应的量化模型。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen1.5/intro.jpg#center width=80%></figure><p>此次更新中，我们不仅像之前一样提供Int4和Int8的GPTQ模型，还提供了AWQ以及GGUF量化模型。为了提升开发者体验，我们将Qwen1.5的代码正式合并到HuggingFace transformers代码库中，所以现在可以直接使用 <code>transformers>=4.37.0</code> 原生代码，而无需指定 <code>trust_remote_code</code> 选项即可进行开发。</p><p>我们已经与<a href=https://vllm.readthedocs.io/>vLLM</a>、<a href=https://github.com/sgl-project/sglang>SGLang</a>（用于部署）、<a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>、<a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>（用于量化）、<a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>、<a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a>（用于微调）以及<a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列可在 <a href=https://ollama.ai/>Ollama</a> 和 <a href=https://lmstudio.ai/>LMStudio</a> 等平台上使用。此外，API 服务不仅在 DashScope 上提供，还在 <a href=https://together.ai/>together.ai</a> 上提供，全球都可访问。请访问<a href=https://api.together.ai/>here</a>开始使用，我们建议您试用<a href=https://api.together.xyz/playground/chat/Qwen/Qwen1.5-72B-Chat>Qwen1.5-72B-chat</a>。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen1.5/com.jpg#center width=100%></figure><p>相较于以往版本，本次更新我们着重提升Chat模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为您带来更佳体验。这次迭代是我们朝向「卓越」模型目标所迈进一个坚实的步伐。</p><h1 id=模型效果>模型效果<a hidden class=anchor aria-hidden=true href=#模型效果>#</a></h1><p>为了全面洞悉 Qwen1.5 的效果表现，我们对 Base 和 Chat 模型在一系列基础及扩展能力上进行了详尽评估，包括如语言理解、代码、推理等在内的基础能力，多语言能力，人类偏好对齐能力，智能体能力，检索增强生成能力（RAG）等。</p><h2 id=基础能力>基础能力<a hidden class=anchor aria-hidden=true href=#基础能力>#</a></h2><p>关于模型基础能力的评测，我们在 MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对 Qwen1.5 进行了评估。</p><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>MMLU</th><th style=text-align:center>C-Eval</th><th style=text-align:center>GSM8K</th><th style=text-align:center>MATH</th><th style=text-align:center>HumanEval</th><th style=text-align:center>MBPP</th><th style=text-align:center>BBH</th><th style=text-align:center>CMMLU</th></tr></thead><tbody><tr><td style=text-align:left>GPT-4</td><td style=text-align:center>86.4</td><td style=text-align:center>69.9</td><td style=text-align:center>92.0</td><td style=text-align:center>45.8</td><td style=text-align:center>67.0</td><td style=text-align:center>61.8</td><td style=text-align:center>86.7</td><td style=text-align:center>71.0</td></tr><tr><td style=text-align:left>Llama2-7B</td><td style=text-align:center>46.8</td><td style=text-align:center>32.5</td><td style=text-align:center>16.7</td><td style=text-align:center>3.3</td><td style=text-align:center>12.8</td><td style=text-align:center>20.8</td><td style=text-align:center>38.2</td><td style=text-align:center>31.8</td></tr><tr><td style=text-align:left>Llama2-13B</td><td style=text-align:center>55.0</td><td style=text-align:center>41.4</td><td style=text-align:center>29.6</td><td style=text-align:center>5.0</td><td style=text-align:center>18.9</td><td style=text-align:center>30.3</td><td style=text-align:center>45.6</td><td style=text-align:center>38.4</td></tr><tr><td style=text-align:left>Llama2-34B</td><td style=text-align:center>62.6</td><td style=text-align:center>-</td><td style=text-align:center>42.2</td><td style=text-align:center>6.2</td><td style=text-align:center>22.6</td><td style=text-align:center>33.0</td><td style=text-align:center>44.1</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Llama2-70B</td><td style=text-align:center>69.8</td><td style=text-align:center>50.1</td><td style=text-align:center>54.4</td><td style=text-align:center>10.6</td><td style=text-align:center>23.7</td><td style=text-align:center>37.7</td><td style=text-align:center>58.4</td><td style=text-align:center>53.6</td></tr><tr><td style=text-align:left>Mistral-7B</td><td style=text-align:center>64.1</td><td style=text-align:center>47.4</td><td style=text-align:center>47.5</td><td style=text-align:center>11.3</td><td style=text-align:center>27.4</td><td style=text-align:center>38.6</td><td style=text-align:center>56.7</td><td style=text-align:center>44.7</td></tr><tr><td style=text-align:left>Mixtral-8x7B</td><td style=text-align:center>70.6</td><td style=text-align:center>-</td><td style=text-align:center>74.4</td><td style=text-align:center>28.4</td><td style=text-align:center>40.2</td><td style=text-align:center>60.7</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Qwen1.5-7B</td><td style=text-align:center>61.0</td><td style=text-align:center>74.1</td><td style=text-align:center>62.5</td><td style=text-align:center>20.3</td><td style=text-align:center>36.0</td><td style=text-align:center>37.4</td><td style=text-align:center>40.2</td><td style=text-align:center>73.1</td></tr><tr><td style=text-align:left>Qwen1.5-14B</td><td style=text-align:center>67.6</td><td style=text-align:center>78.7</td><td style=text-align:center>70.1</td><td style=text-align:center>29.2</td><td style=text-align:center>37.8</td><td style=text-align:center>44.0</td><td style=text-align:center>53.7</td><td style=text-align:center>77.6</td></tr><tr><td style=text-align:left>Qwen1.5-32B</td><td style=text-align:center>73.4</td><td style=text-align:center>83.5</td><td style=text-align:center>77.4</td><td style=text-align:center>36.1</td><td style=text-align:center>37.2</td><td style=text-align:center>49.4</td><td style=text-align:center>66.8</td><td style=text-align:center>82.3</td></tr><tr><td style=text-align:left>Qwen1.5-72B</td><td style=text-align:center>77.5</td><td style=text-align:center>84.1</td><td style=text-align:center>79.5</td><td style=text-align:center>34.1</td><td style=text-align:center>41.5</td><td style=text-align:center>53.4</td><td style=text-align:center>65.5</td><td style=text-align:center>83.5</td></tr></tbody></table><p>在不同模型尺寸下，Qwen1.5 都在评估基准中表现出强劲的性能。特别是，Qwen1.5-72B 在所有基准测试中都远远超越了Llama2-70B，展示了其在语言理解、推理和数学方面的卓越能力。</p><p>最近小型模型的构建也成为了热点之一，我们将模型参数小于 70 亿的 Qwen1.5 模型与社区中最杰出的小型模型进行了比较。结果如下：</p><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Non-Emb Params</th><th style=text-align:center>MMLU</th><th style=text-align:center>C-Eval</th><th style=text-align:center>GSM8K</th><th style=text-align:center>MATH</th><th style=text-align:center>HumanEval</th><th style=text-align:center>MBPP</th><th style=text-align:center>BBH</th><th style=text-align:center>CMMLU</th></tr></thead><tbody><tr><td style=text-align:left>Tinyllama-1.1B</td><td style=text-align:center>1.1B</td><td style=text-align:center>24.3</td><td style=text-align:center>25.0</td><td style=text-align:center>2.3</td><td style=text-align:center>0.7</td><td style=text-align:center>6.7</td><td style=text-align:center>19.9</td><td style=text-align:center>28.8</td><td style=text-align:center>24.0</td></tr><tr><td style=text-align:left>Gemini-Nano-3B</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>22.8</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>27.2</td><td style=text-align:center>42.4</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>StableLM-Zephyr-3B</td><td style=text-align:center>2.7B</td><td style=text-align:center>45.9</td><td style=text-align:center>30.3</td><td style=text-align:center>52.5</td><td style=text-align:center>12.5</td><td style=text-align:center>35.4</td><td style=text-align:center>31.9</td><td style=text-align:center>37.7</td><td style=text-align:center>30.9</td></tr><tr><td style=text-align:left>Phi-2</td><td style=text-align:center>2.5B</td><td style=text-align:center>52.7</td><td style=text-align:center>23.4</td><td style=text-align:center>57.2</td><td style=text-align:center>3.5</td><td style=text-align:center>47.6</td><td style=text-align:center>55.0</td><td style=text-align:center>43.4</td><td style=text-align:center>24.2</td></tr><tr><td style=text-align:left>MiniCPM-2B</td><td style=text-align:center>2.4B</td><td style=text-align:center>53.5</td><td style=text-align:center>51.1</td><td style=text-align:center>53.8</td><td style=text-align:center>10.2</td><td style=text-align:center>50.0</td><td style=text-align:center>47.3</td><td style=text-align:center>36.9</td><td style=text-align:center>51.1</td></tr><tr><td style=text-align:left>Gemma-2B</td><td style=text-align:center>2.0B</td><td style=text-align:center>42.3</td><td style=text-align:center>-</td><td style=text-align:center>17.7</td><td style=text-align:center>11.8</td><td style=text-align:center>22.0</td><td style=text-align:center>29.2</td><td style=text-align:center>35.2</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Qwen1.5-0.5B</td><td style=text-align:center>0.3B</td><td style=text-align:center>39.2</td><td style=text-align:center>50.5</td><td style=text-align:center>22.0</td><td style=text-align:center>3.1</td><td style=text-align:center>12.2</td><td style=text-align:center>6.8</td><td style=text-align:center>18.3</td><td style=text-align:center>46.6</td></tr><tr><td style=text-align:left>Qwen1.5-1.8B</td><td style=text-align:center>1.2B</td><td style=text-align:center>46.8</td><td style=text-align:center>59.7</td><td style=text-align:center>38.4</td><td style=text-align:center>10.1</td><td style=text-align:center>20.1</td><td style=text-align:center>18.0</td><td style=text-align:center>24.2</td><td style=text-align:center>57.8</td></tr><tr><td style=text-align:left>Qwen1.5-4B</td><td style=text-align:center>3.1B</td><td style=text-align:center>56.1</td><td style=text-align:center>67.6</td><td style=text-align:center>57.0</td><td style=text-align:center>10.0</td><td style=text-align:center>25.6</td><td style=text-align:center>29.2</td><td style=text-align:center>32.5</td><td style=text-align:center>66.7</td></tr><tr><td style=text-align:left>Qwen1.5-MoE-A2.7B</td><td style=text-align:center>2.0B</td><td style=text-align:center>62.5</td><td style=text-align:center>79.2</td><td style=text-align:center>61.5</td><td style=text-align:center>21.9</td><td style=text-align:center>34.2</td><td style=text-align:center>36.6</td><td style=text-align:center>39.1</td><td style=text-align:center>79.2</td></tr></tbody></table><p>我们可以自信地说，参数规模低于 70 亿的 Qwen1.5 base 模型，与业界领先的小型模型相比具有很强的竞争力。未来，我们将继续提高小模型的整体效果，并探索如何将大模型的能力有效迁移到小模型之中。</p><h2 id=人类偏好对齐>人类偏好对齐<a hidden class=anchor aria-hidden=true href=#人类偏好对齐>#</a></h2><p>对齐的目的是增强语言的指令跟随能力，生成和人类偏好相近的回复。我们认识到将人类偏好融入学习过程的重要性，因此在对齐最新的 Qwen1.5 系列时有效地采用了直接策略优化（DPO）和近端策略优化（PPO）等技术。</p><p>但是，评估此类聊天模型的质量是一项重大挑战。虽然全面的人工评估是最佳方法，但它在可扩展性和可重复性方面面临着巨大挑战。因此我们借助更先进的大模型作为评委，在两个广泛使用的基准上对 Qwen1.5 进行了初步评估： MT-Bench 和 Alpaca-Eval。评估结果如下：</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen1.5/sft.jpg#center width=80%></figure><p>我们在评测MT-Bench榜单时发现在这个榜单上模型分数有较大的方差，因此我们进行了三轮评测并汇报平均分数和标准差。</p><table><tr><th rowspan=2 align=center>Models</th><th colspan=1 align=center>MT-Bench</th><th colspan=2 align=center>AlpacaEval 2.0</th></tr><tr><th align=center>Avg. Score</th><th align=center>Win Rate</th><th align=center>Length</th></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>8.61<sub>0.04</sub> (8.67/8.61/8.56)</td><td align=center>27.18<sub>1.30</sub></td><td align=center>1600</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>7.91<sub>0.11</sub> (7.99/7.99/7.77)</td><td align=center>19.7<sub>1.12</sub></td><td align=center>1608</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>7.60<sub>0.05</sub> (7.58/7.55/7.66)</td><td align=center>13.20<sub>1.43</sub></td><td align=center>1606</td></tr></table><p>尽管落后于 GPT-4-Turbo，但最大的 Qwen1.5 模型 Qwen1.5-72B-Chat 在 MT-Bench 和 Alpaca-Eval v2 上都表现出不俗的效果，超过了 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与 Mistral Medium 不相上下。</p><p>此外，虽然大模型裁判的评分似乎与回答的长度有关，但我们的观察结果表明 Qwen1.5 并没有产生过长的回答来操纵大模型裁判的偏差。AlpacaEval 2.0 上 Qwen1.5-Chat 的平均长度仅为 1618，与 GPT-4 的长度一致，比 GPT-4-Turbo 短。从通义千问网页端和APP的反馈看，用户更加喜爱新版本模型的回复。</p><h2 id=多语言能力>多语言能力<a hidden class=anchor aria-hidden=true href=#多语言能力>#</a></h2><p>我们挑选了来自欧洲、东亚和东南亚的12种不同语言，全面评估Base模型的多语言能力。从开源社区的公开数据集中，我们构建了如下表所示的评测集合，共涵盖四个不同的维度：考试、理解、翻译、数学。下表提供了每个测试集的详细信息，包括其评测配置、评价指标以及所涉及的具体语言种类。</p><table><thead><tr><th style=text-align:left>Dataset</th><th style=text-align:center>Category</th><th style=text-align:center>Method/Metric</th><th style=text-align:center>Languages</th></tr></thead><tbody><tr><td style=text-align:left>MMLU-multi</td><td style=text-align:center>Exams</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>ar, es, fr, pt, de, it, ru, ja, ko, id</td></tr><tr><td style=text-align:left>M3Exams</td><td style=text-align:center>Exams</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>pt, it, vi, th</td></tr><tr><td style=text-align:left>BELEBELE</td><td style=text-align:center>Understanding</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id</td></tr><tr><td style=text-align:left>XWinograd</td><td style=text-align:center>Understanding</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>fr, pt, ru, ja</td></tr><tr><td style=text-align:left>XCOPA</td><td style=text-align:center>Understanding</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>vi, id, th</td></tr><tr><td style=text-align:left>PAWS-X</td><td style=text-align:center>Understanding</td><td style=text-align:center>5-shot/Acc</td><td style=text-align:center>es, fr, de, ja, ko</td></tr><tr><td style=text-align:left>XStoryCloze</td><td style=text-align:center>Understanding</td><td style=text-align:center>0-shot/Acc</td><td style=text-align:center>ar, es, ru, id</td></tr><tr><td style=text-align:left>Flores(zh/en↔xx)</td><td style=text-align:center>Translation</td><td style=text-align:center>5-shot/BLEU</td><td style=text-align:center>ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id</td></tr><tr><td style=text-align:left>MGSM</td><td style=text-align:center>Math</td><td style=text-align:center>8-shot/Acc</td><td style=text-align:center>es, fr, ru, de, ja, th</td></tr></tbody></table><p>详细的结果如下：</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Exams</th><th style=text-align:center>Understanding</th><th style=text-align:center>Math</th><th style=text-align:center>Translation</th></tr></thead><tbody><tr><td style=text-align:left>GPT-3.5</td><td style=text-align:center>52.24</td><td style=text-align:center>71.84</td><td style=text-align:center>32.80</td><td style=text-align:center>31.85</td></tr><tr><td style=text-align:left>GPT-4</td><td style=text-align:center>71.64</td><td style=text-align:center>83.82</td><td style=text-align:center>80.13</td><td style=text-align:center>34.37</td></tr><tr><td style=text-align:left>Llama2-7B</td><td style=text-align:center>34.03</td><td style=text-align:center>50.13</td><td style=text-align:center>9.40</td><td style=text-align:center>22.19</td></tr><tr><td style=text-align:left>Llama2-13B</td><td style=text-align:center>39.55</td><td style=text-align:center>57.26</td><td style=text-align:center>16.80</td><td style=text-align:center>25.89</td></tr><tr><td style=text-align:left>Llama2-70B</td><td style=text-align:center>55.88</td><td style=text-align:center>73.19</td><td style=text-align:center>40.20</td><td style=text-align:center>31.56</td></tr><tr><td style=text-align:left>Mistral-7B</td><td style=text-align:center>47.12</td><td style=text-align:center>63.30</td><td style=text-align:center>26.33</td><td style=text-align:center>23.33</td></tr><tr><td style=text-align:left>Mixtral-8x7B</td><td style=text-align:center>56.08</td><td style=text-align:center>70.70</td><td style=text-align:center>45.00</td><td style=text-align:center>29.78</td></tr><tr><td style=text-align:left>Qwen1.5-0.5B</td><td style=text-align:center>26.98</td><td style=text-align:center>44.08</td><td style=text-align:center>3.13</td><td style=text-align:center>9.17</td></tr><tr><td style=text-align:left>Qwen1.5-1.8B</td><td style=text-align:center>33.57</td><td style=text-align:center>48.37</td><td style=text-align:center>6.47</td><td style=text-align:center>16.19</td></tr><tr><td style=text-align:left>Qwen1.5-4B</td><td style=text-align:center>41.43</td><td style=text-align:center>59.76</td><td style=text-align:center>21.33</td><td style=text-align:center>23.34</td></tr><tr><td style=text-align:left>Qwen1.5-MoE-A2.7B</td><td style=text-align:center>44.54</td><td style=text-align:center>61.08</td><td style=text-align:center>30.20</td><td style=text-align:center>27.35</td></tr><tr><td style=text-align:left>Qwen1.5-7B</td><td style=text-align:center>47.70</td><td style=text-align:center>67.63</td><td style=text-align:center>37.27</td><td style=text-align:center>28.36</td></tr><tr><td style=text-align:left>Qwen1.5-14B</td><td style=text-align:center>55.72</td><td style=text-align:center>74.10</td><td style=text-align:center>49.93</td><td style=text-align:center>31.69</td></tr><tr><td style=text-align:left>Qwen1.5-72B</td><td style=text-align:center>66.35</td><td style=text-align:center>78.16</td><td style=text-align:center>61.67</td><td style=text-align:center>35.57</td></tr></tbody></table><p>上述结果表明，Qwen1.5 Base模型在12种不同语言的多语言能力方面表现出色，在考试、理解、翻译和数学等各个维度的评估中，均展现优异结果。不论阿拉伯语、西班牙语、法语、日语，还是韩语、泰语，Qwen1.5均展示了在不同语言环境中理解和生成高质量内容的能力。更进一步地，我们也评估了Chat模型的多语言能力，结果如下所示：</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen1.5/lang.png#center width=100%></figure><p>上述结果展示了Qwen1.5 Chat模型强大的多语言能力，可用于翻译、语言理解和多语言聊天等下游应用。我们相信多语言能力的提升，对于其整体通用能力也具有正向的作用。</p><h2 id=长序列>长序列<a hidden class=anchor aria-hidden=true href=#长序列>#</a></h2><p>随着长序列理解的需求不断增加，我们这次推出的 Qwen1.5 模型全系列支持 32K tokens 的上下文。我们在<a href=https://github.com/OpenLMLab/LEval>L-Eval 基准</a>上评估了 Qwen1.5 模型的性能，该基准衡量了模型根据长输入生成答案的能力。结果如下：</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Coursera</th><th style=text-align:center>GSM</th><th style=text-align:center>QuALITY</th><th style=text-align:center>TOEFL</th><th style=text-align:center>SFiction</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td style=text-align:left>GPT3.5-turbo-16k</td><td style=text-align:center>63.51</td><td style=text-align:center>84.00</td><td style=text-align:center>61.38</td><td style=text-align:center>78.43</td><td style=text-align:center>64.84</td><td style=text-align:center>70.43</td></tr><tr><td style=text-align:left>Claude1.3-100k</td><td style=text-align:center>60.03</td><td style=text-align:center>88.00</td><td style=text-align:center>73.76</td><td style=text-align:center>83.64</td><td style=text-align:center>72.65</td><td style=text-align:center>75.62</td></tr><tr><td style=text-align:left>GPT4-32k</td><td style=text-align:center>75.58</td><td style=text-align:center>96.00</td><td style=text-align:center>82.17</td><td style=text-align:center>84.38</td><td style=text-align:center>74.99</td><td style=text-align:center>82.62</td></tr><tr><td style=text-align:left>Qwen-72B-Chat</td><td style=text-align:center>58.13</td><td style=text-align:center>76.00</td><td style=text-align:center>77.22</td><td style=text-align:center>86.24</td><td style=text-align:center>69.53</td><td style=text-align:center>73.42</td></tr><tr><td style=text-align:left>Qwen1.5-0.5B-Chat</td><td style=text-align:center>30.81</td><td style=text-align:center>6.00</td><td style=text-align:center>34.16</td><td style=text-align:center>40.52</td><td style=text-align:center>49.22</td><td style=text-align:center>32.14</td></tr><tr><td style=text-align:left>Qwen1.5-1.8B-Chat</td><td style=text-align:center>39.24</td><td style=text-align:center>37.00</td><td style=text-align:center>42.08</td><td style=text-align:center>55.76</td><td style=text-align:center>44.53</td><td style=text-align:center>43.72</td></tr><tr><td style=text-align:left>Qwen1.5-4B-Chat</td><td style=text-align:center>54.94</td><td style=text-align:center>47.00</td><td style=text-align:center>57.92</td><td style=text-align:center>69.15</td><td style=text-align:center>56.25</td><td style=text-align:center>57.05</td></tr><tr><td style=text-align:left>Qwen1.5-7B-Chat</td><td style=text-align:center>59.74</td><td style=text-align:center>60.00</td><td style=text-align:center>64.36</td><td style=text-align:center>79.18</td><td style=text-align:center>62.50</td><td style=text-align:center>65.16</td></tr><tr><td style=text-align:left>Qwen1.5-14B-Chat</td><td style=text-align:center>69.04</td><td style=text-align:center>79.00</td><td style=text-align:center>74.75</td><td style=text-align:center>83.64</td><td style=text-align:center>75.78</td><td style=text-align:center>76.44</td></tr><tr><td style=text-align:left>Qwen1.5-72B-Chat</td><td style=text-align:center>71.95</td><td style=text-align:center>82.00</td><td style=text-align:center>77.72</td><td style=text-align:center>85.50</td><td style=text-align:center>73.44</td><td style=text-align:center>78.12</td></tr></tbody></table><p>从结果来看，即使像 Qwen1.5-7B-Chat 这样的小规模模型，在上面大5个任务中的4个表现出与 GPT3.5-turbo-16k 类似的性能。而我们最好的模型 Qwen1.5-72B-Chat，仅略微落后于 GPT4-32k。尽管上述结果仅突显了我们在处理 32K tokens 长度时所展现的卓越性能，但这并不代表模型的最大支持长度仅限于 32K。您可以在 <code>config.json</code> 中，将 <code>max_position_embedding</code> 和 <code>sliding_window</code> 尝试修改为更大的值，观察模型在更长上下文理解场景下，是否可以达到您满意的效果。</p><h2 id=链接外部系统>链接外部系统<a hidden class=anchor aria-hidden=true href=#链接外部系统>#</a></h2><p>如今，通用语言模型的一大魅力在于其与外部系统对接的潜能。具体而言，RAG作为一种在社区中快速兴起并广受青睐的任务，有效应对了大语言模型面临的一些典型挑战，比如幻觉、无法获取实时更新或私有数据等问题。此外，语言模型在使用API和根据指令及示例编写代码方面，展现出强大的能力。这使得LLM能够作为代码解释器或AI智能体，发挥更广阔的价值。</p><p>我们首先对 Qwen1.5 系列 Chat 模型，在 RAG 任务上的端到端效果进行了评估。评测基于 <a href=https://arxiv.org/abs/2309.01431>RGB</a> 测试集，是一个用于中英文 RAG 评估的集合：</p><table><tr><th colspan=5 align=center>RGB English Benchmark for Retrieval-Augmented Generation</th></tr><tr><th align=left>Models</th><th align=center>Noise 0.8 (Acc.↑)</th><th align=center>Rejection 1.0 (Acc.↑)</th><th align=center>Integration 0.4 (Acc.↑)</th><th align=center>Counterfactual (Acc.↑)</th></tr><tr><td>GPT4-Turbo</td><td align=center>85.67</td><td align=center>47.33</td><td align=center>60.00</td><td align=center>90.00</td></tr><tr><td>GPT3.5-Turbo</td><td align=center>74.33</td><td align=center>27.67</td><td align=center>47.00</td><td align=center>21.00</td></tr><tr><td>Llama2-70B-Chat</td><td align=center>82.00</td><td align=center>31.00</td><td align=center>56.00</td><td align=center>15.00</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>82.00</td><td align=center>31.00</td><td align=center>56.00</td><td align=center>15.00</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>82.67</td><td align=center>37.00</td><td align=center>67.00</td><td align=center>8.00</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>77.67</td><td align=center>25.00</td><td align=center>52.00</td><td align=center>9.00</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>80.67</td><td align=center>24.00</td><td align=center>60.00</td><td align=center>8.00</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>81.67</td><td align=center>48.67</td><td align=center>61.00</td><td align=center>28.00</td></tr><tr><th colspan=5 align=center>RGB Chinese Benchmark for Retrieval-Augmented Generation</th></tr><tr><th align=center>Models</th><th align=center>Noise 0.8 (Acc.↑)</th><th align=center>Rejection 1.0 (Acc.↑)</th><th align=center>Integration 0.4 (Acc.↑)</th><th align=center>Counterfactual (Acc.↑)</th></tr><tr><td>GPT4-Turbo</td><td align=center>75.00</td><td align=center>38.67</td><td align=center>63.00</td><td align=center>90.00</td></tr><tr><td>GPT3.5-Turbo</td><td align=center>69.00</td><td align=center>13.00</td><td align=center>55.00</td><td align=center>25.00</td></tr><tr><td>Llama2-70B-Chat</td><td align=center>28.00</td><td align=center>17.00</td><td align=center>32.00</td><td align=center>8.00</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>54.67</td><td align=center>28.67</td><td align=center>37.00</td><td align=center>4.00</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>27.33</td><td align=center>4.00</td><td align=center>24.00</td><td align=center>4.00</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>71.00</td><td align=center>10.33</td><td align=center>54.00</td><td align=center>20.00</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>75.00</td><td align=center>16.67</td><td align=center>55.00</td><td align=center>22.00</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>76.00</td><td align=center>51.00</td><td align=center>66.00</td><td align=center>44.00</td></tr></table><p>然后，我们在<a href=https://open-compass.github.io/T-Eval/>T-Eval</a> 基准测试中评估了 Qwen1.5 作为通用代理运行的能力。所有 Qwen1.5 模型都没有经过专门针对该基准的优化：</p><table><tr><th colspan=8 align=center>Agent Performance on T-Eval English</th></tr><tr><th align=left>Models</th><th align=center>Overall</th><th align=center>Instruct</th><th align=center>Plan</th><th align=center>Reason</th><th align=center>Retrieve</th><th align=center>Understand</th><th align=center>Review</th></tr><tr><td>GPT4-Turbo</td><td align=center>86.4</td><td align=center>96.3</td><td align=center>87.8</td><td align=center>65.3</td><td align=center>88.9</td><td align=center>85.8</td><td align=center>94.5</td></tr><tr><td>Llama-2-70B-Chat</td><td align=center>58.59</td><td align=center>77.80</td><td align=center>63.75</td><td align=center>39.07</td><td align=center>51.35</td><td align=center>50.34</td><td align=center>69.20</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>46.68</td><td align=center>63.57</td><td align=center>60.88</td><td align=center>32.59</td><td align=center>17.58</td><td align=center>38.08</td><td align=center>67.35</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>62.15</td><td align=center>42.39</td><td align=center>46.48</td><td align=center>60.35</td><td align=center>76.69</td><td align=center>73.70</td><td align=center>73.31</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>59.67</td><td align=center>71.12</td><td align=center>62.95</td><td align=center>37.60</td><td align=center>61.17</td><td align=center>53.75</td><td align=center>71.46</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>71.77</td><td align=center>86.16</td><td align=center>73.09</td><td align=center>49.51</td><td align=center>72.07</td><td align=center>66.03</td><td align=center>83.78</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>76.69</td><td align=center>80.96</td><td align=center>83.12</td><td align=center>56.89</td><td align=center>80.17</td><td align=center>76.68</td><td align=center>82.34</td></tr><tr><th colspan=8 align=center>Agent Performance on T-Eval Chinese</th></tr><tr><th align=center>Models</th><th align=center>Overall</th><th align=center>Instruct</th><th align=center>Plan</th><th align=center>Reason</th><th align=center>Retrieve</th><th align=center>Understand</th><th align=center>Review</th></tr><tr><td>GPT4-Turbo</td><td align=center>85.9</td><td align=center>97.6</td><td align=center>87.0</td><td align=center>68.4</td><td align=center>89.2</td><td align=center>86.8</td><td align=center>86.0</td></tr><tr><td>Llama-2-70B-Chat</td><td align=center>51.15</td><td align=center>53.78</td><td align=center>56.65</td><td align=center>34.27</td><td align=center>48.24</td><td align=center>50.49</td><td align=center>63.45</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>46.26</td><td align=center>49.64</td><td align=center>61.82</td><td align=center>36.17</td><td align=center>20.26</td><td align=center>47.25</td><td align=center>62.42</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>62.77</td><td align=center>26.38</td><td align=center>60.79</td><td align=center>62.02</td><td align=center>76.60</td><td align=center>77.74</td><td align=center>73.10</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>53.15</td><td align=center>60.56</td><td align=center>62.31</td><td align=center>42.07</td><td align=center>55.28</td><td align=center>55.76</td><td align=center>42.92</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>64.85</td><td align=center>84.25</td><td align=center>64.77</td><td align=center>54.68</td><td align=center>72.35</td><td align=center>68.88</td><td align=center>44.15</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>72.88</td><td align=center>97.50</td><td align=center>80.83</td><td align=center>58.11</td><td align=center>76.14</td><td align=center>71.94</td><td align=center>52.77</td></tr></table><p>为了测试工具调用能力，我们遵循之前做法，使用我们自己开源的 <a href=https://github.com/QwenLM/Qwen/blob/main/eval/evaluate_plugin.py>评估基准</a> ，测试模型正确选择、调用工具的能力，结果如下：</p><table><tr><th colspan=4 align=center>Tool-Use Benchmark</th></tr><tr><th align=left>Models</th><th align=center>Tool Selection (Acc.↑)</th><th align=center>Tool Input (Rouge-L↑)</th><th align=center>False Positive (Acc.↑)</th></tr><tr><td>GPT-4</td><td align=center>98.0</td><td align=center>95.3</td><td align=center>76.1</td></tr><tr><td>GPT-3.5</td><td align=center>74.5</td><td align=center>80.7</td><td align=center>19.4</td></tr><tr><td>Llama-2-70B-Chat</td><td align=center>88.54</td><td align=center>70.36</td><td align=center>0.37</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>94.79</td><td align=center>82.81</td><td align=center>6.34</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>99.31</td><td align=center>94.46</td><td align=center>31.34</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>95.83</td><td align=center>89.48</td><td align=center>92.54</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>93.06</td><td align=center>88.74</td><td align=center>92.91</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>95.14</td><td align=center>91.14</td><td align=center>98.51</td></tr></table><p>最后，由于 Python 代码解释器已成为高级 LLM 越来越强大的工具，我们还在之前开源的 <a href=https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark>评估基准</a> 上评估了我们的模型利用这一工具的能力：</p><table><tr><th colspan=5 align=left>Code Interpreter Benchmark</th></tr><tr><th rowspan=2 align=center>Models</th><th colspan=3 align=center>Accuracy of Code Execution Results (%)</th><th colspan=1 align=center>Executable Rate of Code (%)</th></tr><tr><th align=center>Math↑</th><th align=center>Visualization-Hard↑</th><th align=center>Visualization-Easy↑</th><th align=center>General↑</th></tr><tr><td>GPT-4</td><td align=center>82.8</td><td align=center>66.7</td><td align=center>60.8</td><td align=center>82.8</td></tr><tr><td>GPT-3.5</td><td align=center>47.3</td><td align=center>33.3</td><td align=center>55.7</td><td align=center>74.1</td></tr><tr><td>Mistral-7B-Instruct-v0.2</td><td align=center>25.5</td><td align=center>19.1</td><td align=center>44.3</td><td align=center>62.1</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td align=center>47.8</td><td align=center>33.3</td><td align=center>54.4</td><td align=center>60.3</td></tr><tr><td>Qwen1.5-7B-Chat</td><td align=center>54.0</td><td align=center>35.7</td><td align=center>36.7</td><td align=center>65.5</td></tr><tr><td>Qwen1.5-14B-Chat</td><td align=center>62.1</td><td align=center>46.4</td><td align=center>48.1</td><td align=center>70.6</td></tr><tr><td>Qwen1.5-72B-Chat</td><td align=center>73.1</td><td align=center>52.3</td><td align=center>50.6</td><td align=center>87.9</td></tr></table><p>较大的 Qwen1.5-Chat 模型通常优于较小的模型，接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 Qwen1.5-72B-Chat 模型，也会因编码能力而明显落后于 GPT-4。我们的目标是在未来的版本中，在预训练和对齐过程中提高所有 Qwen 模型的编码能力。</p><h1 id=使用qwen15开发>使用Qwen1.5开发<a hidden class=anchor aria-hidden=true href=#使用qwen15开发>#</a></h1><p>Qwen1.5 最大的不同之处，在于 Qwen1.5 与 HuggingFace transformers 代码库的集成。从 4.37.0 版本开始，您可以直接使用 transformers 库原生代码，而不加载任何自定义代码（指定trust_remote_code选项）来使用 Qwen1.5，像下面这样加载模型：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=c1># This is what we previously used</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen-7B-Chat&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># This is what you can use now</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen1.5-7B-Chat&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>与以前版本相比，使用 Qwen1.5-Chat 模型进行聊天的方式有所不同。您可以使用以下代码与 Qwen1.5 进行聊天：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=c1># 加载模型的设备</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Qwen/Qwen1.5-14B-Chat-AWQ&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen1.5-14B-Chat-AWQ&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;给我介绍一下大型语言模型。&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;你是一个有用的助手。&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>对于Chat模型，我们不再使用额外的 <code>model.chat()</code> 方法，而是直接调用 <code>model.generate()</code>。具体来说，基于 <code>tokenizer_config.json</code> 中编写的聊天模板，您可使用<code>tokenizer.apply_chat_template()</code> 来拼接输入文本，继而分词并调用 <code>model.generate()</code> 执行生成。您可根据 eos_token 来控制何时终止生成。</p><p>我们还提供了AWQ和GPTQ量化模型（包括Int4和Int8模型）供您在低资源和部署场景中，使用Qwen1.5。由于Hugging Face transformers支持 <a href=https://github.com/casper-hansen/AutoAWQ>AWQ</a> 和 <a href=https://github.com/AutoGPTQ/AutoGPTQ>GPTQ</a>，您可以直接像上面所示的方式加载模型并调用，只需更换相应的模型名称即可。</p><p>此外，我们已将我们的代码集成到常用的推理框架中，以便您可以轻松部署模型。目前 <code>vLLM>=0.3.0</code> 和 <code>SGLang>=0.1.11</code> 已经正式支持 Qwen1.5。请查看他们的官方 github 仓库和文档，了解详细用法。以下示例展示如何使用vLLM，为模型构建一个与OpenAI-API兼容的接口：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;model&#34;: &#34;Qwen/Qwen1.5-7B-Chat&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
</span></span></span><span class=line><span class=cl><span class=s1>    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>    ]
</span></span></span><span class=line><span class=cl><span class=s1>    }&#39;</span>
</span></span></code></pre></div><p>对于希望本地运行 LLM 的用户，llama.cpp 也提供了对 Qwen1.5 的支持，我们在 huggingface 模型中心官方提供了GGUF格式的量化模型。您可以使用以下代码在 llama.cpp 中运行 Qwen1.5：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>./main -m qwen1.5-7b-chat-q2_k.gguf -n <span class=m>512</span> --color -i -cml -f prompts/chat-with-qwen.txt
</span></span></code></pre></div><p>此外，您也可以将GGUF模型与Ollama一起使用。基于 <a href=https://ollama.ai/>Ollama</a>的支持，现在可以直接使用一行命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ollama run qwen
</span></span></code></pre></div><p>或者您可以将 GGUF 模型与 <a href=https://github.com/Mozilla-Ocho/llamafile>llamafile</a> 一起使用，以单个文件运行我们的模型。</p><p>为了在本地启动网页版 demo，我们建议您使用 <a href=https://github.com/oobabooga/text-generation-webui>Text generation web UI</a>，非常易用。</p><p>对于希望训练自己定制模型的高级开发者，目前 Qwen1.5 已经实现了 Hugging Face trainer 和 Peft 支持。目前，社区中也提供了易用的监督式微调（SFT）和人类反馈对齐（PPO、DPO等）训练框架，其中 <a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a> 和 <a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a> 已经支持了 Qwen1.5 的训练。我们建议您查看其官方 github 仓库和文档，了解更高级的用法。</p><p>如果您希望将Qwen1.5用于下游应用，如RAG、工具使用、智能体等，可以考虑如 <a href=https://www.llamaindex.ai/>LlamaIndex</a>、<a href=https://www.langchain.com/>LangChain</a>、<a href=https://www.crewai.io/>CrewAI</a> 等社区常用框架，构建与OpenAI-API兼容的API或本地运行模型。</p><p>总之，我们始终将关注点放在优化您的开发体验上，不仅致力于为社区打造卓越的模型，还力求让一切操作更为简单易用。希望您在使用 Qwen1.5 的过程中能满意，也希望模型能在您的研究或应用项目中发挥作用。</p><h1 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h1><p>我们发布了 Qwen1.5 —— Qwen 系列的新一代版本。在这次发布中，我们开源了包括 0.5B、1.8B、4B、7B、14B 和 72B 在内的 6 种大小的 Base 和 Chat 模型，并且我们还提供了量化模型。我们已将 Qwen1.5 的代码合并到 Hugging Face transformers 中，您现在可以直接使用 <code>transformers>=4.37.0</code> 而无需指定 <code>trust_remote_code</code>。此外，我们支持了例如vLLM、SGLang、AutoGPTQ等框架支持Qwen1.5。从现在开始，我们的模型将会更加易用。我们相信这次发布虽然在模型质量上是一小步，但在开发者体验上却是一大步。欢迎加入我们的 <a href=https://discord.gg/yPEP2vHTu4e>Discord</a> 或 <a href=https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png>微信</a> 分享您的体验、评论或任何您喜欢的内容，向我们提出宝贵的意见和建议！</p><h1 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h1><pre tabindex=0><code>@misc{qwen1.5,
    title = {Introducing Qwen1.5},
    url = {https://qwenlm.github.io/blog/qwen1.5/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>