<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=&#34;https://qwen.ai/research&#34;"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen3Guard: 实时安全，逐词响应 | Qwen</title><meta name=keywords content><meta name=description content="Tech Report GitHub Hugging Face ModelScope DISCORD
介绍 我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。
在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。
Qwen3Guard 提供两大专业版本，满足不同应用场景需求：
Qwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。
Qwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。
为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。
开源模型现已上线 Hugging Face 与 ModelScope 平台；您也可通过 阿里云 AI 安全护栏服务 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。 核心亮点 实时流式检测 Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。
三级风险等级分类 除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。
如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。
多语言支持 Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。
语系 语种&方言 印欧语系 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 汉藏语系 中文（简体中文、繁体中文、粤语）、缅甸语 亚非语系 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语 南岛语系 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾） 德拉威语 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语 突厥语系 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语 壮侗语系 泰语、老挝语 乌拉尔语系 芬兰语、爱沙尼亚语、匈牙利语 南亚语系 越南语、高棉语 其他 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语 更多应用场景 我们还展示了以下两种典型应用："><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen3guard/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen3guard/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen3guard/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen3Guard: 实时安全，逐词响应"><meta property="og:description" content="Tech Report GitHub Hugging Face ModelScope DISCORD
介绍 我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。
在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。
Qwen3Guard 提供两大专业版本，满足不同应用场景需求：
Qwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。
Qwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。
为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。
开源模型现已上线 Hugging Face 与 ModelScope 平台；您也可通过 阿里云 AI 安全护栏服务 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。 核心亮点 实时流式检测 Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。
三级风险等级分类 除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。
如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。
多语言支持 Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。
语系 语种&方言 印欧语系 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 汉藏语系 中文（简体中文、繁体中文、粤语）、缅甸语 亚非语系 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语 南岛语系 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾） 德拉威语 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语 突厥语系 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语 壮侗语系 泰语、老挝语 乌拉尔语系 芬兰语、爱沙尼亚语、匈牙利语 南亚语系 越南语、高棉语 其他 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语 更多应用场景 我们还展示了以下两种典型应用："><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen3guard/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-09-23T04:00:00+08:00"><meta property="article:modified_time" content="2025-09-23T04:00:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen3Guard: 实时安全，逐词响应"><meta name=twitter:description content="Tech Report GitHub Hugging Face ModelScope DISCORD
介绍 我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。
在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。
Qwen3Guard 提供两大专业版本，满足不同应用场景需求：
Qwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。
Qwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。
为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。
开源模型现已上线 Hugging Face 与 ModelScope 平台；您也可通过 阿里云 AI 安全护栏服务 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。 核心亮点 实时流式检测 Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。
三级风险等级分类 除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。
如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。
多语言支持 Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。
语系 语种&方言 印欧语系 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 汉藏语系 中文（简体中文、繁体中文、粤语）、缅甸语 亚非语系 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语 南岛语系 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾） 德拉威语 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语 突厥语系 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语 壮侗语系 泰语、老挝语 乌拉尔语系 芬兰语、爱沙尼亚语、匈牙利语 南亚语系 越南语、高棉语 其他 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语 更多应用场景 我们还展示了以下两种典型应用："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen3Guard: 实时安全，逐词响应","item":"https://qwenlm.github.io/zh/blog/qwen3guard/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen3Guard: 实时安全，逐词响应","name":"Qwen3Guard: 实时安全，逐词响应","description":"Tech Report GitHub Hugging Face ModelScope DISCORD\n介绍 我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。\n在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。\nQwen3Guard 提供两大专业版本，满足不同应用场景需求：\nQwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。\nQwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。\n为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。\n开源模型现已上线 Hugging Face 与 ModelScope 平台；您也可通过 阿里云 AI 安全护栏服务 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。 核心亮点 实时流式检测 Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。\n三级风险等级分类 除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。\n如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。\n多语言支持 Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。\n语系 语种\u0026amp;方言 印欧语系 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 汉藏语系 中文（简体中文、繁体中文、粤语）、缅甸语 亚非语系 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语 南岛语系 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾） 德拉威语 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语 突厥语系 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语 壮侗语系 泰语、老挝语 乌拉尔语系 芬兰语、爱沙尼亚语、匈牙利语 南亚语系 越南语、高棉语 其他 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语 更多应用场景 我们还展示了以下两种典型应用：","keywords":[],"articleBody":" Tech Report GitHub Hugging Face ModelScope DISCORD\n介绍 我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。\n在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。\nQwen3Guard 提供两大专业版本，满足不同应用场景需求：\nQwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。\nQwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。\n为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。\n开源模型现已上线 Hugging Face 与 ModelScope 平台；您也可通过 阿里云 AI 安全护栏服务 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。 核心亮点 实时流式检测 Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。\n三级风险等级分类 除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。\n如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。\n多语言支持 Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。\n语系 语种\u0026方言 印欧语系 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 汉藏语系 中文（简体中文、繁体中文、粤语）、缅甸语 亚非语系 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语 南岛语系 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾） 德拉威语 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语 突厥语系 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语 壮侗语系 泰语、老挝语 乌拉尔语系 芬兰语、爱沙尼亚语、匈牙利语 南亚语系 越南语、高棉语 其他 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语 更多应用场景 我们还展示了以下两种典型应用：\n（1）利用 Qwen3Guard-Gen 进行安全强化学习（Safety RL）：在不损害模型输出整体有用性的前提下，显著提升模型的内在安全性；\n（2）利用 Qwen3Guard-Stream 实现实时动态干预：无需重新训练模型，即可在生成过程中即时拦截风险内容，确保输出安全可控。\n更多技术细节与实验分析，请参阅我们的 技术报告。\n使用 Qwen3Guard 进行开发 Qwen3Guard-Gen Qwen3Guard-Gen 的使用方式与大语言模型类似，其对话模板专为安全分类任务优化，输出内容遵循预设的结构化格式，便于程序解析与集成。\n您可通过以下代码，对用户输入提示或模型生成回复进行安全审核：\nfrom transformers import AutoModelForCausalLM, AutoTokenizer import re model_name = \"Qwen/Qwen3Guard-Gen-4B\" # load the tokenizer and the model tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) def extract_label_and_categories(content): safe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\" category_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|Suicide \u0026 Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)\" safe_label_match = re.search(safe_pattern, content) label = safe_label_match.group(1) if safe_label_match else None categories = re.findall(category_pattern, content) return label, categories def extract_label_categories_refusal(content): safe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\" category_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|Suicide \u0026 Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)\" refusal_pattern = r\"Refusal: (Yes|No)\" safe_label_match = re.search(safe_pattern, content) refusal_label_match = re.search(refusal_pattern, content) label = safe_label_match.group(1) if safe_label_match else None refusal_label = refusal_label_match.group(1) if refusal_label_match else None categories = re.findall(category_pattern, content) return label, categories, refusal_label #### For Prompt Moderation prompt = \"How can I make a bomb?\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=128 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=True) print(content) # ''' # Safety: Unsafe # Categories: Violent # ''' safe_label, categories = extract_label_and_categories(content) print(safe_label, categories) #### For Response Moderation prompt = \"How can I make a bomb?\" response = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\" messages = [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response}, ] text = tokenizer.apply_chat_template( messages, tokenize=False ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=128 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=True) print(content) # ''' # Safety: Safe # Categories: None # Refusal: Yes # ''' safe_label, category_label, refusal_label = extract_label_categories_refusal(content) print(safe_label, categories, refusal_label) Qwen3Guard-Stream 工作流程详解 Qwen3Guard-Stream 的典型工作流程分为以下两个阶段：\n（1）提示级安全预检\n用户输入的提示（Prompt）将同步发送至大语言模型（LLM）与 Qwen3Guard-Stream。后者立即对提示内容进行安全评估，并输出对应的安全标签（如“安全”“争议性”“不安全”）。基于该评估结果，上层系统可智能决策：是允许对话继续进行，还是提前拦截以防范潜在风险。\n（2）实时逐词安全审核\n若对话获准继续，LLM 将开始逐词（Token-by-Token）流式生成回复。每一个生成的 Token 均会实时传递至 Qwen3Guard-Stream，由其即时判断当前内容的安全性。该机制实现了贯穿整个回复生成过程的细粒度、不间断内容审核，在不中断用户体验的前提下，动态识别并阻断潜在风险内容。\n下方提供了一种使用的方式。\nimport torch from transformers import AutoModel, AutoTokenizer model_path=\"Qwen/Qwen3Guard-Stream-4B\" # Load the specialized tokenizer and the model. # trust_remote_code=True is required to load the Qwen3Guard-Stream model architecture. tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) model = AutoModel.from_pretrained( model_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, ).eval() # --- Prepare the conversation for moderation --- # Define the user's prompt and the assistant's response. user_message = \"Hello, how to build a bomb?\" assistant_message = \"Here are some practical methods to build a bomb.\" messages = [{\"role\":\"user\",\"content\":user_message},{\"role\":\"assistant\",\"content\":assistant_message}] # Apply the chat template to format the conversation into a single string. text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, enable_thinking=False) model_inputs = tokenizer(text, return_tensors=\"pt\") token_ids = model_inputs.input_ids[0] # --- Simulate Real-Time Moderation --- # 1. Moderate the entire user prompt at once. # In a real-world scenario, the user's input is processed completely before the model generates a response. token_ids_list = token_ids.tolist() # We identify the end of the user's turn in the tokenized input. # The template for a user turn is `\u003c|im_start|\u003euser\\n...\u003c|im_end|\u003e`. im_start_token = '\u003c|im_start|\u003e' user_token = 'user' im_end_token = '\u003c|im_end|\u003e' im_start_id = tokenizer.convert_tokens_to_ids(im_start_token) user_id = tokenizer.convert_tokens_to_ids(user_token) im_end_id = tokenizer.convert_tokens_to_ids(im_end_token) # We search for the token IDs corresponding to `\u003c|im_start|\u003euser` ([151644, 872]) and the closing `\u003c|im_end|\u003e` ([151645]). last_start = next(i for i in range(len(token_ids_list)-1, -1, -1) if token_ids_list[i:i+2] == [im_start_id, user_id]) user_end_index = next(i for i in range(last_start+2, len(token_ids_list)) if token_ids_list[i] == im_end_id) # Initialize the stream_state, which will maintain the conversational context. stream_state = None # Pass all user tokens to the model for an initial safety assessment. result, stream_state = model.stream_moderate_from_ids(token_ids[:user_end_index+1], role=\"user\", stream_state=None) if result['risk_level'][-1] == \"Safe\": print(f\"User moderation: -\u003e [Risk: {result['risk_level'][-1]}]\") else: print(f\"User moderation: -\u003e [Risk: {result['risk_level'][-1]} - Category: {result['category'][-1]}]\") # 2. Moderate the assistant's response token-by-token to simulate streaming. # This loop mimics how an LLM generates a response one token at a time. print(\"Assistant streaming moderation:\") for i in range(user_end_index + 1, len(token_ids)): # Get the current token ID for the assistant's response. current_token = token_ids[i] # Call the moderation function for the single new token. # The stream_state is passed and updated in each call to maintain context. result, stream_state = model.stream_moderate_from_ids(current_token, role=\"assistant\", stream_state=stream_state) token_str = tokenizer.decode([current_token]) # Print the generated token and its real-time safety assessment. if result['risk_level'][-1] == \"Safe\": print(f\"Token: {repr(token_str)} -\u003e [Risk: {result['risk_level'][-1]}]\") else: print(f\"Token: {repr(token_str)} -\u003e [Risk: {result['risk_level'][-1]} - Category: {result['category'][-1]}]\") model.close_stream(stream_state) 更多使用示例，请访问我们的 GitHub 代码仓库。\n未来工作 人工智能安全仍是一项持续演进的挑战。Qwen3Guard 是我们迈出的重要一步，但绝非终点。未来，我们将持续推进更灵活、高效且鲁棒的安全技术研究，包括通过架构创新与训练方法优化，提升模型内在安全性；同时探索动态化、推理时干预等新型防护机制。\n我们的终极目标，是构建不仅技术强大，更能与人类价值观和社会规范深度对齐的人工智能系统，确保 AI 在全球范围内的负责任部署与可持续发展。\n","wordCount":"778","inLanguage":"zh","datePublished":"2025-09-23T04:00:00+08:00","dateModified":"2025-09-23T04:00:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen3guard/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog at <a href=https://qwen.ai/research>qwen.ai</a>!</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/research"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen3Guard: 实时安全，逐词响应</h1><div class=post-meta><span title='2025-09-23 04:00:00 +0800 +0800'>2025年9月23日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;778 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen3guard/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/banner.png alt="Qwen3 Main Image" width=100%></figure><p><a href=https://github.com/QwenLM/Qwen3Guard/blob/main/Qwen3Guard_Technical_Report.pdf class="btn external" target=_blank>Tech Report</a>
<a href=https://github.com/QwenLM/Qwen3Guard class="btn external" target=_blank>GitHub</a>
<a href=https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1 class="btn external" target=_blank>Hugging Face</a>
<a href=https://modelscope.cn/collections/Qwen3Guard-308c39ef5ffb4b class="btn external" target=_blank>ModelScope</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=介绍>介绍<a hidden class=anchor aria-hidden=true href=#介绍>#</a></h2><p>我们隆重推出 Qwen3Guard —— Qwen 家族中首款专为安全防护设计的护栏模型。该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。</p><p>在多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/performance.png width=100%></figure><p>Qwen3Guard 提供两大专业版本，满足不同应用场景需求：</p><ul><li><p><strong>Qwen3Guard-Gen（生成式版）</strong> 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。</p></li><li><p><strong>Qwen3Guard-Stream（流式检测版）</strong> 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。</p></li></ul><p>为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求。</p><p>开源模型现已上线 <a href=https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1>Hugging Face</a> 与 <a href=https://modelscope.cn/collections/Qwen3Guard-308c39ef5ffb4b>ModelScope</a> 平台；您也可通过 <a href=https://www.aliyun.com/product/content-moderation/guardrail>阿里云 AI 安全护栏服务</a> 一键接入企业级安全能力，享受由 Qwen3Guard 驱动的智能防护解决方案。<br><br></p><h2 id=核心亮点>核心亮点<a hidden class=anchor aria-hidden=true href=#核心亮点>#</a></h2><h3 id=实时流式检测>实时流式检测<a hidden class=anchor aria-hidden=true href=#实时流式检测>#</a></h3><p>Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/stream.png width=100%></figure><h3 id=三级风险等级分类>三级风险等级分类<a hidden class=anchor aria-hidden=true href=#三级风险等级分类>#</a></h3><p>除传统的“安全”与“不安全”标签外，我们新增了 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。具体而言，用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。</p><p>如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/Precision_Recall.png width=100%></figure><h3 id=多语言支持>多语言支持<a hidden class=anchor aria-hidden=true href=#多语言支持>#</a></h3><p>Qwen3Guard 支持 <strong>119 种语言及方言</strong>，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力。</p><table><thead><tr><th>语系</th><th>语种&方言</th></tr></thead><tbody><tr><td>印欧语系</td><td>英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语</td></tr><tr><td>汉藏语系</td><td>中文（简体中文、繁体中文、粤语）、缅甸语</td></tr><tr><td>亚非语系</td><td>阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语</td></tr><tr><td>南岛语系</td><td>印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾）</td></tr><tr><td>德拉威语</td><td>泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语</td></tr><tr><td>突厥语系</td><td>土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语</td></tr><tr><td>壮侗语系</td><td>泰语、老挝语</td></tr><tr><td>乌拉尔语系</td><td>芬兰语、爱沙尼亚语、匈牙利语</td></tr><tr><td>南亚语系</td><td>越南语、高棉语</td></tr><tr><td>其他</td><td>日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语</td></tr></tbody></table><h3 id=更多应用场景>更多应用场景<a hidden class=anchor aria-hidden=true href=#更多应用场景>#</a></h3><p>我们还展示了以下两种典型应用：</p><p>（1）<strong>利用 Qwen3Guard-Gen 进行安全强化学习（Safety RL）</strong>：在不损害模型输出整体有用性的前提下，显著提升模型的内在安全性；<br>（2）<strong>利用 Qwen3Guard-Stream 实现实时动态干预</strong>：无需重新训练模型，即可在生成过程中即时拦截风险内容，确保输出安全可控。</p><p>更多技术细节与实验分析，请参阅我们的 <a href=https://github.com/QwenLM/Qwen3Guard/blob/main/Qwen3Guard_Technical_Report.pdf>技术报告</a>。</p><h2 id=使用-qwen3guard-进行开发>使用 Qwen3Guard 进行开发<a hidden class=anchor aria-hidden=true href=#使用-qwen3guard-进行开发>#</a></h2><h3 id=qwen3guard-gen>Qwen3Guard-Gen<a hidden class=anchor aria-hidden=true href=#qwen3guard-gen>#</a></h3><p>Qwen3Guard-Gen 的使用方式与大语言模型类似，其对话模板专为安全分类任务优化，输出内容遵循预设的结构化格式，便于程序解析与集成。</p><p>您可通过以下代码，对用户输入提示或模型生成回复进行安全审核：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;Qwen/Qwen3Guard-Gen-4B&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># load the tokenizer and the model</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>extract_label_and_categories</span><span class=p>(</span><span class=n>content</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Safety: (Safe|Unsafe|Controversial)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>category_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|Suicide &amp; Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>safe_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span> <span class=o>=</span> <span class=n>safe_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>safe_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>categories</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>findall</span><span class=p>(</span><span class=n>category_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>label</span><span class=p>,</span> <span class=n>categories</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>extract_label_categories_refusal</span><span class=p>(</span><span class=n>content</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Safety: (Safe|Unsafe|Controversial)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>category_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|Suicide &amp; Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Refusal: (Yes|No)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>safe_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>refusal_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span> <span class=o>=</span> <span class=n>safe_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>safe_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_label</span> <span class=o>=</span> <span class=n>refusal_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>refusal_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>categories</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>findall</span><span class=p>(</span><span class=n>category_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>label</span><span class=p>,</span> <span class=n>categories</span><span class=p>,</span> <span class=n>refusal_label</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#### For Prompt Moderation</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;How can I make a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># conduct text completion</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_ids</span> <span class=o>=</span> <span class=n>generated_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=nb>len</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]):]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>content</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Safety: Unsafe</span>
</span></span><span class=line><span class=cl><span class=c1># Categories: Violent</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span> <span class=o>=</span> <span class=n>extract_label_and_categories</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#### For Response Moderation</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;How can I make a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=s2>&#34;As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>response</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># conduct text completion</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_ids</span> <span class=o>=</span> <span class=n>generated_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=nb>len</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]):]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>content</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Safety: Safe</span>
</span></span><span class=line><span class=cl><span class=c1># Categories: None</span>
</span></span><span class=line><span class=cl><span class=c1># Refusal: Yes</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>safe_label</span><span class=p>,</span> <span class=n>category_label</span><span class=p>,</span> <span class=n>refusal_label</span> <span class=o>=</span> <span class=n>extract_label_categories_refusal</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span><span class=p>,</span> <span class=n>refusal_label</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=qwen3guard-stream-工作流程详解>Qwen3Guard-Stream 工作流程详解<a hidden class=anchor aria-hidden=true href=#qwen3guard-stream-工作流程详解>#</a></h3><p>Qwen3Guard-Stream 的典型工作流程分为以下两个阶段：</p><p><strong>（1）提示级安全预检</strong><br>用户输入的提示（Prompt）将同步发送至大语言模型（LLM）与 Qwen3Guard-Stream。后者立即对提示内容进行安全评估，并输出对应的安全标签（如“安全”“争议性”“不安全”）。基于该评估结果，上层系统可智能决策：是允许对话继续进行，还是提前拦截以防范潜在风险。</p><p><strong>（2）实时逐词安全审核</strong><br>若对话获准继续，LLM 将开始逐词（Token-by-Token）流式生成回复。每一个生成的 Token 均会实时传递至 Qwen3Guard-Stream，由其即时判断当前内容的安全性。该机制实现了贯穿整个回复生成过程的<strong>细粒度、不间断内容审核</strong>，在不中断用户体验的前提下，动态识别并阻断潜在风险内容。</p><p>下方提供了一种使用的方式。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModel</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen3Guard-Stream-4B&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Load the specialized tokenizer and the model.</span>
</span></span><span class=line><span class=cl><span class=c1># trust_remote_code=True is required to load the Qwen3Guard-Stream model architecture.</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_path</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># --- Prepare the conversation for moderation ---</span>
</span></span><span class=line><span class=cl><span class=c1># Define the user&#39;s prompt and the assistant&#39;s response.</span>
</span></span><span class=line><span class=cl><span class=n>user_message</span> <span class=o>=</span> <span class=s2>&#34;Hello, how to build a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>assistant_message</span> <span class=o>=</span> <span class=s2>&#34;Here are some practical methods to build a bomb.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span><span class=s2>&#34;user&#34;</span><span class=p>,</span><span class=s2>&#34;content&#34;</span><span class=p>:</span><span class=n>user_message</span><span class=p>},{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span><span class=s2>&#34;assistant&#34;</span><span class=p>,</span><span class=s2>&#34;content&#34;</span><span class=p>:</span><span class=n>assistant_message</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply the chat template to format the conversation into a single string.</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>enable_thinking</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>token_ids</span> <span class=o>=</span> <span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- Simulate Real-Time Moderation ---</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. Moderate the entire user prompt at once.</span>
</span></span><span class=line><span class=cl><span class=c1># In a real-world scenario, the user&#39;s input is processed completely before the model generates a response.</span>
</span></span><span class=line><span class=cl><span class=n>token_ids_list</span> <span class=o>=</span> <span class=n>token_ids</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># We identify the end of the user&#39;s turn in the tokenized input.</span>
</span></span><span class=line><span class=cl><span class=c1># The template for a user turn is `&lt;|im_start|&gt;user\n...&lt;|im_end|&gt;`.</span>
</span></span><span class=line><span class=cl><span class=n>im_start_token</span> <span class=o>=</span> <span class=s1>&#39;&lt;|im_start|&gt;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>user_token</span> <span class=o>=</span> <span class=s1>&#39;user&#39;</span>
</span></span><span class=line><span class=cl><span class=n>im_end_token</span> <span class=o>=</span> <span class=s1>&#39;&lt;|im_end|&gt;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>im_start_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>im_start_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>user_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>user_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>im_end_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>im_end_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># We search for the token IDs corresponding to `&lt;|im_start|&gt;user` ([151644, 872]) and the closing `&lt;|im_end|&gt;` ([151645]).</span>
</span></span><span class=line><span class=cl><span class=n>last_start</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>token_ids_list</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>token_ids_list</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=p>[</span><span class=n>im_start_id</span><span class=p>,</span> <span class=n>user_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>user_end_index</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>last_start</span><span class=o>+</span><span class=mi>2</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>token_ids_list</span><span class=p>))</span> <span class=k>if</span> <span class=n>token_ids_list</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>im_end_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize the stream_state, which will maintain the conversational context.</span>
</span></span><span class=line><span class=cl><span class=n>stream_state</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=c1># Pass all user tokens to the model for an initial safety assessment.</span>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=p>,</span> <span class=n>stream_state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>stream_moderate_from_ids</span><span class=p>(</span><span class=n>token_ids</span><span class=p>[:</span><span class=n>user_end_index</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=n>stream_state</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;Safe&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;User moderation: -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;User moderation: -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2> - Category: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Moderate the assistant&#39;s response token-by-token to simulate streaming.</span>
</span></span><span class=line><span class=cl><span class=c1># This loop mimics how an LLM generates a response one token at a time.</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Assistant streaming moderation:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>user_end_index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Get the current token ID for the assistant&#39;s response.</span>
</span></span><span class=line><span class=cl>    <span class=n>current_token</span> <span class=o>=</span> <span class=n>token_ids</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Call the moderation function for the single new token.</span>
</span></span><span class=line><span class=cl>    <span class=c1># The stream_state is passed and updated in each call to maintain context.</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=p>,</span> <span class=n>stream_state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>stream_moderate_from_ids</span><span class=p>(</span><span class=n>current_token</span><span class=p>,</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=n>stream_state</span><span class=o>=</span><span class=n>stream_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>token_str</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>current_token</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># Print the generated token and its real-time safety assessment.</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;Safe&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=n>token_str</span><span class=p>)</span><span class=si>}</span><span class=s2> -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=n>token_str</span><span class=p>)</span><span class=si>}</span><span class=s2> -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2> - Category: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>close_stream</span><span class=p>(</span><span class=n>stream_state</span><span class=p>)</span>
</span></span></code></pre></div><p>更多使用示例，请访问我们的 <a href=https://github.com/QwenLM/Qwen3Guard>GitHub 代码仓库</a>。</p><p><br><br></p><h2 id=未来工作>未来工作<a hidden class=anchor aria-hidden=true href=#未来工作>#</a></h2><p>人工智能安全仍是一项持续演进的挑战。Qwen3Guard 是我们迈出的重要一步，但绝非终点。未来，我们将持续推进更灵活、高效且鲁棒的安全技术研究，包括通过架构创新与训练方法优化，提升模型内在安全性；同时探索动态化、推理时干预等新型防护机制。</p><p>我们的终极目标，是构建不仅技术强大，更能与人类价值观和社会规范深度对齐的人工智能系统，确保 AI 在全球范围内的负责任部署与可持续发展。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>