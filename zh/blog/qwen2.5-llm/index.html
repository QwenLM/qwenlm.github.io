<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-LLM：扩展大型语言模型的边界 | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。
相比Qwen2系列，Qwen2.5带来了以下全新升级：
全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。
更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。
知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。
代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。
数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。
更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。
其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。
模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5-llm/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-llm/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-llm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-LLM：扩展大型语言模型的边界"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。
相比Qwen2系列，Qwen2.5带来了以下全新升级：
全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。
更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。
知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。
代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。
数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。
更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。
其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。
模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5-llm/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-09-19T00:00:03+08:00"><meta property="article:modified_time" content="2024-09-19T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-LLM：扩展大型语言模型的边界"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。
相比Qwen2系列，Qwen2.5带来了以下全新升级：
全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。
更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。
知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。
代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。
数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。
更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。
其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。
模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5-LLM：扩展大型语言模型的边界","item":"https://qwenlm.github.io/zh/blog/qwen2.5-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-LLM：扩展大型语言模型的边界","name":"Qwen2.5-LLM：扩展大型语言模型的边界","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。\n相比Qwen2系列，Qwen2.5带来了以下全新升级：\n全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。\n更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。\n知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。\n代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。\n数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。\n更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。\n其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。\n模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2.","keywords":[],"articleBody":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。\n相比Qwen2系列，Qwen2.5带来了以下全新升级：\n全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。\n更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。\n知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。\n代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。\n数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。\n更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。\n其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。\n模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2.0 开源许可协议，而 Qwen2.5-3B 和 Qwen2.5-72B 分别使用 Qwen Research 许可协议 和 Qwen 许可协议。\n模型 参数量 非Embedding参数量 层数 头数 (KV) Tie Embedding 长下文长度 生成长度 许可协议 Qwen2.5-0.5B 0.49B 0.36B 24 14 / 2 Yes 32K 8K Apache 2.0 Qwen2.5-1.5B 1.54B 1.31B 28 12 / 2 Yes 32K 8K Apache 2.0 Qwen2.5-3B 3.09B 2.77B 36 16 / 2 Yes 32K 8K Qwen Research Qwen2.5-7B 7.61B 6.53B 28 28 / 4 No 128K 8K Apache 2.0 Qwen2.5-14B 14.7B 13.1B 48 40 / 8 No 128K 8K Apache 2.0 Qwen2.5-32B 32.5B 31.0B 64 40 / 8 No 128K 8K Apache 2.0 Qwen2.5-72B 72.7B 70.0B 80 64 / 8 No 128K 8K Qwen 模型表现 在这一部分，我们将通过大量的基准测试来评估 Qwen2.5 基础语言模型和指令调优模型的表现。\nQwen2.5 基础语言模型评估 评估主要考察基础模型在自然语言理解、通用问答、代码、数学、科学知识、推理及多语言能力等方面的表现。\n涉及的评估数据集包括：\n通用任务：MMLU (5-shot)、MMLU-Pro (5-shot)、MMLU-redux (5-shot)、BBH (3-shot)、ARC-C (25-shot)、TruthfulQA (0-shot)、Winogrande (5-shot)、HellaSwag (10-shot)\n数学与科学任务：GPQA (5-shot)、Theorem QA (5-shot)、GSM8K (4-shot)、MATH (4-shot)\n代码任务：HumanEval (0-shot)、HumanEval+ (0-shot)、MBPP (0-shot)、MBPP+ (0-shot)、MultiPL-E (0-shot) (Python、C++、JAVA、PHP、TypeScript、C#、Bash、JavaScript)\n多语言任务：Multi-Exam (M3Exam 5-shot、IndoMMLU 3-shot、ruMMLU 5-shot、mMMLU 5-shot)、Multi-Understanding (BELEBELE 5-shot、XCOPA 5-shot、XWinograd 5-shot、XStoryCloze 0-shot、PAWS-X 5-shot)、Multi-Mathematics (MGSM 8-shot)、Multi-Translation (Flores-101 5-shot)\nQwen2.5-72B 表现 数据集 Llama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B 通用任务 MMLU 79.5 77.8 85.2 84.2 86.1 MMLU-Pro 52.8 51.6 61.6 55.7 58.1 MMLU-redux 75.0 72.9 - 80.5 83.9 BBH 81.0 78.9 85.9 82.4 86.3 ARC-C 68.8 70.7 - 68.9 72.4 TruthfulQA 45.6 51.0 - 54.8 60.4 WindoGrande 85.3 85.0 86.7 85.1 83.9 HellaSwag 88.0 88.7 - 87.3 87.6 数学与科学任务 GPQA 36.3 34.3 - 37.4 45.9 Theoremqa 32.3 35.9 - 42.8 42.4 MATH 42.5 41.7 53.8 50.9 62.1 MMLU-stem 73.7 71.7 - 79.6 82.7 GSM8K 77.6 83.7 89.0 89.0 91.5 代码任务 HumanEval 48.2 46.3 61.0 64.6 59.1 HumanEval+ 42.1 40.2 - 56.1 51.2 MBPP 70.4 71.7 73.0 76.9 84.7 MBPP+ 58.4 58.1 - 63.9 69.2 MultiPL-E 46.3 46.7 - 59.6 60.5 多语言任务 Multi-Exam 70.0 63.5 - 76.6 78.7 Multi-Understanding 79.9 77.7 - 80.7 89.6 Multi-Mathematics 67.1 62.9 - 76.0 76.7 Multi-Translation 38.0 23.3 - 37.8 39.0 Qwen2.5-72B 基础模型在各类任务上明显超过同类模型，以不到 1/5 的参数达到了与 Llama-3-405B 相当的表现。相比它的前身 Qwen2-72B，Qwen2.5-72B 几乎在所有基准评测上都有显著提升，尤其在通用任务、数学和代码竞赛中。\nQwen2.5-14/32B 表现 数据集 Qwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2-57B-A14B Qwen2.5-14B Qwen2.5-32B 通用任务 MMLU 74.3 75.2 77.2 76.5 79.7 83.3 MMLU-pro 44.1 49.1 48.3 43.0 51.2 55.1 MMLU-redux 69.0 - 74.1 72.4 76.6 82.0 BBH 66.8 74.9 76.4 67.0 78.2 84.5 ARC-C 63.6 71.4 65.6 64.1 67.3 70.4 Truthfulqa 57.4 40.1 53.9 57.7 58.4 57.8 Winogrande 81.5 59.7 84.9 79.5 - 82.0 Hellaswag 85.0 86.4 85.9 85.2 - 85.2 数学与科学任务 GPQA 30.8 34.9 37.4 34.3 32.8 48.0 Theoremqa 28.8 35.8 40.0 33.5 43.0 44.1 MATH 36.1 42.7 41.7 43.0 55.6 57.7 MMLU-stem 66.5 71.0 72.6 69.8 76.4 80.9 GSM8K 78.5 81.1 81.7 80.7 90.2 92.9 代码任务 HumanEval 43.3 54.9 46.3 53.0 56.7 58.5 HumanEval+ 40.2 46.3 40.2 46.3 51.2 52.4 MBPP 64.2 75.7 65.5 71.9 76.7 84.5 MBPP+ 53.9 60.2 55.4 57.4 63.2 67.2 MultiPL-E 38.5 48.0 39.5 49.8 53.5 59.4 多语言任务 Multi-Exam 61.6 65.8 58.3 65.5 70.6 75.4 Multi-Understanding 76.5 82.2 73.9 77.0 85.9 88.4 Multi-Mathematics 56.1 61.6 49.3 62.3 68.5 73.7 Multi-Translation 33.5 38.7 30.0 34.5 36.2 37.3 Qwen2.5-14B 模型在多项任务中表现稳健，尤其是在像MMLU和BBH这样的通用任务上，分别取得了 79.7 分和 78.2 分，超越了许多规模更大的竞争对手。Qwen2.5-32B 表现尤为出色，甚至优于参数更大的同类模型。特别是在数学和代码等挑战性任务中，Qwen2.5-32B 大幅领先其前身 Qwen1.5-32B，在 MATH 中获得 57.7分，在MBPP中获得 84.5 分。\nQwen2.5-7B 表现 数据集 Mistral-7B Llama3-8B Gemma2-9B Qwen2-7B Qwen2.5-7B #Non-emb Params 7.0B 7.0B 8.2B 6.5B 6.5B 通用任务 MMLU 64.2 66.6 71.3 70.3 74.2 MMLU-pro 30.9 35.4 44.7 40.1 45.0 MMLU-redux 58.1 61.6 67.9 68.1 71.1 BBH 56.1 57.7 68.2 62.3 70.4 ARC-C 60.0 59.3 68.2 60.6 63.7 Trurhfulqa 42.2 44.0 45.3 54.2 56.4 Winogrande 78.4 77.4 79.5 77.0 75.9 Hellaswag 83.3 82.1 81.9 80.7 80.2 数学与科学任务 GPQA 24.7 25.8 32.8 30.8 36.4 Theoremqa 19.2 22.1 28.9 29.6 36.0 MATH 10.2 20.5 37.7 43.5 49.8 MMLU-stem 50.1 55.3 65.1 64.2 72.3 GSM8K 36.2 55.3 70.7 80.2 85.4 代码任务 HumanEval 29.3 33.5 37.8 51.2 57.9 HumanEval+ 24.4 29.3 30.5 43.3 50.6 MBPP 51.1 53.9 62.2 64.2 74.9 MBPP+ 40.9 44.4 50.6 51.9 62.9 MultiPL-E 29.4 22.6 34.9 41.0 50.3 多语言任务 Multi-Exam 47.1 52.3 61.2 59.2 59.4 Multi-Understanding 63.3 68.6 78.3 72.0 79.3 Multi-Mathematics 26.3 36.3 53.0 57.5 57.8 Multi-Translation 23.3 31.9 36.5 31.5 32.4 Qwen2.5-7B在多个基准测试中超越了它的前代和同类竞争者。尽管它的非嵌入参数更少，但能够在各类任务中的表现更加出色。例如，Qwen2.5-7B 在 MMLU 通用基准测试中得分 74.2，在数学测试MATH中的得分为 49.8，而在代码任务HumanEval中取得了 57.9 分。\nQwen2.5-0.5B/1.5B/3B 表现 数据集 Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B 通用任务 MMLU 44.3 47.5 55.9 60.9 52.2 65.6 MMLU-pro 14.7 15.7 21.6 28.5 23.0 34.6 MMLU-redux 40.7 45.1 51.8 58.5 50.9 63.7 BBH 18.2 20.3 36.5 45.1 41.9 56.3 ARC-C 31.0 35.6 43.7 54.7 55.7 56.5 Trurhfulqa 39.7 40.2 45.9 46.6 36.2 48.9 Winogrande 56.9 56.3 65.0 65.0 71.5 71.1 Hellaswag 49.1 52.1 67.0 67.9 74.6 74.6 数学与科学任务 GPQA 29.8 24.8 20.7 24.2 25.3 26.3 Theoremqa 9.6 16.0 14.8 22.1 15.9 27.4 MATH 11.2 19.5 21.6 35.0 18.3 42.6 MMLU-stem 27.5 39.8 42.7 54.8 45.8 62.5 GSM8K 36.4 41.6 46.9 68.5 30.3 79.1 代码任务 HumanEval 22.6 30.5 34.8 37.2 19.5 42.1 HumanEval+ 18.9 26.8 29.9 32.9 15.9 36.0 MBPP 33.1 39.3 46.9 60.2 42.1 57.1 MBPP+ 27.6 33.8 37.6 49.6 33.6 49.4 MultiPL-E 16.3 18.9 27.9 33.1 17.6 41.2 多语言任务 Multi-Exam 29.4 30.8 43.1 47.9 38.1 54.6 Multi-Understanding 40.4 41.0 50.7 65.1 46.8 76.6 Multi-Mathematics 7.8 13.5 21.3 37.5 18.2 48.9 Multi-Translation 14.1 15.3 23.8 25.0 26.9 29.3 对于移动端模型，Qwen2.5-0.5B、1.5B 和 3B 在几乎所有评测中都表现了强劲的性能。值得一提的是，Qwen2.5-0.5B 模型，在一些数学和编程任务中甚至超过了Gemma2-2.6B。\n指令微调模型评估 评估主要考察指令微调模型在自然语言理解、通用问答、推理、代码、数学、指令遵循及人类对齐等方面的表现。\n涉及的评估数据集包括：\n通用任务：MMLU-Pro、MMLU-redux\n数学与科学任务：GPQA、GSM8K、MATH\n代码任务：HumanEval、MBPP、MultiPL-E、LiveCodeBench 2305-2409、LiveBench 0831\n指令和对齐任务：IFeval strict-prompt、Arena-Hard、AlignBench v1.1、MTbench\nQwen2.5-72B-Instruct 表现· 数据集 Mistral-Large2 Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct MMLU-Pro 69.4 66.4 73.3 64.4 71.1 MMLU-redux 83.0 83.0 86.2 81.6 86.8 GPQA 52.0 46.7 51.1 42.4 49.0 MATH 69.9 68.0 73.8 69.0 83.1 GSM8K 92.7 95.1 96.8 93.2 95.8 HumanEval 92.1 80.5 89.0 86.0 86.6 MBPP 80.0 84.2 84.5 80.2 88.2 MultiPL-E 76.9 68.2 73.5 69.2 75.1 LiveCodeBench 2305-2409 42.2 32.1 41.6 32.2 55.5 LiveBench 0831 48.5 46.6 53.2 41.5 52.3 IFeval strict-prompt 64.1 83.6 86.0 77.6 84.1 Arena-Hard 73.1 55.7 69.3 48.1 81.2 AlignBench v1.1 7.69 5.94 5.95 8.15 8.16 MTbench 8.61 8.79 9.08 9.12 9.35 Qwen2.5-72B-Instruct 模型展现出了极为优异的表现，甚至在多个核心任务上超越了参数量巨大的 Llama-3.1-405B，在数学（MATH: 83.1）、代码（LiveCodeBench: 55.5）以及对话任务（Arena-Hard: 81.2）中表现尤为突出。与基础模型 Qwen2.5-72B 及前身 Qwen2-72B-Instruct 相比，Qwen2.5-72B-Instruct 在各项任务上的表现都有显著提升。\nQwen2.5-Turbo \u0026 Qwen2.5-14B-Instruct \u0026 Qwen2.5-32B-Instruct 表现 数据集 Qwen2-57B-A14B-Instruct Gemma2-27B-IT GPT4o-mini Qwen-Turbo Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct MMLU-Pro 52.8 55.5 63.1 64.8 63.7 69.0 MMLU-redux 72.6 75.7 81.5 80.4 80.0 83.9 GPQA 34.3 38.4 40.2 44.4 45.5 49.5 MATH 49.1 54.4 70.2 81.0 80.0 83.1 GSM8K 85.3 90.4 93.2 93.6 94.8 95.9 HumanEval 79.9 78.7 88.4 86.6 83.5 88.4 MBPP 70.9 81.0 85.7 80.2 82.0 84.0 MultiPL-E 66.4 67.4 75.0 73.0 72.8 75.4 LiveCodeBench 2305-2409 22.5 - 40.7 43.1 42.6 51.2 LiveBench 0831 31.1 39.6 43.3 41.6 44.4 50.7 IFeval strict-prompt 59.9 77.1 80.4 74.9 81.0 79.5 Arena-Hard 17.8 57.5 74.9 68.4 68.3 74.5 AlignBench v1.1 7.02 7.22 7.81 7.99 7.94 7.93 MTbench 8.55 9.10 - 8.86 8.88 9.20 Qwen2.5-32B-Instruct 在大多数任务中表现优于同类规模的模型。与 GPT-4o-mini 相比，我们的开源模型 Qwen2.5-14B-Instruct 与 API 模型 Qwen-Turbo也在所有任务都中展现出了相当的竞争力。\nQwen2.5-7B-Instruct 表现 数据集 Gemma2-9b-IT Llama3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct MMLU-Pro 52.1 48.3 44.1 56.3 MMLU-redux 72.8 67.2 67.3 75.4 GPQA 32.8 32.8 34.3 36.4 MATH 44.3 51.9 52.9 75.5 GSM8K 76.7 84.5 85.7 91.6 HumanEval 68.9 72.6 79.9 84.8 MBPP 74.9 69.6 67.2 79.2 MultiPL-E 53.4 50.7 59.1 70.4 LiveCodeBench 2305-2409 18.9 8.3 23.9 28.7 LiveBench 0831 30.6 26.7 29.2 35.9 IFeval strict-prompt 70.1 75.9 54.7 71.2 Arena-Hard 41.6 27.8 25.0 52.0 AlignBench v1.1 7.05 4.75 7.13 7.33 MTbench 8.49 8.23 8.26 8.75 Qwen2.5-7B-Instruct 在除了 IFeval 的所有任务中表现均优于竞争对手 Gemma2-9b-IT 和 Llama3.1-8B-Instruct，尤其是在数学（MATH: 75.5）和代码（HumanEval: 84.8）任务上优势明显。\nQwen2.5-3B-Instruct 表现 数据集 Gemma2-2B-IT Phi3.5-mini-Instruct MiniCPM3-4B Qwen2.5-3B-Instruct Non-Emb Params 2.0B 3.6B 4.0B 2.8B MMLU-Pro 26.7 47.5 43.0 43.7 MMLU-redux 51.9 67.7 59.9 64.4 GPQA 29.3 27.2 31.3 30.3 MATH 26.6 48.5 46.6 65.9 GSM8K 63.2 86.2 81.1 86.7 HumanEval 68.9 72.6 74.4 74.4 MBPP 74.9 63.2 72.5 72.7 MultiPL-E 30.5 47.2 49.1 60.2 LiveCodeBench 2305-2409 5.8 15.8 23.8 19.9 LiveBench 0831 20.1 27.4 27.6 26.8 IFeval strict-prompt 51.0 52.1 68.4 58.2 在适用移动端的指令模型中，Qwen2.5-3B-Instruct 的参数量虽然少于 Phi3.5-mini-Instruct 和 MiniCPM3-4B，但在数学和编程任务上仍然具有优势，同时在语言理解方面也展现出不错的实力。\nQwen2.5-0.5B/1.5B-Instruct 表现 数据集 Qwen2-0.5B-Instruct Qwen2.5-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2.5-1.5B-Instruct MMLU-Pro 14.4 15.0 22.9 32.4 MMLU-redux 12.9 24.1 41.2 50.7 GPQA 23.7 29.8 21.2 29.8 MATH 13.9 34.4 25.3 55.2 GSM8K 40.1 49.6 61.6 73.2 HumanEval 31.1 35.4 42.1 61.6 MBPP 39.7 49.6 44.2 63.2 MultiPL-E 20.8 28.5 38.5 50.4 LiveCodeBench 2305-2409 1.6 5.1 4.5 14.8 LiveBench 0831 7.4 12.6 12.4 18.8 IFeval strict-prompt 14.6 27.9 29.0 42.5 Qwen2.5-1.5B-Instruct 及 Qwen2.5-0.5B-Instruct 的性能相比前代大幅提升，使它们尤其适合在资源极度受限的端侧场景下应用。\n多语言表现 为了更好地评估指令微调模型的多语言表现，我们收集并扩展了以下基准测试：\nIFEval（多语言）：我们将IFEval进行翻译，构建了多语言版本的 IFEval。在此过程中，我们移除了语言特定（例如“以字母A开头”）的测试用例。每种语言我们都收集了100个测试用例，包括阿拉伯语（ar）、西班牙语（es）、法语（fr）、印尼语（in）、日语（ja）、韩语（ko）、葡萄牙语（pt）和越南语（vi）。所有用例都由付费标注人员进行检查，并在必要时进行修改。\n知识能力测试：我们选用了五个类似 MMLU 的多选题基准测试来验证 Qwen2.5 系列模型的多语言知识掌握情况，包括：AMMLU（阿拉伯语）、JMMLU（日语）、KMMLU（韩语）、IndoMMLU（印尼语）和 TurkishMMLU（土耳其语）。此外，我们还展示了翻译版MMLU（即 okapi_MMLU，将英文MMLU翻译为多种语言）的性能表现。\nMGSM8K（扩展版）：在原版 MGSM8K 包含的语言外，我们还增加了阿拉伯语（ar）、韩语（ko）、葡萄牙语（pt）和越南语（vi）的支持。我们将 250 个测试用例翻译成这四种语言，保持与其他 MGSM8K 支持语言测试数量一致。所有示例也由付费标注人员进行了检查和必要的修改。\n文化差异：我们还使用了 BLEnD 基准测试，旨在评估大模型对于文化差异的处理能力，以进一步验证 Qwen2.5 系列模型的表现。\n数据集 Qwen2-72B-Instruct Llama3.1-70B-Instruct Qwen2.5-32B-Instruct Mistral-Large-Instruct-2407 (123B) GPT4o-mini Qwen2.5-72B-Instruct 指令遵循任务 IFEval（多语言） 79.69 80.47 82.68 82.69 85.03 86.98 知识任务 AMMLU（阿拉伯语） 68.85 70.08 70.44 69.24 69.73 72.44 JMMLU（日语） 77.37 73.89 76.55 75.77 73.74 80.56 KMMLU（韩语） 57.04 53.23 60.75 56.42 56.77 61.96 IndoMMLU（印尼语） 66.31 67.50 66.42 63.21 67.75 69.25 TurkishMMLU（土耳其语） 69.22 66.89 72.41 64.78 71.19 76.12 okapi MMLU（翻译） 77.84 76.49 77.16 78.37 73.44 79.97 数学任务 MGSM8K（扩展版） 82.72 73.31 87.15 89.01 87.36 88.16 文化差异任务 BLEnD 25.90 30.49 27.88 33.47 35.91 32.48 数据集 Qwen2-7B-Instruct Llama3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9B-Instruct Mistral-Nemo-Instruct-2407 (12B) Qwen2.5-14B-Instruct 指令遵循任务 IFEval（多语言） 51.43 60.68 74.87 77.47 64.59 77.08 知识任务 AMMLU（阿拉伯语） 54.87 54.28 59.78 60.26 53.92 66.81 JMMLU（日语） 57.71 53.26 61.88 64.59 55.17 72.78 KMMLU（韩语） 43.96 42.28 46.59 46.24 42.22 59.71 IndoMMLU（印尼语） 54.05 53.92 56.42 61.73 50.76 65.09 TurkishMMLU（土耳其语） 49.27 45.61 54.28 55.44 34.44 66.85 okapi MMLU（翻译） 60.47 55.18 66.98 46.72 59.65 72.12 数学任务 MGSM8K（扩展版） 56.13 66.05 66.11 78.37 54.75 82.27 文化差异任务 BLEnD 22.49 19.47 23.66 28.31 26.61 26.99 实例演示 我们准备了一些实例演示，来体现 Qwen2.5 的新特性和改进之处，涵盖了生成JSON格式输出、撰写长篇内容以及理解结构化数据等能力。\nExample: Generating JSON Output\rNext\rJSON Output\rExample: Structured Data Understanding\rNext\rTable Understanding\rExample: Long Text Generation\rNext\rText Generation\r","wordCount":"1564","inLanguage":"zh","datePublished":"2024-09-19T00:00:03+08:00","dateModified":"2024-09-19T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5-llm/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-LLM：扩展大型语言模型的边界</h1><div class=post-meta><span title='2024-09-19 00:00:03 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1564 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5-llm/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过<a href=https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm>阿里云大模型服务平台</a>的API服务进行体验。</p><p>相比Qwen2系列，Qwen2.5带来了以下全新升级：</p><ol><li><p><strong>全面开源</strong>：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— <strong>Qwen2.5-14B</strong> 和 <strong>Qwen2.5-32B</strong>，以及一款适合移动端的 <strong>Qwen2.5-3B</strong>。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。</p></li><li><p><strong>更大规模、更高质量的预数据训练集</strong>：我们的预训练数据集规模从 7T tokens 扩展到了 <strong>18T</strong> tokens。</p></li><li><p><strong>知识储备升级</strong>：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 <strong>74.2</strong>，和从84.2提升到 <strong>86.1</strong>。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。</p></li><li><p><strong>代码能力增强</strong>：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 <strong>55.5</strong>、<strong>75.1</strong> 和 <strong>88.2</strong>，优于Qwen2-72B-Instruct的32.2、69.2和80.2。</p></li><li><p><strong>数学能力提升</strong>：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 <strong>75.5/83.1</strong>。</p></li><li><p><strong>更符合人类偏好</strong>：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 <strong>48.1</strong> 大幅提升至 <strong>81.2</strong>，MT-Bench得分也从 <strong>9.12</strong> 提升到了 <strong>9.35</strong>，与之前的Qwen2-72B相比提升显著。</p></li><li><p><strong>其他核心能力提升</strong>：Qwen2.5在 <strong>指令跟随</strong>、生成 <strong>长文本</strong>（从1K升级到 <strong>8K tokens</strong>）、理解 <strong>结构化数据</strong>（如表格），以及生成 <strong>结构化输出</strong>（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 <strong>系统提示</strong>，用户可以给模型设置 <strong>特定角色</strong> 或 <strong>自定义条件</strong>。</p></li></ol><h1 id=模型基础信息>模型基础信息<a hidden class=anchor aria-hidden=true href=#模型基础信息>#</a></h1><p>本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2.0 开源许可协议，而 Qwen2.5-3B 和 Qwen2.5-72B 分别使用 Qwen Research 许可协议 和 Qwen 许可协议。</p><table><thead><tr><th style=text-align:left>模型</th><th style=text-align:center>参数量</th><th style=text-align:center>非Embedding参数量</th><th style=text-align:center>层数</th><th style=text-align:center>头数 (KV)</th><th style=text-align:center>Tie Embedding</th><th style=text-align:center>长下文长度</th><th style=text-align:center>生成长度</th><th style=text-align:center>许可协议</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2.5-0.5B</td><td style=text-align:center>0.49B</td><td style=text-align:center>0.36B</td><td style=text-align:center>24</td><td style=text-align:center>14 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-1.5B</td><td style=text-align:center>1.54B</td><td style=text-align:center>1.31B</td><td style=text-align:center>28</td><td style=text-align:center>12 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-3B</td><td style=text-align:center>3.09B</td><td style=text-align:center>2.77B</td><td style=text-align:center>36</td><td style=text-align:center>16 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen Research</td></tr><tr><td style=text-align:left>Qwen2.5-7B</td><td style=text-align:center>7.61B</td><td style=text-align:center>6.53B</td><td style=text-align:center>28</td><td style=text-align:center>28 / 4</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-14B</td><td style=text-align:center>14.7B</td><td style=text-align:center>13.1B</td><td style=text-align:center>48</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-32B</td><td style=text-align:center>32.5B</td><td style=text-align:center>31.0B</td><td style=text-align:center>64</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-72B</td><td style=text-align:center>72.7B</td><td style=text-align:center>70.0B</td><td style=text-align:center>80</td><td style=text-align:center>64 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen</td></tr></tbody></table><h1 id=模型表现>模型表现<a hidden class=anchor aria-hidden=true href=#模型表现>#</a></h1><p>在这一部分，我们将通过大量的基准测试来评估 Qwen2.5 基础语言模型和指令调优模型的表现。</p><h2 id=qwen25-基础语言模型评估>Qwen2.5 基础语言模型评估<a hidden class=anchor aria-hidden=true href=#qwen25-基础语言模型评估>#</a></h2><p>评估主要考察基础模型在自然语言理解、通用问答、代码、数学、科学知识、推理及多语言能力等方面的表现。</p><p>涉及的评估数据集包括：</p><p><strong>通用任务</strong>：MMLU (5-shot)、MMLU-Pro (5-shot)、MMLU-redux (5-shot)、BBH (3-shot)、ARC-C (25-shot)、TruthfulQA (0-shot)、Winogrande (5-shot)、HellaSwag (10-shot)</p><p><strong>数学与科学任务</strong>：GPQA (5-shot)、Theorem QA (5-shot)、GSM8K (4-shot)、MATH (4-shot)</p><p><strong>代码任务</strong>：HumanEval (0-shot)、HumanEval+ (0-shot)、MBPP (0-shot)、MBPP+ (0-shot)、MultiPL-E (0-shot) (Python、C++、JAVA、PHP、TypeScript、C#、Bash、JavaScript)</p><p><strong>多语言任务</strong>：Multi-Exam (M3Exam 5-shot、IndoMMLU 3-shot、ruMMLU 5-shot、mMMLU 5-shot)、Multi-Understanding (BELEBELE 5-shot、XCOPA 5-shot、XWinograd 5-shot、XStoryCloze 0-shot、PAWS-X 5-shot)、Multi-Mathematics (MGSM 8-shot)、Multi-Translation (Flores-101 5-shot)</p><h3 id=qwen25-72b-表现>Qwen2.5-72B 表现<a hidden class=anchor aria-hidden=true href=#qwen25-72b-表现>#</a></h3><table><thead><tr><th style=text-align:left>数据集</th><th style=text-align:center>Llama-3-70B</th><th style=text-align:center>Mixtral-8x22B</th><th style=text-align:center>Llama-3-405B</th><th style=text-align:center>Qwen2-72B</th><th style=text-align:center><strong>Qwen2.5-72B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>通用任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>79.5</td><td style=text-align:center>77.8</td><td style=text-align:center>85.2</td><td style=text-align:center>84.2</td><td style=text-align:center><strong>86.1</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>52.8</td><td style=text-align:center>51.6</td><td style=text-align:center><strong>61.6</strong></td><td style=text-align:center>55.7</td><td style=text-align:center>58.1</td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>75.0</td><td style=text-align:center>72.9</td><td style=text-align:center>-</td><td style=text-align:center>80.5</td><td style=text-align:center><strong>83.9</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>81.0</td><td style=text-align:center>78.9</td><td style=text-align:center>85.9</td><td style=text-align:center>82.4</td><td style=text-align:center><strong>86.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>68.8</td><td style=text-align:center>70.7</td><td style=text-align:center>-</td><td style=text-align:center>68.9</td><td style=text-align:center><strong>72.4</strong></td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>45.6</td><td style=text-align:center>51.0</td><td style=text-align:center>-</td><td style=text-align:center>54.8</td><td style=text-align:center><strong>60.4</strong></td></tr><tr><td style=text-align:left>WindoGrande</td><td style=text-align:center>85.3</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.7</strong></td><td style=text-align:center>85.1</td><td style=text-align:center>83.9</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center>88.0</td><td style=text-align:center><strong>88.7</strong></td><td style=text-align:center>-</td><td style=text-align:center>87.3</td><td style=text-align:center>87.6</td></tr><tr><td style=text-align:left><em><strong>数学与科学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>36.3</td><td style=text-align:center>34.3</td><td style=text-align:center>-</td><td style=text-align:center>37.4</td><td style=text-align:center><strong>45.9</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>32.3</td><td style=text-align:center>35.9</td><td style=text-align:center>-</td><td style=text-align:center><strong>42.8</strong></td><td style=text-align:center>42.4</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>42.5</td><td style=text-align:center>41.7</td><td style=text-align:center>53.8</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>62.1</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>73.7</td><td style=text-align:center>71.7</td><td style=text-align:center>-</td><td style=text-align:center>79.6</td><td style=text-align:center><strong>82.7</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>77.6</td><td style=text-align:center>83.7</td><td style=text-align:center>89.0</td><td style=text-align:center>89.0</td><td style=text-align:center><strong>91.5</strong></td></tr><tr><td style=text-align:left><em><strong>代码任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>48.2</td><td style=text-align:center>46.3</td><td style=text-align:center><strong>61.0</strong></td><td style=text-align:center>64.6</td><td style=text-align:center>59.1</td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>42.1</td><td style=text-align:center>40.2</td><td style=text-align:center>-</td><td style=text-align:center><strong>56.1</strong></td><td style=text-align:center>51.2</td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>70.4</td><td style=text-align:center>71.7</td><td style=text-align:center>73.0</td><td style=text-align:center>76.9</td><td style=text-align:center><strong>84.7</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>58.4</td><td style=text-align:center>58.1</td><td style=text-align:center>-</td><td style=text-align:center>63.9</td><td style=text-align:center><strong>69.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>46.3</td><td style=text-align:center>46.7</td><td style=text-align:center>-</td><td style=text-align:center>59.6</td><td style=text-align:center><strong>60.5</strong></td></tr><tr><td style=text-align:left><em><strong>多语言任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>70.0</td><td style=text-align:center>63.5</td><td style=text-align:center>-</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>78.7</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>79.9</td><td style=text-align:center>77.7</td><td style=text-align:center>-</td><td style=text-align:center>80.7</td><td style=text-align:center><strong>89.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>67.1</td><td style=text-align:center>62.9</td><td style=text-align:center>-</td><td style=text-align:center>76.0</td><td style=text-align:center><strong>76.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>38.0</td><td style=text-align:center>23.3</td><td style=text-align:center>-</td><td style=text-align:center>37.8</td><td style=text-align:center><strong>39.0</strong></td></tr></tbody></table><p>Qwen2.5-72B 基础模型在各类任务上明显超过同类模型，以不到 1/5 的参数达到了与 Llama-3-405B 相当的表现。相比它的前身 Qwen2-72B，Qwen2.5-72B 几乎在所有基准评测上都有显著提升，尤其在通用任务、数学和代码竞赛中。</p><h3 id=qwen25-1432b-表现>Qwen2.5-14/32B 表现<a hidden class=anchor aria-hidden=true href=#qwen25-1432b-表现>#</a></h3><table><thead><tr><th style=text-align:left>数据集</th><th style=text-align:center>Qwen1.5-32B</th><th style=text-align:center>Gemma2-27B</th><th style=text-align:center>Yi-1.5-34B</th><th style=text-align:center>Qwen2-57B-A14B</th><th style=text-align:center><strong>Qwen2.5-14B</strong></th><th style=text-align:center><strong>Qwen2.5-32B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>通用任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>74.3</td><td style=text-align:center>75.2</td><td style=text-align:center>77.2</td><td style=text-align:center>76.5</td><td style=text-align:center>79.7</td><td style=text-align:center><strong>83.3</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>44.1</td><td style=text-align:center>49.1</td><td style=text-align:center>48.3</td><td style=text-align:center>43.0</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>55.1</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>69.0</td><td style=text-align:center>-</td><td style=text-align:center>74.1</td><td style=text-align:center>72.4</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>82.0</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>66.8</td><td style=text-align:center>74.9</td><td style=text-align:center>76.4</td><td style=text-align:center>67.0</td><td style=text-align:center>78.2</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>63.6</td><td style=text-align:center><strong>71.4</strong></td><td style=text-align:center>65.6</td><td style=text-align:center>64.1</td><td style=text-align:center>67.3</td><td style=text-align:center>70.4</td></tr><tr><td style=text-align:left>Truthfulqa</td><td style=text-align:center>57.4</td><td style=text-align:center>40.1</td><td style=text-align:center>53.9</td><td style=text-align:center>57.7</td><td style=text-align:center><strong>58.4</strong></td><td style=text-align:center>57.8</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>81.5</td><td style=text-align:center>59.7</td><td style=text-align:center><strong>84.9</strong></td><td style=text-align:center>79.5</td><td style=text-align:center>-</td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.4</strong></td><td style=text-align:center>85.9</td><td style=text-align:center>85.2</td><td style=text-align:center>-</td><td style=text-align:center>85.2</td></tr><tr><td style=text-align:left><em><strong>数学与科学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>30.8</td><td style=text-align:center>34.9</td><td style=text-align:center>37.4</td><td style=text-align:center>34.3</td><td style=text-align:center>32.8</td><td style=text-align:center><strong>48.0</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>28.8</td><td style=text-align:center>35.8</td><td style=text-align:center>40.0</td><td style=text-align:center>33.5</td><td style=text-align:center>43.0</td><td style=text-align:center><strong>44.1</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>36.1</td><td style=text-align:center>42.7</td><td style=text-align:center>41.7</td><td style=text-align:center>43.0</td><td style=text-align:center>55.6</td><td style=text-align:center><strong>57.7</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>66.5</td><td style=text-align:center>71.0</td><td style=text-align:center>72.6</td><td style=text-align:center>69.8</td><td style=text-align:center>76.4</td><td style=text-align:center><strong>80.9</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>78.5</td><td style=text-align:center>81.1</td><td style=text-align:center>81.7</td><td style=text-align:center>80.7</td><td style=text-align:center>90.2</td><td style=text-align:center><strong>92.9</strong></td></tr><tr><td style=text-align:left><em><strong>代码任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>43.3</td><td style=text-align:center>54.9</td><td style=text-align:center>46.3</td><td style=text-align:center>53.0</td><td style=text-align:center>56.7</td><td style=text-align:center><strong>58.5</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>52.4</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>64.2</td><td style=text-align:center>75.7</td><td style=text-align:center>65.5</td><td style=text-align:center>71.9</td><td style=text-align:center>76.7</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>53.9</td><td style=text-align:center>60.2</td><td style=text-align:center>55.4</td><td style=text-align:center>57.4</td><td style=text-align:center>63.2</td><td style=text-align:center><strong>67.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>38.5</td><td style=text-align:center>48.0</td><td style=text-align:center>39.5</td><td style=text-align:center>49.8</td><td style=text-align:center>53.5</td><td style=text-align:center><strong>59.4</strong></td></tr><tr><td style=text-align:left><em><strong>多语言任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>61.6</td><td style=text-align:center>65.8</td><td style=text-align:center>58.3</td><td style=text-align:center>65.5</td><td style=text-align:center>70.6</td><td style=text-align:center><strong>75.4</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>76.5</td><td style=text-align:center>82.2</td><td style=text-align:center>73.9</td><td style=text-align:center>77.0</td><td style=text-align:center>85.9</td><td style=text-align:center><strong>88.4</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>56.1</td><td style=text-align:center>61.6</td><td style=text-align:center>49.3</td><td style=text-align:center>62.3</td><td style=text-align:center>68.5</td><td style=text-align:center><strong>73.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>33.5</td><td style=text-align:center>38.7</td><td style=text-align:center>30.0</td><td style=text-align:center>34.5</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>37.3</strong></td></tr></tbody></table><p>Qwen2.5-14B 模型在多项任务中表现稳健，尤其是在像MMLU和BBH这样的通用任务上，分别取得了 79.7 分和 78.2 分，超越了许多规模更大的竞争对手。Qwen2.5-32B 表现尤为出色，甚至优于参数更大的同类模型。特别是在数学和代码等挑战性任务中，Qwen2.5-32B 大幅领先其前身 Qwen1.5-32B，在 MATH 中获得 57.7分，在MBPP中获得 84.5 分。</p><h3 id=qwen25-7b-表现>Qwen2.5-7B 表现<a hidden class=anchor aria-hidden=true href=#qwen25-7b-表现>#</a></h3><table><thead><tr><th style=text-align:left>数据集</th><th style=text-align:center>Mistral-7B</th><th style=text-align:center>Llama3-8B</th><th style=text-align:center>Gemma2-9B</th><th style=text-align:center>Qwen2-7B</th><th style=text-align:center><strong>Qwen2.5-7B</strong></th></tr></thead><tbody><tr><td style=text-align:left>#Non-emb Params</td><td style=text-align:center>7.0B</td><td style=text-align:center>7.0B</td><td style=text-align:center>8.2B</td><td style=text-align:center>6.5B</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:left><em><strong>通用任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>64.2</td><td style=text-align:center>66.6</td><td style=text-align:center>71.3</td><td style=text-align:center>70.3</td><td style=text-align:center><strong>74.2</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>30.9</td><td style=text-align:center>35.4</td><td style=text-align:center>44.7</td><td style=text-align:center>40.1</td><td style=text-align:center><strong>45.0</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>58.1</td><td style=text-align:center>61.6</td><td style=text-align:center>67.9</td><td style=text-align:center>68.1</td><td style=text-align:center><strong>71.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>56.1</td><td style=text-align:center>57.7</td><td style=text-align:center>68.2</td><td style=text-align:center>62.3</td><td style=text-align:center><strong>70.4</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>60.0</td><td style=text-align:center>59.3</td><td style=text-align:center><strong>68.2</strong></td><td style=text-align:center>60.6</td><td style=text-align:center>63.7</td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>42.2</td><td style=text-align:center>44.0</td><td style=text-align:center>45.3</td><td style=text-align:center>54.2</td><td style=text-align:center><strong>56.4</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>78.4</td><td style=text-align:center>77.4</td><td style=text-align:center><strong>79.5</strong></td><td style=text-align:center>77.0</td><td style=text-align:center>75.9</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center><strong>83.3</strong></td><td style=text-align:center>82.1</td><td style=text-align:center>81.9</td><td style=text-align:center>80.7</td><td style=text-align:center>80.2</td></tr><tr><td style=text-align:left><em><strong>数学与科学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>24.7</td><td style=text-align:center>25.8</td><td style=text-align:center>32.8</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>36.4</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>19.2</td><td style=text-align:center>22.1</td><td style=text-align:center>28.9</td><td style=text-align:center>29.6</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>10.2</td><td style=text-align:center>20.5</td><td style=text-align:center>37.7</td><td style=text-align:center>43.5</td><td style=text-align:center><strong>49.8</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>50.1</td><td style=text-align:center>55.3</td><td style=text-align:center>65.1</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>72.3</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.2</td><td style=text-align:center>55.3</td><td style=text-align:center>70.7</td><td style=text-align:center>80.2</td><td style=text-align:center><strong>85.4</strong></td></tr><tr><td style=text-align:left><em><strong>代码任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>33.5</td><td style=text-align:center>37.8</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>57.9</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>24.4</td><td style=text-align:center>29.3</td><td style=text-align:center>30.5</td><td style=text-align:center>43.3</td><td style=text-align:center><strong>50.6</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>51.1</td><td style=text-align:center>53.9</td><td style=text-align:center>62.2</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>74.9</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>40.9</td><td style=text-align:center>44.4</td><td style=text-align:center>50.6</td><td style=text-align:center>51.9</td><td style=text-align:center><strong>62.9</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>29.4</td><td style=text-align:center>22.6</td><td style=text-align:center>34.9</td><td style=text-align:center>41.0</td><td style=text-align:center><strong>50.3</strong></td></tr><tr><td style=text-align:left><em><strong>多语言任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>47.1</td><td style=text-align:center>52.3</td><td style=text-align:center><strong>61.2</strong></td><td style=text-align:center>59.2</td><td style=text-align:center>59.4</td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>63.3</td><td style=text-align:center>68.6</td><td style=text-align:center>78.3</td><td style=text-align:center>72.0</td><td style=text-align:center><strong>79.3</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>26.3</td><td style=text-align:center>36.3</td><td style=text-align:center>53.0</td><td style=text-align:center>57.5</td><td style=text-align:center><strong>57.8</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>23.3</td><td style=text-align:center>31.9</td><td style=text-align:center><strong>36.5</strong></td><td style=text-align:center>31.5</td><td style=text-align:center>32.4</td></tr></tbody></table><p>Qwen2.5-7B在多个基准测试中超越了它的前代和同类竞争者。尽管它的非嵌入参数更少，但能够在各类任务中的表现更加出色。例如，Qwen2.5-7B 在 MMLU 通用基准测试中得分 74.2，在数学测试MATH中的得分为 49.8，而在代码任务HumanEval中取得了 57.9 分。</p><h3 id=qwen25-05b15b3b-表现>Qwen2.5-0.5B/1.5B/3B 表现<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b3b-表现>#</a></h3><table><thead><tr><th style=text-align:left>数据集</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center><strong>Qwen2.5-0.5B</strong></th><th style=text-align:center>Qwen2-1.5B</th><th style=text-align:center><strong>Qwen2.5-1.5B</strong></th><th style=text-align:center>Gemma2-2.6B</th><th style=text-align:center><strong>Qwen2.5-3B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>通用任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>44.3</td><td style=text-align:center>47.5</td><td style=text-align:center>55.9</td><td style=text-align:center>60.9</td><td style=text-align:center>52.2</td><td style=text-align:center><strong>65.6</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>14.7</td><td style=text-align:center>15.7</td><td style=text-align:center>21.6</td><td style=text-align:center>28.5</td><td style=text-align:center>23.0</td><td style=text-align:center><strong>34.6</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>40.7</td><td style=text-align:center>45.1</td><td style=text-align:center>51.8</td><td style=text-align:center>58.5</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>63.7</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>18.2</td><td style=text-align:center>20.3</td><td style=text-align:center>36.5</td><td style=text-align:center>45.1</td><td style=text-align:center>41.9</td><td style=text-align:center><strong>56.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>31.0</td><td style=text-align:center>35.6</td><td style=text-align:center>43.7</td><td style=text-align:center>54.7</td><td style=text-align:center>55.7</td><td style=text-align:center><strong>56.5</strong></td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>39.7</td><td style=text-align:center>40.2</td><td style=text-align:center>45.9</td><td style=text-align:center>46.6</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>56.9</td><td style=text-align:center>56.3</td><td style=text-align:center>65.0</td><td style=text-align:center>65.0</td><td style=text-align:center><strong>71.5</strong></td><td style=text-align:center>71.1</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>49.1</td><td style=text-align:center>52.1</td><td style=text-align:center>67.0</td><td style=text-align:center>67.9</td><td style=text-align:center>74.6</td><td style=text-align:center><strong>74.6</strong></td></tr><tr><td style=text-align:left><em><strong>数学与科学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>29.8</td><td style=text-align:center>24.8</td><td style=text-align:center>20.7</td><td style=text-align:center>24.2</td><td style=text-align:center>25.3</td><td style=text-align:center><strong>26.3</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>9.6</td><td style=text-align:center>16.0</td><td style=text-align:center>14.8</td><td style=text-align:center>22.1</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>27.4</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>11.2</td><td style=text-align:center>19.5</td><td style=text-align:center>21.6</td><td style=text-align:center>35.0</td><td style=text-align:center>18.3</td><td style=text-align:center><strong>42.6</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>27.5</td><td style=text-align:center>39.8</td><td style=text-align:center>42.7</td><td style=text-align:center>54.8</td><td style=text-align:center>45.8</td><td style=text-align:center><strong>62.5</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.4</td><td style=text-align:center>41.6</td><td style=text-align:center>46.9</td><td style=text-align:center>68.5</td><td style=text-align:center>30.3</td><td style=text-align:center><strong>79.1</strong></td></tr><tr><td style=text-align:left><em><strong>代码任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>22.6</td><td style=text-align:center>30.5</td><td style=text-align:center>34.8</td><td style=text-align:center>37.2</td><td style=text-align:center>19.5</td><td style=text-align:center><strong>42.1</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>18.9</td><td style=text-align:center>26.8</td><td style=text-align:center>29.9</td><td style=text-align:center>32.9</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>33.1</td><td style=text-align:center>39.3</td><td style=text-align:center>46.9</td><td style=text-align:center><strong>60.2</strong></td><td style=text-align:center>42.1</td><td style=text-align:center>57.1</td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>27.6</td><td style=text-align:center>33.8</td><td style=text-align:center>37.6</td><td style=text-align:center><strong>49.6</strong></td><td style=text-align:center>33.6</td><td style=text-align:center>49.4</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>16.3</td><td style=text-align:center>18.9</td><td style=text-align:center>27.9</td><td style=text-align:center>33.1</td><td style=text-align:center>17.6</td><td style=text-align:center><strong>41.2</strong></td></tr><tr><td style=text-align:left><em><strong>多语言任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>29.4</td><td style=text-align:center>30.8</td><td style=text-align:center>43.1</td><td style=text-align:center>47.9</td><td style=text-align:center>38.1</td><td style=text-align:center><strong>54.6</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>40.4</td><td style=text-align:center>41.0</td><td style=text-align:center>50.7</td><td style=text-align:center>65.1</td><td style=text-align:center>46.8</td><td style=text-align:center><strong>76.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>7.8</td><td style=text-align:center>13.5</td><td style=text-align:center>21.3</td><td style=text-align:center>37.5</td><td style=text-align:center>18.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>14.1</td><td style=text-align:center>15.3</td><td style=text-align:center>23.8</td><td style=text-align:center>25.0</td><td style=text-align:center>26.9</td><td style=text-align:center><strong>29.3</strong></td></tr></tbody></table><p>对于移动端模型，Qwen2.5-0.5B、1.5B 和 3B 在几乎所有评测中都表现了强劲的性能。值得一提的是，Qwen2.5-0.5B 模型，在一些数学和编程任务中甚至超过了Gemma2-2.6B。</p><h2 id=指令微调模型评估>指令微调模型评估<a hidden class=anchor aria-hidden=true href=#指令微调模型评估>#</a></h2><p>评估主要考察指令微调模型在自然语言理解、通用问答、推理、代码、数学、指令遵循及人类对齐等方面的表现。</p><p>涉及的评估数据集包括：</p><p><strong>通用任务</strong>：MMLU-Pro、MMLU-redux</p><p><strong>数学与科学任务</strong>：GPQA、GSM8K、MATH</p><p><strong>代码任务</strong>：HumanEval、MBPP、MultiPL-E、LiveCodeBench 2305-2409、LiveBench 0831</p><p><strong>指令和对齐任务</strong>：IFeval strict-prompt、Arena-Hard、AlignBench v1.1、MTbench</p><h3 id=qwen25-72b-instruct-表现>Qwen2.5-72B-Instruct 表现·<a hidden class=anchor aria-hidden=true href=#qwen25-72b-instruct-表现>#</a></h3><table><thead><tr><th>数据集</th><th>Mistral-Large2 Instruct</th><th>Llama-3.1-70B-Instruct</th><th>Llama-3.1-405B-Instruct</th><th>Qwen2-72B-Instruct</th><th><strong>Qwen2.5-72B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>69.4</td><td>66.4</td><td><strong>73.3</strong></td><td>64.4</td><td>71.1</td></tr><tr><td>MMLU-redux</td><td>83.0</td><td>83.0</td><td>86.2</td><td>81.6</td><td><strong>86.8</strong></td></tr><tr><td>GPQA</td><td><strong>52.0</strong></td><td>46.7</td><td>51.1</td><td>42.4</td><td>49.0</td></tr><tr><td>MATH</td><td>69.9</td><td>68.0</td><td>73.8</td><td>69.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>92.7</td><td>95.1</td><td><strong>96.8</strong></td><td>93.2</td><td>95.8</td></tr><tr><td>HumanEval</td><td><strong>92.1</strong></td><td>80.5</td><td>89.0</td><td>86.0</td><td>86.6</td></tr><tr><td>MBPP</td><td>80.0</td><td>84.2</td><td>84.5</td><td>80.2</td><td><strong>88.2</strong></td></tr><tr><td>MultiPL-E</td><td><strong>76.9</strong></td><td>68.2</td><td>73.5</td><td>69.2</td><td>75.1</td></tr><tr><td>LiveCodeBench 2305-2409</td><td>42.2</td><td>32.1</td><td>41.6</td><td>32.2</td><td><strong>55.5</strong></td></tr><tr><td>LiveBench 0831</td><td>48.5</td><td>46.6</td><td><strong>53.2</strong></td><td>41.5</td><td>52.3</td></tr><tr><td>IFeval strict-prompt</td><td>64.1</td><td>83.6</td><td><strong>86.0</strong></td><td>77.6</td><td>84.1</td></tr><tr><td>Arena-Hard</td><td>73.1</td><td>55.7</td><td>69.3</td><td>48.1</td><td><strong>81.2</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.69</td><td>5.94</td><td>5.95</td><td>8.15</td><td><strong>8.16</strong></td></tr><tr><td>MTbench</td><td>8.61</td><td>8.79</td><td>9.08</td><td>9.12</td><td><strong>9.35</strong></td></tr></tbody></table><p>Qwen2.5-72B-Instruct 模型展现出了极为优异的表现，甚至在多个核心任务上超越了参数量巨大的 Llama-3.1-405B，在数学（MATH: 83.1）、代码（LiveCodeBench: 55.5）以及对话任务（Arena-Hard: 81.2）中表现尤为突出。与基础模型 Qwen2.5-72B 及前身 Qwen2-72B-Instruct 相比，Qwen2.5-72B-Instruct 在各项任务上的表现都有显著提升。</p><h3 id=qwen25-turbo--qwen25-14b-instruct--qwen25-32b-instruct-表现>Qwen2.5-Turbo & Qwen2.5-14B-Instruct & Qwen2.5-32B-Instruct 表现<a hidden class=anchor aria-hidden=true href=#qwen25-turbo--qwen25-14b-instruct--qwen25-32b-instruct-表现>#</a></h3><table><thead><tr><th>数据集</th><th>Qwen2-57B-A14B-Instruct</th><th>Gemma2-27B-IT</th><th>GPT4o-mini</th><th><strong>Qwen-Turbo</strong></th><th><strong>Qwen2.5-14B-Instruct</strong></th><th><strong>Qwen2.5-32B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.8</td><td>55.5</td><td>63.1</td><td>64.8</td><td>63.7</td><td><strong>69.0</strong></td></tr><tr><td>MMLU-redux</td><td>72.6</td><td>75.7</td><td>81.5</td><td>80.4</td><td>80.0</td><td><strong>83.9</strong></td></tr><tr><td>GPQA</td><td>34.3</td><td>38.4</td><td>40.2</td><td>44.4</td><td>45.5</td><td><strong>49.5</strong></td></tr><tr><td>MATH</td><td>49.1</td><td>54.4</td><td>70.2</td><td>81.0</td><td>80.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>85.3</td><td>90.4</td><td>93.2</td><td>93.6</td><td>94.8</td><td><strong>95.9</strong></td></tr><tr><td>HumanEval</td><td>79.9</td><td>78.7</td><td><strong>88.4</strong></td><td>86.6</td><td>83.5</td><td><strong>88.4</strong></td></tr><tr><td>MBPP</td><td>70.9</td><td>81.0</td><td><strong>85.7</strong></td><td>80.2</td><td>82.0</td><td>84.0</td></tr><tr><td>MultiPL-E</td><td>66.4</td><td>67.4</td><td>75.0</td><td>73.0</td><td>72.8</td><td><strong>75.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>22.5</td><td>-</td><td>40.7</td><td>43.1</td><td>42.6</td><td><strong>51.2</strong></td></tr><tr><td>LiveBench 0831</td><td>31.1</td><td>39.6</td><td>43.3</td><td>41.6</td><td>44.4</td><td><strong>50.7</strong></td></tr><tr><td>IFeval strict-prompt</td><td>59.9</td><td>77.1</td><td>80.4</td><td>74.9</td><td><strong>81.0</strong></td><td>79.5</td></tr><tr><td>Arena-Hard</td><td>17.8</td><td>57.5</td><td><strong>74.9</strong></td><td>68.4</td><td>68.3</td><td>74.5</td></tr><tr><td>AlignBench v1.1</td><td>7.02</td><td>7.22</td><td>7.81</td><td><strong>7.99</strong></td><td>7.94</td><td>7.93</td></tr><tr><td>MTbench</td><td>8.55</td><td>9.10</td><td>-</td><td>8.86</td><td>8.88</td><td><strong>9.20</strong></td></tr></tbody></table><p>Qwen2.5-32B-Instruct 在大多数任务中表现优于同类规模的模型。与 GPT-4o-mini 相比，我们的开源模型 Qwen2.5-14B-Instruct 与 API 模型 Qwen-Turbo也在所有任务都中展现出了相当的竞争力。</p><h3 id=qwen25-7b-instruct-表现>Qwen2.5-7B-Instruct 表现<a hidden class=anchor aria-hidden=true href=#qwen25-7b-instruct-表现>#</a></h3><table><thead><tr><th>数据集</th><th>Gemma2-9b-IT</th><th>Llama3.1-8B-Instruct</th><th>Qwen2-7B-Instruct</th><th><strong>Qwen2.5-7B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.1</td><td>48.3</td><td>44.1</td><td><strong>56.3</strong></td></tr><tr><td>MMLU-redux</td><td>72.8</td><td>67.2</td><td>67.3</td><td><strong>75.4</strong></td></tr><tr><td>GPQA</td><td>32.8</td><td>32.8</td><td>34.3</td><td><strong>36.4</strong></td></tr><tr><td>MATH</td><td>44.3</td><td>51.9</td><td>52.9</td><td><strong>75.5</strong></td></tr><tr><td>GSM8K</td><td>76.7</td><td>84.5</td><td>85.7</td><td><strong>91.6</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td>79.9</td><td><strong>84.8</strong></td></tr><tr><td>MBPP</td><td>74.9</td><td>69.6</td><td>67.2</td><td><strong>79.2</strong></td></tr><tr><td>MultiPL-E</td><td>53.4</td><td>50.7</td><td>59.1</td><td><strong>70.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>18.9</td><td>8.3</td><td>23.9</td><td><strong>28.7</strong></td></tr><tr><td>LiveBench 0831</td><td>30.6</td><td>26.7</td><td>29.2</td><td><strong>35.9</strong></td></tr><tr><td>IFeval strict-prompt</td><td>70.1</td><td><strong>75.9</strong></td><td>54.7</td><td>71.2</td></tr><tr><td>Arena-Hard</td><td>41.6</td><td>27.8</td><td>25.0</td><td><strong>52.0</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.05</td><td>4.75</td><td>7.13</td><td><strong>7.33</strong></td></tr><tr><td>MTbench</td><td>8.49</td><td>8.23</td><td>8.26</td><td><strong>8.75</strong></td></tr></tbody></table><p>Qwen2.5-7B-Instruct 在除了 IFeval 的所有任务中表现均优于竞争对手 Gemma2-9b-IT 和 Llama3.1-8B-Instruct，尤其是在数学（MATH: 75.5）和代码（HumanEval: 84.8）任务上优势明显。</p><h3 id=qwen25-3b-instruct-表现>Qwen2.5-3B-Instruct 表现<a hidden class=anchor aria-hidden=true href=#qwen25-3b-instruct-表现>#</a></h3><table><thead><tr><th>数据集</th><th>Gemma2-2B-IT</th><th>Phi3.5-mini-Instruct</th><th>MiniCPM3-4B</th><th><strong>Qwen2.5-3B-Instruct</strong></th></tr></thead><tbody><tr><td>Non-Emb Params</td><td>2.0B</td><td>3.6B</td><td>4.0B</td><td>2.8B</td></tr><tr><td>MMLU-Pro</td><td>26.7</td><td><strong>47.5</strong></td><td>43.0</td><td>43.7</td></tr><tr><td>MMLU-redux</td><td>51.9</td><td><strong>67.7</strong></td><td>59.9</td><td>64.4</td></tr><tr><td>GPQA</td><td>29.3</td><td>27.2</td><td><strong>31.3</strong></td><td>30.3</td></tr><tr><td>MATH</td><td>26.6</td><td>48.5</td><td>46.6</td><td><strong>65.9</strong></td></tr><tr><td>GSM8K</td><td>63.2</td><td>86.2</td><td>81.1</td><td><strong>86.7</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td><strong>74.4</strong></td><td><strong>74.4</strong></td></tr><tr><td>MBPP</td><td><strong>74.9</strong></td><td>63.2</td><td>72.5</td><td>72.7</td></tr><tr><td>MultiPL-E</td><td>30.5</td><td>47.2</td><td>49.1</td><td><strong>60.2</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>5.8</td><td>15.8</td><td><strong>23.8</strong></td><td>19.9</td></tr><tr><td>LiveBench 0831</td><td>20.1</td><td>27.4</td><td><strong>27.6</strong></td><td>26.8</td></tr><tr><td>IFeval strict-prompt</td><td>51.0</td><td>52.1</td><td><strong>68.4</strong></td><td>58.2</td></tr></tbody></table><p>在适用移动端的指令模型中，Qwen2.5-3B-Instruct 的参数量虽然少于 Phi3.5-mini-Instruct 和 MiniCPM3-4B，但在数学和编程任务上仍然具有优势，同时在语言理解方面也展现出不错的实力。</p><h3 id=qwen25-05b15b-instruct-表现>Qwen2.5-0.5B/1.5B-Instruct 表现<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b-instruct-表现>#</a></h3><table><thead><tr><th>数据集</th><th>Qwen2-0.5B-Instruct</th><th><strong>Qwen2.5-0.5B-Instruct</strong></th><th>Qwen2-1.5B-Instruct</th><th><strong>Qwen2.5-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>14.4</td><td><strong>15.0</strong></td><td>22.9</td><td><strong>32.4</strong></td></tr><tr><td>MMLU-redux</td><td>12.9</td><td><strong>24.1</strong></td><td>41.2</td><td><strong>50.7</strong></td></tr><tr><td>GPQA</td><td>23.7</td><td><strong>29.8</strong></td><td>21.2</td><td><strong>29.8</strong></td></tr><tr><td>MATH</td><td>13.9</td><td><strong>34.4</strong></td><td>25.3</td><td><strong>55.2</strong></td></tr><tr><td>GSM8K</td><td>40.1</td><td><strong>49.6</strong></td><td>61.6</td><td><strong>73.2</strong></td></tr><tr><td>HumanEval</td><td>31.1</td><td><strong>35.4</strong></td><td>42.1</td><td><strong>61.6</strong></td></tr><tr><td>MBPP</td><td>39.7</td><td><strong>49.6</strong></td><td>44.2</td><td><strong>63.2</strong></td></tr><tr><td>MultiPL-E</td><td>20.8</td><td><strong>28.5</strong></td><td>38.5</td><td><strong>50.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>1.6</td><td><strong>5.1</strong></td><td>4.5</td><td><strong>14.8</strong></td></tr><tr><td>LiveBench 0831</td><td>7.4</td><td><strong>12.6</strong></td><td>12.4</td><td><strong>18.8</strong></td></tr><tr><td>IFeval strict-prompt</td><td>14.6</td><td><strong>27.9</strong></td><td>29.0</td><td><strong>42.5</strong></td></tr></tbody></table><p>Qwen2.5-1.5B-Instruct 及 Qwen2.5-0.5B-Instruct 的性能相比前代大幅提升，使它们尤其适合在资源极度受限的端侧场景下应用。</p><h3 id=多语言表现>多语言表现<a hidden class=anchor aria-hidden=true href=#多语言表现>#</a></h3><p>为了更好地评估指令微调模型的多语言表现，我们收集并扩展了以下基准测试：</p><ul><li><p><strong>IFEval（多语言）</strong>：我们将IFEval进行翻译，构建了多语言版本的 IFEval。在此过程中，我们移除了语言特定（例如“以字母A开头”）的测试用例。每种语言我们都收集了100个测试用例，包括阿拉伯语（ar）、西班牙语（es）、法语（fr）、印尼语（in）、日语（ja）、韩语（ko）、葡萄牙语（pt）和越南语（vi）。所有用例都由付费标注人员进行检查，并在必要时进行修改。</p></li><li><p><strong>知识能力测试</strong>：我们选用了五个类似 MMLU 的多选题基准测试来验证 Qwen2.5 系列模型的多语言知识掌握情况，包括：AMMLU（阿拉伯语）、JMMLU（日语）、KMMLU（韩语）、IndoMMLU（印尼语）和 TurkishMMLU（土耳其语）。此外，我们还展示了翻译版MMLU（即 okapi_MMLU，将英文MMLU翻译为多种语言）的性能表现。</p></li><li><p><strong>MGSM8K（扩展版）</strong>：在原版 MGSM8K 包含的语言外，我们还增加了阿拉伯语（ar）、韩语（ko）、葡萄牙语（pt）和越南语（vi）的支持。我们将 250 个测试用例翻译成这四种语言，保持与其他 MGSM8K 支持语言测试数量一致。所有示例也由付费标注人员进行了检查和必要的修改。</p></li><li><p><strong>文化差异</strong>：我们还使用了 BLEnD 基准测试，旨在评估大模型对于文化差异的处理能力，以进一步验证 Qwen2.5 系列模型的表现。</p></li></ul><table><thead><tr><th style=text-align:center>数据集</th><th style=text-align:center>Qwen2-72B-Instruct</th><th style=text-align:center>Llama3.1-70B-Instruct</th><th style=text-align:center>Qwen2.5-32B-Instruct</th><th style=text-align:center>Mistral-Large-Instruct-2407 (123B)</th><th style=text-align:center>GPT4o-mini</th><th style=text-align:center>Qwen2.5-72B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>指令遵循任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval（多语言）</td><td style=text-align:center>79.69</td><td style=text-align:center>80.47</td><td style=text-align:center>82.68</td><td style=text-align:center>82.69</td><td style=text-align:center>85.03</td><td style=text-align:center><strong>86.98</strong></td></tr><tr><td style=text-align:center><em><strong>知识任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU（阿拉伯语）</td><td style=text-align:center>68.85</td><td style=text-align:center>70.08</td><td style=text-align:center>70.44</td><td style=text-align:center>69.24</td><td style=text-align:center>69.73</td><td style=text-align:center><strong>72.44</strong></td></tr><tr><td style=text-align:center>JMMLU（日语）</td><td style=text-align:center>77.37</td><td style=text-align:center>73.89</td><td style=text-align:center>76.55</td><td style=text-align:center>75.77</td><td style=text-align:center>73.74</td><td style=text-align:center><strong>80.56</strong></td></tr><tr><td style=text-align:center>KMMLU（韩语）</td><td style=text-align:center>57.04</td><td style=text-align:center>53.23</td><td style=text-align:center>60.75</td><td style=text-align:center>56.42</td><td style=text-align:center>56.77</td><td style=text-align:center><strong>61.96</strong></td></tr><tr><td style=text-align:center>IndoMMLU（印尼语）</td><td style=text-align:center>66.31</td><td style=text-align:center>67.50</td><td style=text-align:center>66.42</td><td style=text-align:center>63.21</td><td style=text-align:center>67.75</td><td style=text-align:center><strong>69.25</strong></td></tr><tr><td style=text-align:center>TurkishMMLU（土耳其语）</td><td style=text-align:center>69.22</td><td style=text-align:center>66.89</td><td style=text-align:center>72.41</td><td style=text-align:center>64.78</td><td style=text-align:center>71.19</td><td style=text-align:center><strong>76.12</strong></td></tr><tr><td style=text-align:center>okapi MMLU（翻译）</td><td style=text-align:center>77.84</td><td style=text-align:center>76.49</td><td style=text-align:center>77.16</td><td style=text-align:center>78.37</td><td style=text-align:center>73.44</td><td style=text-align:center><strong>79.97</strong></td></tr><tr><td style=text-align:center><em><strong>数学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K（扩展版）</td><td style=text-align:center>82.72</td><td style=text-align:center>73.31</td><td style=text-align:center>87.15</td><td style=text-align:center><strong>89.01</strong></td><td style=text-align:center>87.36</td><td style=text-align:center>88.16</td></tr><tr><td style=text-align:center><em><strong>文化差异任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>25.90</td><td style=text-align:center>30.49</td><td style=text-align:center>27.88</td><td style=text-align:center>33.47</td><td style=text-align:center><strong>35.91</strong></td><td style=text-align:center>32.48</td></tr></tbody></table><table><thead><tr><th style=text-align:center>数据集</th><th style=text-align:center>Qwen2-7B-Instruct</th><th style=text-align:center>Llama3.1-8B-Instruct</th><th style=text-align:center>Qwen2.5-7B-Instruct</th><th style=text-align:center>Gemma-2-9B-Instruct</th><th style=text-align:center>Mistral-Nemo-Instruct-2407 (12B)</th><th style=text-align:center>Qwen2.5-14B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>指令遵循任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval（多语言）</td><td style=text-align:center>51.43</td><td style=text-align:center>60.68</td><td style=text-align:center>74.87</td><td style=text-align:center><strong>77.47</strong></td><td style=text-align:center>64.59</td><td style=text-align:center>77.08</td></tr><tr><td style=text-align:center><em><strong>知识任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU（阿拉伯语）</td><td style=text-align:center>54.87</td><td style=text-align:center>54.28</td><td style=text-align:center>59.78</td><td style=text-align:center>60.26</td><td style=text-align:center>53.92</td><td style=text-align:center><strong>66.81</strong></td></tr><tr><td style=text-align:center>JMMLU（日语）</td><td style=text-align:center>57.71</td><td style=text-align:center>53.26</td><td style=text-align:center>61.88</td><td style=text-align:center>64.59</td><td style=text-align:center>55.17</td><td style=text-align:center><strong>72.78</strong></td></tr><tr><td style=text-align:center>KMMLU（韩语）</td><td style=text-align:center>43.96</td><td style=text-align:center>42.28</td><td style=text-align:center>46.59</td><td style=text-align:center>46.24</td><td style=text-align:center>42.22</td><td style=text-align:center><strong>59.71</strong></td></tr><tr><td style=text-align:center>IndoMMLU（印尼语）</td><td style=text-align:center>54.05</td><td style=text-align:center>53.92</td><td style=text-align:center>56.42</td><td style=text-align:center>61.73</td><td style=text-align:center>50.76</td><td style=text-align:center><strong>65.09</strong></td></tr><tr><td style=text-align:center>TurkishMMLU（土耳其语）</td><td style=text-align:center>49.27</td><td style=text-align:center>45.61</td><td style=text-align:center>54.28</td><td style=text-align:center>55.44</td><td style=text-align:center>34.44</td><td style=text-align:center><strong>66.85</strong></td></tr><tr><td style=text-align:center>okapi MMLU（翻译）</td><td style=text-align:center>60.47</td><td style=text-align:center>55.18</td><td style=text-align:center>66.98</td><td style=text-align:center>46.72</td><td style=text-align:center>59.65</td><td style=text-align:center><strong>72.12</strong></td></tr><tr><td style=text-align:center><em><strong>数学任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K（扩展版）</td><td style=text-align:center>56.13</td><td style=text-align:center>66.05</td><td style=text-align:center>66.11</td><td style=text-align:center>78.37</td><td style=text-align:center>54.75</td><td style=text-align:center><strong>82.27</strong></td></tr><tr><td style=text-align:center><em><strong>文化差异任务</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>22.49</td><td style=text-align:center>19.47</td><td style=text-align:center>23.66</td><td style=text-align:center><strong>28.31</strong></td><td style=text-align:center>26.61</td><td style=text-align:center>26.99</td></tr></tbody></table><h1 id=实例演示>实例演示<a hidden class=anchor aria-hidden=true href=#实例演示>#</a></h1><p>我们准备了一些实例演示，来体现 Qwen2.5 的新特性和改进之处，涵盖了生成JSON格式输出、撰写长篇内容以及理解结构化数据等能力。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Generating JSON Output</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>JSON Output</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/json_output.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Structured Data Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Table Understanding</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/table_understanding.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Long Text Generation</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Text Generation</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/long_text.mp4 autoplay></video></figure></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>