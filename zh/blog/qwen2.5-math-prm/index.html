<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>面向有效的数学推理过程监督 | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DISCORD
引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。
过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。
今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。
ProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。
Process Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。
Best-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。
如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。
ProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。
结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。
引用 如果你觉得我们的工作有用，欢迎引用！
@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-math-prm/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="面向有效的数学推理过程监督"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DISCORD
引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。
过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。
今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。
ProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。
Process Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。
Best-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。
如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。
ProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。
结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。
引用 如果你觉得我们的工作有用，欢迎引用！
@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-14T00:00:03+08:00"><meta property="article:modified_time" content="2025-01-14T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="面向有效的数学推理过程监督"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DISCORD
引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。
过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。
今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。
ProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。
Process Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。
Best-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。
如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。
ProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。
结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。
引用 如果你觉得我们的工作有用，欢迎引用！
@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"面向有效的数学推理过程监督","item":"https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"面向有效的数学推理过程监督","name":"面向有效的数学推理过程监督","description":"GITHUB HUGGING FACE MODELSCOPE DISCORD\n引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。\n过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。\n今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。\nProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。\nProcess Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。\nBest-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。\n如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。\nProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。\n结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。\n引用 如果你觉得我们的工作有用，欢迎引用！\n@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412.","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DISCORD\n引言 近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。 此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。 因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。\n过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。\n今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。\nProcessBench 开源 ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。 它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。 模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。 ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。\nProcess Reward Model 开源 我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。 我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。\nBest-of-N 评测 按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。 每个候选回答的得分是该回答中每个步骤的分数乘积。 我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。\n如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。 此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。\nProcessBench 我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。 与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。 此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。 一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。\n结论 ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。 除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。 我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。 更多细节请查看我们下面的论文。\n引用 如果你觉得我们的工作有用，欢迎引用！\n@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412.06559}, year={2024} } @article{prmlessons, title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, author={ Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2501.07301}, year={2025} } ","wordCount":"143","inLanguage":"zh","datePublished":"2025-01-14T00:00:03+08:00","dateModified":"2025-01-14T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>面向有效的数学推理过程监督</h1><div class=post-meta><span title='2025-01-14 00:00:03 +0800 +0800'>2025年1月14日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;143 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5-math-prm/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/Qwen2.5-Math-PRM.png#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2-Math class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen25-math-66eaa240a1b7d5ee65f1da3e class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h1><p>近年来，大型语言模型（LLMs）在数学推理方面取得了显著进展，但它们仍可能犯错误，如计算错误或逻辑错误，导致得出错误结论。
此外，即使最终答案正确，这些强大的模型也经常编造看似合理的推理步骤，其中最终答案基于有缺陷的计算或推导过程，这削弱了LLMs推理过程的可靠性和可信度。
因此，自动识别推理过程中的错误对于其可扩展监督变得越来越重要。</p><p>过程奖励模型（Process Reward Models, PRMs）作为数学推理过程监督中的一种有前途的方法出现，旨在识别和减轻推理过程中的中间错误。在评估方面，以往的研究主要依赖于响应级别的Best-of-N（BoN）评估，即根据PRM从N个候选答案中选择得分最高的响应。</p><p>今天，我们开源了最先进的PRMs，它优于现有的开源PRM。我们还发布了步骤级别的评估标准 ProcessBench，用于测量模型识别数学推理中错误步骤的能力。</p><h1 id=processbench-开源>ProcessBench 开源<a hidden class=anchor aria-hidden=true href=#processbench-开源>#</a></h1><p>ProcessBench旨在衡量模型识别数学推理中错误步骤的能力。
它由3,400个测试案例组成，主要集中在竞赛和奥林匹克级别的数学问题上。每个测试案例包含一个逐步解决方案，并由人类专家标注错误位置。
模型需要识别出第一个错误的步骤，或者得出所有步骤都正确的结论。
ProcessBench可以用于评估两种类型的模型：PRMs和批评模型，后者通过提示通用语言模型来逐步检查回答中的步骤。</p><h1 id=process-reward-model-开源>Process Reward Model 开源<a hidden class=anchor aria-hidden=true href=#process-reward-model-开源>#</a></h1><p>我们发布了两个PRMs，即Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B，它们分别在Qwen2.5-Math-7B-Instruct和Qwen2.5-Math-72B-Instruct上进行微调得来。
我们训练的PRMs在BoN评估中表现出色，在ProcessBench上的错误步骤识别能力也更强。</p><h2 id=best-of-n-评测>Best-of-N 评测<a hidden class=anchor aria-hidden=true href=#best-of-n-评测>#</a></h2><p>按照Qwen2.5-Math的方法，我们从多个数学基准测试中用Qwen2.5-Math-7B-Instruct采样了八个回答（即N=8），包括GSM8K、MATH、Minerva Math、GaoKao 2023 En、OlympiadBench、College Math和MMLU STEM。
每个候选回答的得分是该回答中每个步骤的分数乘积。
我们将八次采样中的多数投票结果（maj@8）作为基线，将pass@8（即在八次采样中有任意一个采样得出正确最终答案的测试样本比例）作为上限。</p><p>如下表所示，Qwen2.5-Math-PRM-7B相比其他同等规模的PRMs表现出更优的性能。值得注意的是，它在所有7项任务中均优于maj@8，平均提高了1.4%。
此外，Qwen2.5-Math-PRM-72B的整体性能略优于Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B，特别是在Minerva Math和MMLU STEM任务中表现显著。</p><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/prm_bon.png#center width=100%></figure><h2 id=processbench>ProcessBench<a hidden class=anchor aria-hidden=true href=#processbench>#</a></h2><p>我们还在ProcessBench上评估了我们的PRMs，以测量其识别错误步骤的能力。
与LLM-as-judge相比，Qwen2.5-Math-PRM-7B以较小规模在所有开源LLM中表现出色；对于闭源模型，Qwen2.5-Math-PRM-7B超越了GPT-4o-0806，但在性能上仍与o1-mini存在差距。
此外，与现有的PRMs相比，Qwen2.5-Math-PRM-7B和Qwen2.5-Math-PRM-72B都显示出显著的优势。
一个有趣的观察是，ORM Qwen2.5-Math-RM-72B在识别步骤错误方面也表现出不错的能力，甚至超过了某些开源PRMs。</p><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/prm_processbench.png#center width=100%></figure><h1 id=结论>结论<a hidden class=anchor aria-hidden=true href=#结论>#</a></h1><p>ProcessBench展示了现有PRMs面临的挑战，并填补了PRMs步骤级别评估的空白。
除了开源PRMs以外，我们还在论文中通过广泛的实证研究识别了当前PRMs数据构建方法的局限性，并揭示了仅使用响应级别BoN评估PRMs的潜在偏差。
我们希望ProcessBench、我们开发PRM的最佳实践能够促进未来对推理过程监督的研究和开发。
更多细节请查看我们下面的论文。</p><h1 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h1><p>如果你觉得我们的工作有用，欢迎引用！</p><pre tabindex=0><code>@article{processbench,
  title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, 
  author={
    Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and
    Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin
  },
  journal={arXiv preprint arXiv:2412.06559},
  year={2024}
}
@article{prmlessons,
  title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, 
  author={
    Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin
  },
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>