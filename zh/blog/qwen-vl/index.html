<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen-VL全新升级！ | Qwen</title><meta name=keywords content><meta name=description content="我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen-vl/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen-vl/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen-vl/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen-VL全新升级！"><meta property="og:description" content="我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen-vl/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-01-25T13:33:00+08:00"><meta property="article:modified_time" content="2024-01-25T13:33:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen-VL全新升级！"><meta name=twitter:description content="我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：
显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。
Model DocVQA
Document understanding ChartQA
Chart understanding AI2D
Science diagrams TextVQA
Text reading MMMU
College-level problems MathVista
Mathematical reasoning MM-Bench-CN
Natural image QA in Chinese Other Best
Open-source LVLM 81.6%
(CogAgent) 68.4%
(CogAgent) 73.7%
(Fuyu-Medium) 76.1%
(CogAgent) 45.9%
(Yi-VL-34B) 36.7%
(SPHINX-V2) 72.4%
(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen-VL全新升级！","item":"https://qwenlm.github.io/zh/blog/qwen-vl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen-VL全新升级！","name":"Qwen-VL全新升级！","description":"我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：\n显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。\nModel DocVQA\nDocument understanding ChartQA\nChart understanding AI2D\nScience diagrams TextVQA\nText reading MMMU\nCollege-level problems MathVista\nMathematical reasoning MM-Bench-CN\nNatural image QA in Chinese Other Best\nOpen-source LVLM 81.6%\n(CogAgent) 68.4%\n(CogAgent) 73.7%\n(Fuyu-Medium) 76.1%\n(CogAgent) 45.9%\n(Yi-VL-34B) 36.7%\n(SPHINX-V2) 72.4%\n(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79.","keywords":[],"articleBody":"我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：\n显著提升与图像相关的推理能力； 在识别、提取和分析图像及其内含文本中的细节方面有明显增强； 支持百万像素以上的高清晰度图像以及各种宽高比的图像。 Model Name 模型描述 qwen-vl-plus Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。 qwen-vl-max Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。 相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。\nModel DocVQA\nDocument understanding ChartQA\nChart understanding AI2D\nScience diagrams TextVQA\nText reading MMMU\nCollege-level problems MathVista\nMathematical reasoning MM-Bench-CN\nNatural image QA in Chinese Other Best\nOpen-source LVLM 81.6%\n(CogAgent) 68.4%\n(CogAgent) 73.7%\n(Fuyu-Medium) 76.1%\n(CogAgent) 45.9%\n(Yi-VL-34B) 36.7%\n(SPHINX-V2) 72.4%\n(InternLM-XComposer-VL) Gemini Pro 88.1% 74.1% 73.9% 74.6% 47.9% 45.2% 74.3% Gemini Ultra 90.9% 80.8% 1 79.5% 1 82.3% 1 59.4% 1 53.0% 1 - GPT-4V 88.4% 78.5% 78.2% 78.0% 56.8% 49.9% 73.9% Qwen-VL-Plus 91.4% 78.1% 75.9% 78.9% 45.2% 43.3% 68.0% Qwen-VL-Max 93.1% 1 79.8% 2 79.3% 2 79.5% 2 51.4% 3 51.0% 2 75.1% 1 最新版本的Qwen-VL-Plus和Qwen-VL-Max模型不仅在基准测试性能上表现出色，而且在解决实际场景中的问题方面也展现了显著的进步。这些先进的模型能够轻松进行对话互动、识别名人和地标、生成文本，并且它们在描述和解读视觉内容的能力上有了显著提升。\n以下是一些实际应用示例：\n1. 基础识别能力 最新的Qwen-VL模型在准确描述和识别图像中的复杂信息以及提供详尽背景和回答相关问题方面表现出更高的能力。例如，Qwen-VL不仅能识别常见的物体，还能识别名人和地标。此外，Qwen-VL还能根据视觉输入创作各种语言的诗歌，并能对日常截图进行分析解读。\nExample: celebrity recognition\rNext\rUser\rwho are they? Qwen-VL\rThese are some of the most famous and highest-paid celebrities in the world. From left to right, top to bottom:\nJustin Bieber (Canadian singer) Lionel Messi (Argentine soccer player) Cristiano Ronaldo (Portuguese soccer player) Elon Musk (South African-born American entrepreneur and businessman) Kylie Jenner (American media personality and socialite) Taylor Swift (American singer-songwriter) Robert Downey Jr. (American actor) Scarlett Johansson (American actress) It’s worth noting that this is just a small sample of the many talented and successful celebrities out there, and that fame and wealth are not the only measures of success or importance.\nUser\rtheir chinese name?\rQwen-VL\rHere are the Chinese names of the celebrities mentioned:\nJustin Bieber: 贾斯汀·比伯 Lionel Messi: 莱昂内尔·梅西 Cristiano Ronaldo: 克里斯蒂亚诺·罗纳尔多 Elon Musk: 埃隆·马斯克 Kylie Jenner: 凯莉·詹娜 Taylor Swift: 泰勒·斯威夫特 Robert Downey Jr.: 小罗伯特·唐尼 Scarlett Johansson: 斯嘉丽·约翰逊 Note that these are just one possible translation of their names into Chinese, and there may be other variations used in different contexts or regions.\nExample: landmark recognition\rNext\rUser\rWhere is it in the picture? How did you recognize it? Qwen-VL\rThe picture shows a busy street at night in New York City, USA. The Empire State Building can be seen in the background, which is a distinctive landmark of New York City. Therefore, I recognized it as New York City.\rExample: poem writing\rNext\rUser\rLook at pictures and write a poem. Qwen-VL\rA rocky crag by the sea, A place of solitude and mystery. The water calm and blue, Reflecting the sky above, anew.\nThe mountain stands tall and proud, Its peak reaching towards the clouds. A symbol of strength and resilience, Enduring through time’s relentless persistence.\nThe world around may change and shift, But this mountain remains steadfast and uplifted. A reminder to us all, To stand firm in our own personal call.\nSo let us take a moment to pause, And appreciate this natural cause. For in its beauty and grandeur, We find inspiration and renewal, forever.\nExample: mobile app screenshot understanding\rNext\rUser\rDescribe the image. Qwen-VL\rThe image is a screenshot of an app displaying a map of Beijing, China. The map is centered on the Summer Palace and Yuyuantan Park. The app provides directions for driving from Yuyuantan Park to the Summer Palace. It offers two options: one takes 22 minutes and the other takes 25 minutes. The app also indicates that there are local plate restrictions on the route.\r2. 视觉Agent能力 除了在描述和识别方面的基本功能外，Qwen-VL还具有强大的定位和检索特定元素的能力。例如，它能够精准地在图像中标识出黑色的汽车。此外，Qwen-VL还具备根据场景的当前上下文进行判断、推理和决策的能力。\nExample: Locate Whatever You Desire\rNext\rUser\rLocate the red car Qwen-VL\rThe red car is located in the bottom right corner of the image. Example: Contextually Anchored Image Description Generation\rNext\rUser\rWhat does the pointed news talk about? Qwen-VL\rThe pointed news is about OpenAI’s chatbot ChatGPT having personalized instructions to store your preferences.\rExample: Autonomous Driving\rNext\rUser\rThe current vehicle speed is 150km/h. According to the picture, how should the vehicle operate? A. To accelerate B. Slow down C. Maintain the original speed D. Parking Choose an answer and give reasons. Qwen-VL\rB. Slow down\nAccording to the traffic sign in the picture, the maximum speed limit for the current lane is 100 km/h. If the current vehicle speed is 150 km/h, it exceeds the speed limit and should slow down to comply with the traffic regulations.\n3. 视觉推理能力 最新的Qwen-VL的一项重大突破在于其基于视觉输入进行复杂推理的能力。这项增强的视觉推理功能超越了简单的内容描述，能够理解和解读诸如流程图、图表和其他符号系统等复杂的表达形式。在问题解决和推理领域，Qwen-VL-Plus/Max不仅擅长数学问题求解和信息组织，还在对图表和图形进行深度诠释与分析方面表现出色。\nExample: Math\rNext\rUser\rread the image and solve it step by step Qwen-VL\rThe image shows two geometric figures, a rectangular prism and a cube, with their dimensions labeled. The problem asks to calculate the surface area and volume of these figures.\nFor the rectangular prism: Length (l) = 4 cm Width (w) = 2 cm Height (h) = 3 cm Surface Area (SA) of a rectangular prism = 2lw + 2lh + 2wh Volume (V) of a rectangular prism = lwh\nSubstituting the given values: SA = 2(4cm)(2cm) + 2(4cm)(3cm) + 2(2cm)(3cm) SA = 16cm² + 24cm² + 12cm² SA = 52cm²\nV = (4cm)(2cm)(3cm) V = 24cm³\nFor the cube: Side length (a) = 3 dm Surface Area (SA) of a cube = 6a² Volume (V) of a cube = a³\nSubstituting the given value: SA = 6(3dm)² SA = 6(9dm²) SA = 54dm²\nV = (3dm)³ V = 27dm³\nSo, the surface area and volume for each figure are as follows:\nFor the rectangular prism:\nSurface Area = 52 cm² Volume = 24 cm³ For the cube:\nSurface Area = 54 dm² Volume = 27 dm³ Example: Code\rNext\rUser\rconvert the process into a python code program Qwen-VL\rThe image shows a Scratch programming block, which can be converted into Python code as follows:\nimport random # Set up the variables a = random.randint(1, 50) b = random.randint(1, 50) answer = a + b # Ask the user for their input response = int(input(f\"What is {a} + {b}? \")) # Check if the response matches the answer if response == answer: print(\"Correct!\") else: print(\"Incorrect!\") This Python code generates two random numbers between 1 and 50, adds them together to get the answer, and then asks the user for their response. It then checks if the response matches the answer, and prints out a message accordingly.\nExample: Table\rNext\rUser\rSpot a data point that stands out in these charts and what that implicates. Then produce a detailed markdown table for all the data shown. Qwen-VL\rThe most striking data point in these charts is the high percentage of plastic waste that is mismanaged in the United States, which stands at 4%. This implies that a significant portion of plastic waste in the US is not being properly disposed of or recycled, leading to environmental pollution and potential harm to ecosystems and human health.\nHere’s a detailed markdown table for all the data shown:\nRegion Landfilled (2019) Mismanaged (2019) Incinerated (2019) Recycled (2019) World 49% 22% 19% 9% United States 73% 4% 19% 4% Europe 44% 6% 38% 12% Asia (excluding China and India) 39% 34% 19% 8% Note: The data source for this information is the OECD (Organisation for Economic Co-operation and Development), and the percentages represent the share of plastic waste that is recycled, landfilled, incinerated, and mismanaged in each region in 2019.\nExample: Reasoning\rNext\rUser\rLook at this sequence of three shapes. What shape should come as the fourth shape? Explain your reasoning with detailed descriptions of the first shapes. Qwen-VL\rThe sequence of shapes is a triangle, square, and pentagon, followed by a question mark. This pattern suggests that the next shape in the sequence should be a hexagon. The pattern appears to be based on the number of sides in each shape, with each subsequent shape having one more side than the previous one. Therefore, the fourth shape should have six sides, making it a hexagon.\rUser\r翻译成中文\rQwen-VL\r看这个三个形状的序列。下一个形状应该是什么形状？解释你的推理过程。 形状序列是三角形、正方形和五边形，然后是一个问号。这表明下一个形状应该是六边形。这个序列似乎基于每个形状的边数，每个后续形状比前一个形状多一条边。因此，第四个形状应该有六条边，使它成为一个六边形。\rExample: Model Flow\rNext\rUser\rexplain the picture Qwen-VL\rThe image presents a three-stage training pipeline for the Qwen-VL series, a visual language model. Each stage is represented by a box labeled “QwenLM” and is accompanied by a smaller box indicating the type of data used in that stage.\nIn Stage 1, the model is pre-trained using image-text pairs. This stage involves learning from a large dataset of images and their corresponding text descriptions to understand the relationship between visual and linguistic information.\nStage 2 is multi-task pre-training with multi-task and interleaved VL data. Here, the model is trained on multiple tasks simultaneously, such as image captioning, visual question answering, and referring expression comprehension. The use of interleaved data helps the model learn to switch between tasks efficiently.\nFinally, in Stage 3, the model undergoes supervised finetuning with chat-interleaved VL data. This stage involves refining the model’s performance on specific tasks using labeled data. The chat-interleaved data provides a more conversational context for the model to learn from, enhancing its ability to understand and respond to natural language queries.\nOverall, this three-stage training pipeline aims to develop a robust visual language model capable of understanding and generating human-like responses to visual and linguistic inputs.\n4. 文字识别及处理 在识别中文和英文文本方面。Qwen-VL-Plus/Max现在能够高效地从表格和文档中提取信息，并根据自定义输出需求重新格式化这些信息。此外，它还具备一种高效的密集文本识别与转换机制，在处理包含大量信息的文档时表现非常出色。该技术支持各种极端纵横比的图像，确保了对多样视觉内容进行处理的灵活性。\nExample: Information Retrieval\rNext\rUser\rPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if the information is not available in the image, put “N/A” instead). {“class”: xxx, “DLN”: xxx, “DOB”: xxx, “Name”: xxx, “Address”: xxx, “EXP”: xxx, “ISS”: xxX, “SEX”: xxx, “HGT”: xxx, “WGT”: xxx, “EYES”: xxx, “HAIR”: xxX, “DONOR”: xxx} Qwen-VL\r{“class”: “C”, “DLN”: “I1234568”, “DOB”: “08/31/1977”, “Name”: “IMA CARDHOLDER”, “Address”: “2570 24TH STREET ANYTOWN, CA 95818”, “EXP”: “08/31/2014”, “ISS”: “08/31/2009”, “SEX”: “F”, “HGT”: “5’-05\"”, “WGT”: “125 lb”, “EYES”: “BRN”, “HAIR”: “BRN”, “DONOR”: “YES”}\rExample: Text Information Recognition \u0026 Processing\rNext\rUser\r规范格式输出图中所有文字 Qwen-VL\r图片中的文字如下：\n学习提示\n郭沫若的诗集《女神》是中国新诗的代表作品，它以崭新的内容和形式，表达了“五四”时期狂飙突进的时代精神。《立在地球边上放号》是《女神》中富有代表性的一首诗。诗人设想站在地球边上全方位俯瞰地球，放声呼唤，纵情高歌，想象着那怒涌的白云、壮丽的北冰洋和狂暴的太平洋，意在赞美摧毁旧世界、创造新生活的雄强之力，体现了“五四”所焕发的自由宏阔、雄奇奔放的气概。阅读时要注意联系“五四”特定的时代氛围来理解这首诗的内涵与形式特征。\n闻一多的《红烛》化用“蜡炬”这一古典意象，赋予它新的含义，赞美的红烛以“烧蜡成灰”来点亮世界的奉献精神。注意体会诗人如何借助与红烛的“对话”表达青春的困惑与希望，以及对理想的坚毅追求。洋溢在诗中的幻想和情绪渲染，叹词的回环使用，诗句长短错落形成的节奏美，也是欣赏时关注的。\n昌耀的《峨日朵雪峰之侧》内蕴丰富，阅读时要注意发挥想象，体味“雪峰”、“太阳”、“蜘蛛”等意象营造出的凝重而又壮美的氛围，感受其中蕴含的谦卑而强劲的生命力量。\n雪莱的《致云雀》意蕴深刻，比喻新颖，要注意体会诗人的情感，理解云雀的象征意义；把握作品节与节之间环环相扣的特点，欣赏诗人想象的奇特和运用修辞手法的大胆、灵活。\n学习这组诗作，要多借助朗读，理清诗歌的抒情脉络，把握意象的内涵，感悟诗人抒发的情思，体会象征手法的运用。\n第一单元\n11\n请注意，图片中的文字可能需要进一步的排版和格式化，以便于阅读和理解。如果您需要帮助，请随时告诉我。\nExample: Text Information Recognition \u0026 Processing\rNext\rUser\rA brief summary of the blog Qwen-VL\rThe blog post announces several new updates and improvements to OpenAI’s platform, including:\nGPT-4 Turbo with a 128K context window, which is more capable and supports a 128K context window. New Assistants API that makes it easier for developers to build their own assistive AI apps and tools. New multimodal capabilities in the platform, including text-to-speech (TTS). Improved instruction following and JSON mode. Reproducible outputs and log probabilities. Updated GPT-3.5 Turbo with a 16K context window by default. The post also mentions that these updates will be rolled out to all paying customers starting at 10am PT today.\n如何使用 当前，用户可以通过Huggingface Spaces、通义千问官方网站以及Dashscope APIs来使用Qwen-VL-Plus和Qwen-VL-Max模型。\nQwen-VL-Plus Qwen-VL-Max 登录千问官网 https://tongyi.aliyun.com/qianwen，并使用图像理解模式来使用最新的Qwen-VL模型。 通过DashScope使用Qwen-VL-Plus和Qwen-VL-Max的API(DashScope). 总结 Qwen-VL-Plus和Qwen-VL-Max在提升高分辨率识别、文本分析和图像推理能力方面取得了重大突破。这些模型现在已经达到了GPT4-V和Gemini的性能水平，在诸如MMMU、CMMMU和MathVista等众多任务上超越了所有其他开源及专有模型。它们在文档分析（DocVQA）和中文相关的图像理解（MM-Bench-CN）等领域取得了领先的表现。\n我们的目标是持续挖掘并提升Qwen-VL的潜力，使其能够在更广泛的应用场景中发挥作用。我们可以设想一个具备超人视觉和语言理解能力的Qwen-VL助手，无论是在日常对话中，还是在复杂的驾驶环境和编程场景下，都能提供强有力的支持。\n尽管我们还有很长的路要走，但我们坚信通过持续优化和扩展，Qwen-VL将逐步进化到能够像人类认知一样感知和理解世界！\n官网：https://tongyi.aliyun.com/qianwen Github: https://github.com/QwenLM/Qwen-VL Huggingface: http://huggingface.co/Qwen/Qwen-VL-Chat ModelScope: https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo API: https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api Discord: https://discord.gg/CV4E9rpNSD ","wordCount":"1715","inLanguage":"zh","datePublished":"2024-01-25T13:33:00+08:00","dateModified":"2024-01-25T13:33:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen-vl/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen-VL全新升级！</h1><div class=post-meta><span title='2024-01-25 13:33:00 +0800 +0800'>2024年1月25日</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;1715 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen-vl/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p>我们在Qwen语言模型的基础上，结合此前我们提出的多模态多任务训练，以解决多模态模型在泛化能力上的局限性，并于2023年9月开源了多模态模型Qwen-VL。最近，Qwen-VL系列有了重大升级，推出了两个增强版本：Qwen-VL-Plus和Qwen-VL-Max。这两个版本的关键提升包括：</p><ul><li>显著提升与图像相关的推理能力；</li><li>在识别、提取和分析图像及其内含文本中的细节方面有明显增强；</li><li>支持百万像素以上的高清晰度图像以及各种宽高比的图像。</li></ul><table><tr><td style=width:15%><b>Model Name</b></td><td><b>模型描述</b></td></tr><tr><td>qwen-vl-plus</td><td><sup>Qwen的增强型大规模视觉语言模型。该模型针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高清分辨率及任意图像输入的宽高比。它在各类视觉任务上都展现出卓越的性能表现。</sup></td></tr><tr><td>qwen-vl-max</td><td><sup>Qwen的最强大视觉语言模型。相较于增强版本，该模型在视觉推理和指令执行能力上做出了进一步提升，提供了更高级别的视觉感知与认知理解力,在更广泛复杂的任务上都能实现最优性能。</sup></td></tr></table><p>相比于开源版本的Qwen-VL，这两个模型在多个文本-图像多模态任务中与Gemini Ultra和GPT-4V的表现相当，显著超越了之前开源模型的最佳结果。值得一提的是，Qwen-VL-Max在中文问题回答和中文文本理解任务上超越了OpenAI的GPT-4V以及谷歌的Gemini。下文展示了实验结果及真实用例。</p><table><thead><tr><th style=width:20%>Model</th><th>DocVQA<br><sup><sup>Document understanding</sup><sup></th><th>ChartQA<br><sup><sup>Chart understanding</sup></sup></th><th>AI2D<br><sup><sup>Science diagrams</sup></sup></th><th>TextVQA<br><sup><sup>Text reading</sup></sup></th><th>MMMU<br><sup><sup>College-level problems</sup></sup></th><th>MathVista<br><sup><sup>Mathematical reasoning</sup></sup></th><th>MM-Bench-CN<br><sup><sup>Natural image QA in Chinese</sup></sup></th></tr></thead><tbody align=center><tr><td>Other Best<br><sup>Open-source LVLM</sup></td><td>81.6%<br><sup><sup>(CogAgent)</sup></sup></td><td>68.4%<br><sup><sup>(CogAgent)</sup></sup></td><td>73.7%<br><sup><sup>(Fuyu-Medium)</sup></sup></td><td>76.1%<br><sup><sup>(CogAgent)</sup></sup></td><td>45.9%<br><sup><sup>(Yi-VL-34B)</sup></sup></td><td>36.7%<br><sup><sup>(SPHINX-V2)</sup></sup></td><td>72.4%<br><sup><sup>(InternLM-XComposer-VL)</sup></sup></td></tr><tr><td>Gemini Pro</td><td>88.1%</td><td>74.1%</td><td>73.9%</td><td>74.6%</td><td>47.9%</td><td>45.2%</td><td>74.3%</td></tr><tr><td>Gemini Ultra</td><td>90.9%</td><td>80.8% <sup>1</sup></td><td>79.5% <sup>1</sup></td><td>82.3% <sup>1</sup></td><td>59.4% <sup>1</sup></td><td>53.0% <sup>1</sup></td><td>-</td></tr><tr><td>GPT-4V</td><td>88.4%</td><td>78.5%</td><td>78.2%</td><td>78.0%</td><td>56.8%</td><td>49.9%</td><td>73.9%</td></tr><tr><td><b>Qwen-VL-Plus</b></td><td>91.4%</td><td>78.1%</td><td>75.9%</td><td>78.9%</td><td>45.2%</td><td>43.3%</td><td>68.0%</td></tr><tr><td><b>Qwen-VL-Max</b></td><td>93.1% <sup>1</sup></td><td>79.8% <sup>2</sup></td><td>79.3% <sup>2</sup></td><td>79.5% <sup>2</sup></td><td>51.4% <sup>3</sup></td><td>51.0% <sup>2</sup></td><td>75.1% <sup>1</sup></td></tr></tbody></table><p>最新版本的Qwen-VL-Plus和Qwen-VL-Max模型不仅在基准测试性能上表现出色，而且在解决实际场景中的问题方面也展现了显著的进步。这些先进的模型能够轻松进行对话互动、识别名人和地标、生成文本，并且它们在描述和解读视觉内容的能力上有了显著提升。</p><p>以下是一些实际应用示例：</p><h3 id=1-基础识别能力>1. 基础识别能力<a hidden class=anchor aria-hidden=true href=#1-基础识别能力>#</a></h3><p>最新的Qwen-VL模型在准确描述和识别图像中的复杂信息以及提供详尽背景和回答相关问题方面表现出更高的能力。例如，Qwen-VL不仅能识别常见的物体，还能识别名人和地标。此外，Qwen-VL还能根据视觉输入创作各种语言的诗歌，并能对日常截图进行分析解读。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: celebrity recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>who are they?
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/1_celebrity.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>These are some of the most famous and highest-paid celebrities in the world. From left to right, top to bottom:</p><ul><li>Justin Bieber (Canadian singer)</li><li>Lionel Messi (Argentine soccer player)</li><li>Cristiano Ronaldo (Portuguese soccer player)</li><li>Elon Musk (South African-born American entrepreneur and businessman)</li><li>Kylie Jenner (American media personality and socialite)</li><li>Taylor Swift (American singer-songwriter)</li><li>Robert Downey Jr. (American actor)</li><li>Scarlett Johansson (American actress)</li></ul><p>It&rsquo;s worth noting that this is just a small sample of the many talented and successful celebrities out there, and that fame and wealth are not the only measures of success or importance.</p></div><div class=role>User</div><div class=content>their chinese name?</div><div class=role>Qwen-VL</div><div class=content><p>Here are the Chinese names of the celebrities mentioned:</p><ul><li>Justin Bieber: 贾斯汀·比伯</li><li>Lionel Messi: 莱昂内尔·梅西</li><li>Cristiano Ronaldo: 克里斯蒂亚诺·罗纳尔多</li><li>Elon Musk: 埃隆·马斯克</li><li>Kylie Jenner: 凯莉·詹娜</li><li>Taylor Swift: 泰勒·斯威夫特</li><li>Robert Downey Jr.: 小罗伯特·唐尼</li><li>Scarlett Johansson: 斯嘉丽·约翰逊</li></ul><p>Note that these are just one possible translation of their names into Chinese, and there may be other variations used in different contexts or regions.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: landmark recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Where is it in the picture? How did you recognize it?
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/Image_Case_1_2.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content>The picture shows a busy street at night in New York City, USA. The Empire State Building can be seen in the background, which is a distinctive landmark of New York City. Therefore, I recognized it as New York City.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: poem writing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Look at pictures and write a poem.
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/Image_Case_1_3.png alt=image></div><div class=role>Qwen-VL</div><div class=content><p>A rocky crag by the sea,
A place of solitude and mystery.
The water calm and blue,
Reflecting the sky above, anew.</p><p>The mountain stands tall and proud,
Its peak reaching towards the clouds.
A symbol of strength and resilience,
Enduring through time&rsquo;s relentless persistence.</p><p>The world around may change and shift,
But this mountain remains steadfast and uplifted.
A reminder to us all,
To stand firm in our own personal call.</p><p>So let us take a moment to pause,
And appreciate this natural cause.
For in its beauty and grandeur,
We find inspiration and renewal, forever.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: mobile app screenshot understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Describe the image.<figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/Image_Case_1_4.png width=40%></figure></div><div class=role>Qwen-VL</div><div class=content>The image is a screenshot of an app displaying a map of Beijing, China. The map is centered on the Summer Palace and Yuyuantan Park. The app provides directions for driving from Yuyuantan Park to the Summer Palace. It offers two options: one takes 22 minutes and the other takes 25 minutes. The app also indicates that there are local plate restrictions on the route.</div></div></div></div><h3 id=2-视觉agent能力>2. 视觉Agent能力<a hidden class=anchor aria-hidden=true href=#2-视觉agent能力>#</a></h3><p>除了在描述和识别方面的基本功能外，Qwen-VL还具有强大的定位和检索特定元素的能力。例如，它能够精准地在图像中标识出黑色的汽车。此外，Qwen-VL还具备根据场景的当前上下文进行判断、推理和决策的能力。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Locate Whatever You Desire</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Locate the red car
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/2_car_input.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content>The red car is located in the bottom right corner of the image.
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/2_car_output.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Contextually Anchored Image Description Generation</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What does the pointed news talk about?
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/2_news.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content>The pointed news is about OpenAI&rsquo;s chatbot ChatGPT having personalized instructions to store your preferences.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Autonomous Driving</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>The current vehicle speed is 150km/h. According to the picture, how should the vehicle operate?
A. To accelerate
B. Slow down
C. Maintain the original speed
D. Parking
Choose an answer and give reasons.
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/2_drive.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>B. Slow down</p><p>According to the traffic sign in the picture, the maximum speed limit for the current lane is 100 km/h. If the current vehicle speed is 150 km/h, it exceeds the speed limit and should slow down to comply with the traffic regulations.</p></div></div></div></div><h3 id=3-视觉推理能力>3. 视觉推理能力<a hidden class=anchor aria-hidden=true href=#3-视觉推理能力>#</a></h3><p>最新的Qwen-VL的一项重大突破在于其基于视觉输入进行复杂推理的能力。这项增强的视觉推理功能超越了简单的内容描述，能够理解和解读诸如流程图、图表和其他符号系统等复杂的表达形式。在问题解决和推理领域，Qwen-VL-Plus/Max不仅擅长数学问题求解和信息组织，还在对图表和图形进行深度诠释与分析方面表现出色。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Math</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>read the image and solve it step by step
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_1_math_cube.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>The image shows two geometric figures, a rectangular prism and a cube, with their dimensions labeled. The problem asks to calculate the surface area and volume of these figures.</p><ol><li>For the rectangular prism:</li></ol><ul><li>Length (l) = 4 cm</li><li>Width (w) = 2 cm</li><li>Height (h) = 3 cm</li></ul><p>Surface Area (SA) of a rectangular prism = 2lw + 2lh + 2wh
Volume (V) of a rectangular prism = lwh</p><p>Substituting the given values:
SA = 2(4cm)(2cm) + 2(4cm)(3cm) + 2(2cm)(3cm)
SA = 16cm² + 24cm² + 12cm²
SA = 52cm²</p><p>V = (4cm)(2cm)(3cm)
V = 24cm³</p><ol start=2><li>For the cube:</li></ol><ul><li>Side length (a) = 3 dm</li></ul><p>Surface Area (SA) of a cube = 6a²
Volume (V) of a cube = a³</p><p>Substituting the given value:
SA = 6(3dm)²
SA = 6(9dm²)
SA = 54dm²</p><p>V = (3dm)³
V = 27dm³</p><p>So, the surface area and volume for each figure are as follows:</p><p>For the rectangular prism:</p><ul><li>Surface Area = 52 cm²</li><li>Volume = 24 cm³</li></ul><p>For the cube:</p><ul><li>Surface Area = 54 dm²</li><li>Volume = 27 dm³</li></ul></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>convert the process into a python code program
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_2_python_code.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>The image shows a Scratch programming block, which can be converted into Python code as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set up the variables</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>answer</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ask the user for their input</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>input</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;What is </span><span class=si>{</span><span class=n>a</span><span class=si>}</span><span class=s2> + </span><span class=si>{</span><span class=n>b</span><span class=si>}</span><span class=s2>? &#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Check if the response matches the answer</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>response</span> <span class=o>==</span> <span class=n>answer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Correct!&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Incorrect!&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>This Python code generates two random numbers between 1 and 50, adds them together to get the answer, and then asks the user for their response. It then checks if the response matches the answer, and prints out a message accordingly.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Table</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Spot a data point that stands out in these charts and what that implicates. Then produce a detailed markdown table for all the data shown.
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_3_data_chart.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>The most striking data point in these charts is the high percentage of plastic waste that is mismanaged in the United States, which stands at 4%. This implies that a significant portion of plastic waste in the US is not being properly disposed of or recycled, leading to environmental pollution and potential harm to ecosystems and human health.</p><p>Here&rsquo;s a detailed markdown table for all the data shown:</p><table><thead><tr><th>Region</th><th>Landfilled (2019)</th><th>Mismanaged (2019)</th><th>Incinerated (2019)</th><th>Recycled (2019)</th></tr></thead><tbody><tr><td>World</td><td>49%</td><td>22%</td><td>19%</td><td>9%</td></tr><tr><td>United States</td><td>73%</td><td>4%</td><td>19%</td><td>4%</td></tr><tr><td>Europe</td><td>44%</td><td>6%</td><td>38%</td><td>12%</td></tr><tr><td>Asia (excluding China and India)</td><td>39%</td><td>34%</td><td>19%</td><td>8%</td></tr></tbody></table><p>Note: The data source for this information is the OECD (Organisation for Economic Co-operation and Development), and the percentages represent the share of plastic waste that is recycled, landfilled, incinerated, and mismanaged in each region in 2019.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Reasoning</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Look at this sequence of three shapes. What shape should come as the fourth shape? Explain your reasoning with detailed descriptions of the first shapes.
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_4_polygon_reasoning.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content>The sequence of shapes is a triangle, square, and pentagon, followed by a question mark. This pattern suggests that the next shape in the sequence should be a hexagon. The pattern appears to be based on the number of sides in each shape, with each subsequent shape having one more side than the previous one. Therefore, the fourth shape should have six sides, making it a hexagon.</div><div class=role>User</div><div class=content>翻译成中文</div><div class=role>Qwen-VL</div><div class=content>看这个三个形状的序列。下一个形状应该是什么形状？解释你的推理过程。
形状序列是三角形、正方形和五边形，然后是一个问号。这表明下一个形状应该是六边形。这个序列似乎基于每个形状的边数，每个后续形状比前一个形状多一条边。因此，第四个形状应该有六条边，使它成为一个六边形。</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Model Flow</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>explain the picture
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_5_model_flow.jpeg alt=image></div><div class=role>Qwen-VL</div><div class=content><p>The image presents a three-stage training pipeline for the Qwen-VL series, a visual language model. Each stage is represented by a box labeled &ldquo;QwenLM&rdquo; and is accompanied by a smaller box indicating the type of data used in that stage.</p><p>In Stage 1, the model is pre-trained using image-text pairs. This stage involves learning from a large dataset of images and their corresponding text descriptions to understand the relationship between visual and linguistic information.</p><p>Stage 2 is multi-task pre-training with multi-task and interleaved VL data. Here, the model is trained on multiple tasks simultaneously, such as image captioning, visual question answering, and referring expression comprehension. The use of interleaved data helps the model learn to switch between tasks efficiently.</p><p>Finally, in Stage 3, the model undergoes supervised finetuning with chat-interleaved VL data. This stage involves refining the model&rsquo;s performance on specific tasks using labeled data. The chat-interleaved data provides a more conversational context for the model to learn from, enhancing its ability to understand and respond to natural language queries.</p><p>Overall, this three-stage training pipeline aims to develop a robust visual language model capable of understanding and generating human-like responses to visual and linguistic inputs.</p></div></div></div></div><h3 id=4-文字识别及处理>4. 文字识别及处理<a hidden class=anchor aria-hidden=true href=#4-文字识别及处理>#</a></h3><p>在识别中文和英文文本方面。Qwen-VL-Plus/Max现在能够高效地从表格和文档中提取信息，并根据自定义输出需求重新格式化这些信息。此外，它还具备一种高效的密集文本识别与转换机制，在处理包含大量信息的文档时表现非常出色。该技术支持各种极端纵横比的图像，确保了对多样视觉内容进行处理的灵活性。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Information Retrieval</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if the information is not available in the image, put &ldquo;N/A&rdquo; instead).
{&ldquo;class&rdquo;: xxx, &ldquo;DLN&rdquo;: xxx, &ldquo;DOB&rdquo;: xxx, &ldquo;Name&rdquo;: xxx, &ldquo;Address&rdquo;: xxx, &ldquo;EXP&rdquo;: xxx, &ldquo;ISS&rdquo;: xxX, &ldquo;SEX&rdquo;: xxx, &ldquo;HGT&rdquo;: xxx, &ldquo;WGT&rdquo;: xxx, &ldquo;EYES&rdquo;: xxx, &ldquo;HAIR&rdquo;: xxX, &ldquo;DONOR&rdquo;: xxx}
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/3_6_drive_license.jpg alt=image></div><div class=role>Qwen-VL</div><div class=content>{&ldquo;class&rdquo;: &ldquo;C&rdquo;, &ldquo;DLN&rdquo;: &ldquo;I1234568&rdquo;, &ldquo;DOB&rdquo;: &ldquo;08/31/1977&rdquo;, &ldquo;Name&rdquo;: &ldquo;IMA CARDHOLDER&rdquo;, &ldquo;Address&rdquo;: &ldquo;2570 24TH STREET ANYTOWN, CA 95818&rdquo;, &ldquo;EXP&rdquo;: &ldquo;08/31/2014&rdquo;, &ldquo;ISS&rdquo;: &ldquo;08/31/2009&rdquo;, &ldquo;SEX&rdquo;: &ldquo;F&rdquo;, &ldquo;HGT&rdquo;: &ldquo;5&rsquo;-05"&rdquo;, &ldquo;WGT&rdquo;: &ldquo;125 lb&rdquo;, &ldquo;EYES&rdquo;: &ldquo;BRN&rdquo;, &ldquo;HAIR&rdquo;: &ldquo;BRN&rdquo;, &ldquo;DONOR&rdquo;: &ldquo;YES&rdquo;}</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Text Information Recognition & Processing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>规范格式输出图中所有文字<figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/4_text_information_1.jpeg width=60%></figure></div><div class=role>Qwen-VL</div><div class=content><p>图片中的文字如下：</p><blockquote><p>学习提示</p><p>郭沫若的诗集《女神》是中国新诗的代表作品，它以崭新的内容和形式，表达了“五四”时期狂飙突进的时代精神。《立在地球边上放号》是《女神》中富有代表性的一首诗。诗人设想站在地球边上全方位俯瞰地球，放声呼唤，纵情高歌，想象着那怒涌的白云、壮丽的北冰洋和狂暴的太平洋，意在赞美摧毁旧世界、创造新生活的雄强之力，体现了“五四”所焕发的自由宏阔、雄奇奔放的气概。阅读时要注意联系“五四”特定的时代氛围来理解这首诗的内涵与形式特征。</p><p>闻一多的《红烛》化用“蜡炬”这一古典意象，赋予它新的含义，赞美的红烛以“烧蜡成灰”来点亮世界的奉献精神。注意体会诗人如何借助与红烛的“对话”表达青春的困惑与希望，以及对理想的坚毅追求。洋溢在诗中的幻想和情绪渲染，叹词的回环使用，诗句长短错落形成的节奏美，也是欣赏时关注的。</p><p>昌耀的《峨日朵雪峰之侧》内蕴丰富，阅读时要注意发挥想象，体味“雪峰”、“太阳”、“蜘蛛”等意象营造出的凝重而又壮美的氛围，感受其中蕴含的谦卑而强劲的生命力量。</p><p>雪莱的《致云雀》意蕴深刻，比喻新颖，要注意体会诗人的情感，理解云雀的象征意义；把握作品节与节之间环环相扣的特点，欣赏诗人想象的奇特和运用修辞手法的大胆、灵活。</p><p>学习这组诗作，要多借助朗读，理清诗歌的抒情脉络，把握意象的内涵，感悟诗人抒发的情思，体会象征手法的运用。</p><p>第一单元<br>11</p></blockquote><p>请注意，图片中的文字可能需要进一步的排版和格式化，以便于阅读和理解。如果您需要帮助，请随时告诉我。</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Text Information Recognition & Processing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>A brief summary of the blog<figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/4_text_information_2.jpeg width=8%></figure></div><div class=role>Qwen-VL</div><div class=content><p>The blog post announces several new updates and improvements to OpenAI&rsquo;s platform, including:</p><ul><li>GPT-4 Turbo with a 128K context window, which is more capable and supports a 128K context window.</li><li>New Assistants API that makes it easier for developers to build their own assistive AI apps and tools.</li><li>New multimodal capabilities in the platform, including text-to-speech (TTS).</li><li>Improved instruction following and JSON mode.</li><li>Reproducible outputs and log probabilities.</li><li>Updated GPT-3.5 Turbo with a 16K context window by default.</li></ul><p>The post also mentions that these updates will be rolled out to all paying customers starting at 10am PT today.</p></div></div></div></div><h2 id=如何使用>如何使用<a hidden class=anchor aria-hidden=true href=#如何使用>#</a></h2><p>当前，用户可以通过Huggingface Spaces、通义千问官方网站以及Dashscope APIs来使用Qwen-VL-Plus和Qwen-VL-Max模型。</p><ul><li><a href=https://huggingface.co/spaces/Qwen/Qwen-VL-Plus>Qwen-VL-Plus</a></li><li><a href=https://huggingface.co/spaces/Qwen/Qwen-VL-Max>Qwen-VL-Max</a></li></ul><iframe src=https://qwen-qwen-vl-max.hf.space frameborder=0 width=850 height=1100></iframe><ul><li><p>登录千问官网 <a href=https://tongyi.aliyun.com/qianwen>https://tongyi.aliyun.com/qianwen</a>，并使用图像理解模式来使用最新的Qwen-VL模型。<figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/qwen_web.png#center width=60%></figure></p></li><li><p>通过DashScope使用Qwen-VL-Plus和Qwen-VL-Max的API(<a href=https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start>DashScope</a>).<figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-VL/blog/dash_api.jpeg#center width=80%></figure></p></li></ul><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>Qwen-VL-Plus和Qwen-VL-Max在提升高分辨率识别、文本分析和图像推理能力方面取得了重大突破。这些模型现在已经达到了GPT4-V和Gemini的性能水平，在诸如MMMU、CMMMU和MathVista等众多任务上超越了所有其他开源及专有模型。它们在文档分析（DocVQA）和中文相关的图像理解（MM-Bench-CN）等领域取得了领先的表现。</p><p>我们的目标是持续挖掘并提升Qwen-VL的潜力，使其能够在更广泛的应用场景中发挥作用。我们可以设想一个具备超人视觉和语言理解能力的Qwen-VL助手，无论是在日常对话中，还是在复杂的驾驶环境和编程场景下，都能提供强有力的支持。</p><p>尽管我们还有很长的路要走，但我们坚信通过持续优化和扩展，Qwen-VL将逐步进化到能够像人类认知一样感知和理解世界！</p><ul><li>官网：<a href=https://tongyi.aliyun.com/qianwen>https://tongyi.aliyun.com/qianwen</a></li><li>Github: <a href=https://github.com/QwenLM/Qwen-VL>https://github.com/QwenLM/Qwen-VL</a></li><li>Huggingface: <a href=http://huggingface.co/Qwen/Qwen-VL-Chat>http://huggingface.co/Qwen/Qwen-VL-Chat</a></li><li>ModelScope: <a href=https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo>https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo</a></li><li>API: <a href=https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api>https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api</a></li><li>Discord: <a href=https://discord.gg/CV4E9rpNSD>https://discord.gg/CV4E9rpNSD</a></li></ul></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>