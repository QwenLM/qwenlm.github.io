<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>你好，Qwen2 | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：
5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！
模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:
模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。
上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。
我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：
地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="你好，Qwen2"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：
5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！
模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:
模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。
上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。
我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：
地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。"><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-06-07T00:00:00+08:00"><meta property="article:modified_time" content="2024-07-16T00:00:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="你好，Qwen2"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：
5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！
模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:
模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。
上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。
我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：
地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"你好，Qwen2","item":"https://qwenlm.github.io/zh/blog/qwen2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"你好，Qwen2","name":"你好，Qwen2","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：\n5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！\n模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:\n模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。\n上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。\n我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：\n地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：\n5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！\n模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:\n模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。\n上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。\n我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：\n地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。\n模型评测 相比Qwen1.5，Qwen2在大规模模型实现了非常大幅度的效果提升。我们对Qwen2-72B进行了全方位的评测。在针对预训练语言模型的评估中，对比当前最优的开源模型，Qwen2-72B在包括自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型，如Llama-3-70B以及Qwen1.5最大的模型Qwen1.5-110B。这得益于其预训练数据及训练方法的优化。\n大规模预训练后，我们对模型进行精细的微调，以提升其智能水平，让其表现更接近人类。这个过程进一步提升了代码、数学、推理、指令遵循、多语言理解等能力。此外，模型学会对齐人类价值观，它也随之变得更加对人类有帮助、诚实以及安全。我们的微调过程遵循的原则是使训练尽可能规模化的同时并且尽可能减少人工标注。我们探索了如何采用多种自动方法以获取高质量、可靠、有创造力的指令和偏好数据，其中包括针对数学的拒绝采样、针对代码和指令遵循的代码执行反馈、针对创意写作的回译、针对角色扮演的scalable oversight、等等。在训练方面，我们结合了有监督微调、反馈模型训练以及在线DPO等方法。我们还采用了在线模型合并的方法减少对齐税。这些做法都大幅提升了模型的基础能力以及模型的智能水平。\n我们全面评估了Qwen2-72B-Instruct在16个基准测试中的表现。Qwen2-72B-Instruct在提升基础能力以及对齐人类价值观这两方面取得了较好的平衡。相比Qwen1.5的72B模型，Qwen2-72B-Instruct在所有评测中均大幅超越，并且了取得了匹敌Llama-3-70B-Instruct的表现。1\n而在小模型方面，Qwen2系列模型基本能够超越同等规模的最优开源模型甚至更大规模的模型。相比近期推出的最好的模型，Qwen2-7B-Instruct依然能在多个评测上取得显著的优势，尤其是代码及中文理解上。1\n亮点 代码 \u0026 数学 我们持续投入提升Qwen的代码及数学能力。在代码方面，我们成功将CodeQwen1.5的成功经验融入Qwen2的研发中，实现了在多种编程语言上的显著效果提升。而在数学方面，大规模且高质量的数据帮助Qwen2-72B-Instruct实现了数学解题能力的飞升。\n长文本处理 Qwen2系列中的所有Instruct模型，均在32k上下文长度上进行训练，并通过YARN或Dual Chunk Attention等技术扩展至更长的上下文长度。\n下图展示了我们在Needle in a Haystack测试集上的结果。值得注意的是，Qwen2-72B-Instruct能够完美处理128k上下文长度内的信息抽取任务。结合其本身强大的性能，只要有充足的算力，它一定能成为你处理长文本任务的首选！\n此外，Qwen2系列中的其他模型的表现也十分突出：Qwen2-7B-Instruct几乎完美地处理长达128k的上下文；Qwen2-57B-A14B-Instruct则能处理64k的上下文长度；而该系列中的两个较小模型则支持32k的上下文长度。\n除了长上下文模型，我们还开源了一个智能体解决方案，用于高效处理100万tokens级别的上下文。更多详细信息，请参见我们关于该主题的博客文章。\n安全 下表展示了大型模型在四种多语言不安全查询类别（非法活动、欺诈、色情、隐私暴力）中生成有害响应的比例。测试数据来源于Jailbreak，并被翻译成多种语言进行评估。我们发现Llama-3在处理多语言提示方面表现不佳，因此没有将其纳入比较。通过显著性检验（P值），我们发现Qwen2-72B-Instruct模型在安全性方面与GPT-4的表现相当，并且显著优于Mistral-8x22B模型。\nLanguage Illegal Activity Fraud Pornography Privacy Violence GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct zh 0% 13% 0% 0% 17% 0% 43% 47% 53% 0% 10% 0% en 0% 7% 0% 0% 23% 0% 37% 67% 63% 0% 27% 3% ar 0% 13% 0% 0% 7% 0% 15% 26% 15% 3% 13% 0% es 0% 7% 0% 3% 0% 0% 48% 64% 50% 3% 7% 3% fr 0% 3% 0% 3% 3% 7% 3% 19% 7% 0% 27% 0% ko 0% 4% 0% 3% 8% 4% 17% 29% 10% 0% 26% 4% pt 0% 7% 0% 3% 7% 3% 47% 57% 47% 4% 26% 4% th 0% 10% 0% 7% 23% 3% 13% 17% 10% 13% 7% 7% vi 0% 4% 0% 4% 11% 0% 22% 26% 22% 0% 0% 0% Average 0% 8% 0% 3% 11% 2% 27% 39% 31% 3% 16% 2% 使用Qwen2 现在，模型均已开源在Hugging Face和ModelScope上。欢迎查阅模型卡了解具体用法和更多关于模型的信息，如特性、指标等。\n长时间以来，来自开源生态的朋友们一致支持着Qwen的发展，包括微调（Axolotl、Llama-Factory、Firefly、Swift、XTuner）、量化（AutoGPTQ、AutoAWQ、Neural Compressor）、部署（vLLM、SGL、SkyPilot、TensorRT-LLM、OpenVino、TGI）、本地运行（MLX、Llama.cpp、Ollama、LM Studio）、Agent及RAG（检索增强生成）框架（LlamaIndex, CrewAI, OpenDevin）、评测（LMSys, OpenCompass, Open LLM Leaderboard）、模型二次开发（Dolphin, Openbuddy）。想了解更多关于如何在三方框架中使用Qwen，欢迎阅读各项目的官方文档以及我们的官方文档了解更多用法！\n当然，这里还有很多一直帮助我们的朋友们未被提及。我们真诚地感谢大家的支持，我们也希望社区的合作能够携手推动开源AI的发展。\n模型许可 此次我们采用不同的模型许可。除了Qwen2-72B依旧使用此前的Qianwen License外，其余模型，包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B以及Qwen2-57B-A14B在内，均采用Apache 2.0的许可。我们希望本次开放程度的提升能够加速Qwen2在全球各地的落地及商业应用。\nQwen2的下一步是什么？ 我们还在训练更大的模型，继续探索模型及数据的Scaling Law。此外，我们还将把Qwen2扩展成多模态模型，融入视觉及语音的理解。在不久的将来，我们还会继续开源新模型。敬请期待！\n引用 如果你觉得我们的工作对你有帮助，欢迎引用！\n@article{qwen2, title={Qwen2 Technical Report}, author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } 附录 预训练语言模型评测 对预训练模型的评估主要集中在自然语言理解、通用问题回答、代码、数学、科学知识、推理、多语言能力等能力上。\n评测数据集包括\n英语任务: MMLU (5-shot)、MMLU-Pro (5-shot)、GPQA (5shot)、Theorem QA (5-shot)、BBH (3-shot)、HellaSwag (10-shot)、Winogrande (5-shot)、TruthfulQA (0-shot)以及ARC-C (25-shot)\n代码任务: EvalPlus (0-shot) (HumanEval、MBPP, HumanEval+、MBPP+)、MultiPL-E (0-shot) (Python、C++、JAVA、PHP、TypeScript、C#、Bash和JavaScript)\n数学任务: GSM8K (4-shot)、MATH (4-shot)\n中文任务: C-Eval(5-shot)、CMMLU (5-shot)\n多语言任务: Multi-Exam (M3Exam 5-shot、IndoMMLU 3-shot、ruMMLU 5-shot、mMMLU 5-shot)、Multi-Understanding (BELEBELE 5-shot、XCOPA 5-shot、XWinograd 5-shot、XStoryCloze 0-shot、PAWS-X 5-shot)、Multi-Mathematics (MGSM 8-shot)、Multi-Translation (Flores-101 5-shot)\nQwen2-72B Datasets DeepSeek-V2 Mixtral-8x22B Llama-3-70B Qwen1.5-72B Qwen1.5-110B Qwen2-72B Architecture MoE MoE Dense Dense Dense Dense #Activated Params 21B 39B 70B 72B 110B 72B #Params 236B 140B 70B 72B 110B 72B English MMLU 78.5 77.8 79.5 77.5 80.4 84.2 MMLU-Pro - 49.5 52.8 45.8 49.4 55.6 GPQA - 34.3 36.3 36.3 35.9 37.9 Theorem QA - 35.9 32.3 29.3 34.9 43.1 BBH 78.9 78.9 81.0 65.5 74.8 82.4 HellaSwag 87.8 88.7 88.0 86.0 87.5 87.6 WindoGrande 84.8 85.0 85.3 83.0 83.5 85.1 ARC-C 70.0 70.7 68.8 65.9 69.6 68.9 TruthfulQA 42.2 51.0 45.6 59.6 49.6 54.8 Coding HumanEval 45.7 46.3 48.2 46.3 54.3 64.6 MBPP 73.9 71.7 70.4 66.9 70.9 76.9 EvalPlus 55.0 54.1 54.8 52.9 57.7 65.4 MultiPL-E 44.4 46.7 46.3 41.8 52.7 59.6 Mathematics GSM8K 79.2 83.7 83.0 79.5 85.4 89.5 MATH 43.6 41.7 42.5 34.1 49.6 51.1 Chinese C-Eval 81.7 54.6 65.2 84.1 89.1 91.0 CMMLU 84.0 53.4 67.2 83.5 88.3 90.1 Multilingual Mulit-Exam 67.5 63.5 70.0 66.4 75.6 76.6 Multi-Understanding 77.0 77.7 79.9 78.2 78.2 80.7 Multi-Mathematics 58.8 62.9 67.1 61.7 64.4 76.0 Multi-Translation 36.0 23.3 38.0 35.6 36.2 37.8 Qwen2-57B-A14B Datasets Jamba Mixtral-8x7B Yi-1.5-34B Qwen1.5-32B Qwen2-57B-A14B Architecture MoE MoE Dense Dense MoE #Activated Params 12B 12B 34B 32B 14B #Params 52B 47B 34B 32B 57B English MMLU 67.4 71.8 77.1 74.3 76.5 MMLU-Pro - 41.0 48.3 44.0 43.0 GPQA - 29.2 - 30.8 34.3 Theorem QA - 23.2 - 28.8 33.5 BBH 45.4 50.3 76.4 66.8 67.0 HellaSwag 87.1 86.5 85.9 85.0 85.2 Winogrande 82.5 81.9 84.9 81.5 79.5 ARC-C 64.4 66.0 65.6 63.6 64.1 TruthfulQA 46.4 51.1 53.9 57.4 57.7 Coding HumanEval 29.3 37.2 46.3 43.3 53.0 MBPP - 63.9 65.5 64.2 71.9 EvalPlus - 46.4 51.9 50.4 57.2 MultiPL-E - 39.0 39.5 38.5 49.8 Mathematics GSM8K 59.9 62.5 82.7 76.8 80.7 MATH - 30.8 41.7 36.1 43.0 Chinese C-Eval - - - 83.5 87.7 CMMLU - - 84.8 82.3 88.5 Multilingual Multi-Exam - 56.1 58.3 61.6 65.5 Multi-Understanding - 70.7 73.9 76.5 77.0 Multi-Mathematics - 45.0 49.3 56.1 62.3 Multi-Translation - 29.8 30.0 33.5 34.5 Qwen2-7B Datasets Mistral-7B Gemma-7B Llama-3-8B Qwen1.5-7B Qwen2-7B # Params 7.2B 8.5B 8.0B 7.7B 7.6B # Non-emb Params 7.0B 7.8B 7.0B 6.5B 6.5B English MMLU 64.2 64.6 66.6 61.0 70.3 MMLU-Pro 30.9 33.7 35.4 29.9 40.0 GPQA 24.7 25.7 25.8 26.7 31.8 Theorem QA 19.2 21.5 22.1 14.2 31.1 BBH 56.1 55.1 57.7 40.2 62.6 HellaSwag 83.2 82.2 82.1 78.5 80.7 Winogrande 78.4 79.0 77.4 71.3 77.0 ARC-C 60.0 61.1 59.3 54.2 60.6 TruthfulQA 42.2 44.8 44.0 51.1 54.2 Coding HumanEval 29.3 37.2 33.5 36.0 51.2 MBPP 51.1 50.6 53.9 51.6 65.9 EvalPlus 36.4 39.6 40.3 40.0 54.2 MultiPL-E 29.4 29.7 22.6 28.1 46.3 Mathematics GSM8K 52.2 46.4 56.0 62.5 79.9 MATH 13.1 24.3 20.5 20.3 44.2 Chinese C-Eval 47.4 43.6 49.5 74.1 83.2 CMMLU - - 50.8 73.1 83.9 Multilingual Multi-Exam 47.1 42.7 52.3 47.7 59.2 Multi-Understanding 63.3 58.3 68.6 67.6 72.0 Multi-Mathematics 26.3 39.1 36.3 37.3 57.5 Multi-Translation 23.3 31.2 31.9 28.4 31.5 Qwen2-0.5B \u0026 Qwen2-1.5B Datasets Phi-2 Gemma-2B MiniCPM Qwen1.5-1.8B Qwen2-0.5B Qwen2-1.5B #Non-Emb Params 2.5B 2.0B 2.4B 1.3B 0.35B 1.3B MMLU 52.7 42.3 53.5 46.8 45.4 56.5 MMLU-Pro - 15.9 - - 14.7 21.8 Theorem QA - - - - 8.9 15.0 HumanEval 47.6 22.0 50.0 20.1 22.0 31.1 MBPP 55.0 29.2 47.3 18.0 22.0 37.4 GSM8K 57.2 17.7 53.8 38.4 36.5 58.5 MATH 3.5 11.8 10.2 10.1 10.7 21.7 BBH 43.4 35.2 36.9 24.2 28.4 37.2 HellaSwag 73.1 71.4 68.3 61.4 49.3 66.6 Winogrande 74.4 66.8 - 60.3 56.8 66.2 ARC-C 61.1 48.5 - 37.9 31.5 43.9 TruthfulQA 44.5 33.1 - 39.4 39.7 45.9 C-Eval 23.4 28.0 51.1 59.7 58.2 70.6 CMMLU 24.2 - 51.1 57.8 55.1 70.3 指令微调模型评测1 Qwen2-72B-Instruct Datasets Llama-3-70B-Instruct Qwen1.5-72B-Chat Qwen2-72B-Instruct English MMLU 82.0 75.6 82.3 MMLU-Pro 56.2 51.7 64.4 GPQA 41.9 39.4 42.4 TheroemQA 42.5 28.8 44.4 MT-Bench 8.95 8.61 9.12 Arena-Hard 41.1 36.1 48.1 IFEval (Prompt Strict-Acc.) 77.3 55.8 77.6 Coding HumanEval 81.7 71.3 86.0 MBPP 82.3 71.9 80.2 MultiPL-E 63.4 48.1 69.2 EvalPlus 75.2 66.9 79.0 LiveCodeBench 29.3 17.9 35.7 Mathematics GSM8K 93.0 82.7 91.1 MATH 50.4 42.5 59.7 Chinese C-Eval 61.6 76.1 83.8 AlignBench 7.42 7.28 8.27 Qwen2-57B-A14B-Instruct Datasets Mixtral-8x7B-Instruct-v0.1 Yi-1.5-34B-Chat Qwen1.5-32B-Chat Qwen2-57B-A14B-Instruct Architecture MoE Dense Dense MoE #Activated Params 12B 34B 32B 14B #Params 47B 34B 32B 57B English MMLU 71.4 76.8 74.8 75.4 MMLU-Pro 43.3 52.3 46.4 52.8 GPQA - - 30.8 34.3 TheroemQA - - 30.9 33.1 MT-Bench 8.30 8.50 8.30 8.55 Coding HumanEval 45.1 75.2 68.3 79.9 MBPP 59.5 74.6 67.9 70.9 MultiPL-E - - 50.7 66.4 EvalPlus 48.5 - 63.6 71.6 LiveCodeBench 12.3 - 15.2 25.5 Mathematics GSM8K 65.7 90.2 83.6 79.6 MATH 30.7 50.1 42.4 49.1 Chinese C-Eval - - 76.7 80.5 AlignBench 5.70 7.20 7.19 7.36 Qwen2-7B-Instruct Datasets Llama-3-8B-Instruct Yi-1.5-9B-Chat GLM-4-9B-Chat Qwen1.5-7B-Chat Qwen2-7B-Instruct English MMLU 68.4 69.5 72.4 59.5 70.5 MMLU-Pro 41.0 - - 29.1 44.1 GPQA 34.2 - - 27.8 25.3 TheroemQA 23.0 - - 14.1 25.3 MT-Bench 8.05 8.20 8.35 7.60 8.41 Coding Humaneval 62.2 66.5 71.8 46.3 79.9 MBPP 67.9 - - 48.9 67.2 MultiPL-E 48.5 - - 27.2 59.1 Evalplus 60.9 - - 44.8 70.3 LiveCodeBench 17.3 - - 6.0 26.6 Mathematics GSM8K 79.6 84.8 79.6 60.3 82.3 MATH 30.0 47.7 50.6 23.2 49.6 Chinese C-Eval 45.9 - 75.6 67.3 77.2 AlignBench 6.20 6.90 7.01 6.20 7.21 Qwen2-0.5B-Instruct \u0026 Qwen2-1.5B-Instruct Datasets Qwen1.5-0.5B-Chat Qwen2-0.5B-Instruct Qwen1.5-1.8B-Chat Qwen2-1.5B-Instruct MMLU 35.0 37.9 43.7 52.4 HumanEval 9.1 17.1 25.0 37.8 GSM8K 11.3 40.1 35.3 61.6 C-Eval 37.2 45.2 55.3 63.8 IFEval (Prompt Strict-Acc.) 14.6 20.0 16.8 29.0 多语言能力评测 我们通过多个跨语言开放基准测试以及人工评估，比较了Qwen2指令微调模型与其他近期的大型语言模型。对于基准测试，我们展示了在2个评估数据集上的结果：\nM-MMLU： 来自Okapi的多语言常识理解数据集（我们在阿、德、西、法、意、荷、俄、乌、越、中这几个子集进行测试） MGSM：包含德、英、西、法、日、俄、泰、中和孟在内的数学评测。 结果如下所示：\nModels M-MMLU (5-shot) MGSM (0-shot, CoT) Proprietary LLMs GPT-4-0613 78.0 87.0 GPT-4-Turbo-0409 79.3 90.5 GPT-4o-0513 83.2 89.6 Claude-3-Opus-20240229 80.1 91.0 Claude-3-Sonnet-20240229 71.0 85.6 Open-source LLMs command-r-plus-110b 65.5 63.5 Qwen1.5-7B-Chat 50.0 37.0 Qwen1.5-32B-Chat 65.0 65.0 Qwen1.5-72B-Chat 68.4 71.7 Qwen2-7B-Instruct 60.0 57.0 Qwen2-57B-A14B-Instruct 68.0 74.0 Qwen2-72B-Instruct 78.0 86.6 针对人工评测，我们使用内部评估集比较了Qwen2-72B-Instruct与GPT3.5、GPT4和Claude-3-Opus，该评测集包括10种语言：ar（阿拉伯语）、es（西班牙语）、fr（法语）、ko（韩语）、th（泰语）、vi（越南语）、pt（葡萄牙语）、id（印度尼西亚语）、ja（日语）和ru（俄语）。\nModels ar es fr ko th vi pt id ja ru Average Claude-3-Opus-20240229 4.15 4.31 4.23 4.23 4.01 3.98 4.09 4.40 3.85 4.25 4.15 GPT-4o-0513 3.55 4.26 4.16 4.40 4.09 4.14 3.89 4.39 3.72 4.32 4.09 GPT-4-Turbo-0409 3.44 4.08 4.19 4.24 4.11 3.84 3.86 4.09 3.68 4.27 3.98 Qwen2-72B-Instruct 3.86 4.10 4.01 4.14 3.75 3.91 3.97 3.83 3.63 4.15 3.93 GPT-4-0613 3.55 3.92 3.94 3.87 3.83 3.95 3.55 3.77 3.06 3.63 3.71 GPT-3.5-Turbo-1106 2.52 4.07 3.47 2.37 3.38 2.90 3.37 3.56 2.75 3.24 3.16 将上述结果分类求平均后，结果如下所示：\nModels Knowledge Understanding Creation Math Claude-3-Opus-20240229 3.64 4.45 4.42 3.81 GPT-4o-0513 3.76 4.35 4.45 3.53 GPT-4-Turbo-0409 3.42 4.29 4.35 3.58 Qwen2-72B-Instruct 3.41 4.07 4.36 3.61 GPT-4-0613 3.42 4.09 4.10 3.32 GPT-3.5-Turbo-1106 3.37 3.67 3.89 2.97 以上结果均反映了Qwen2指令微调模型突出的多语言能力。\n2024-07-16 更新: 指令微调模型结果可能与技术报告存在差异；如有不同，以技术报告为准。 ↩︎ ↩︎ ↩︎\n","wordCount":"1599","inLanguage":"zh","datePublished":"2024-06-07T00:00:00+08:00","dateModified":"2024-07-16T00:00:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>你好，Qwen2</h1><div class=post-meta><span title='2024-06-07 00:00:00 +0800 +0800'>2024年6月7日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1599 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen.jpg#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h1><p>历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：</p><ul><li>5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及<strong>Qwen2-72B</strong>；</li><li>在中文英语的基础上，训练数据中增加了<strong>27</strong>种语言相关的高质量数据；</li><li>多个评测基准上的领先表现；</li><li>代码和数学能力显著提升；</li><li>增大了上下文长度支持，最高达到<strong>128K</strong> tokens（Qwen2-72B-Instruct）。<br><br></li></ul><p>目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！</p><h2 id=模型基础信息>模型基础信息<a hidden class=anchor aria-hidden=true href=#模型基础信息>#</a></h2><p>Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:</p><table><thead><tr><th style=text-align:left>模型</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center>Qwen2-1.5B</th><th style=text-align:center>Qwen2-7B</th><th style=text-align:center>Qwen2-57B-A14B</th><th style=text-align:center>Qwen2-72B</th></tr></thead><tbody><tr><td style=text-align:left>参数量</td><td style=text-align:center>0.49B</td><td style=text-align:center>1.54B</td><td style=text-align:center>7.07B</td><td style=text-align:center>57.41B</td><td style=text-align:center>72.71B</td></tr><tr><td style=text-align:left>非Embedding参数量</td><td style=text-align:center>0.35B</td><td style=text-align:center>1.31B</td><td style=text-align:center>5.98B</td><td style=text-align:center>56.32B</td><td style=text-align:center>70.21B</td></tr><tr><td style=text-align:left>GQA</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td></tr><tr><td style=text-align:left>Tie Embedding</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>False</td><td style=text-align:center>False</td><td style=text-align:center>False</td></tr><tr><td style=text-align:left>上下文长度</td><td style=text-align:center>32K</td><td style=text-align:center>32K</td><td style=text-align:center>128K</td><td style=text-align:center>64K</td><td style=text-align:center>128K</td></tr></tbody></table><p>在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。</p><p>上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行<a href=https://github.com/gkamradt/LLMTest_NeedleInAHaystack>大海捞针</a>等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。</p><p>我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：</p><table><thead><tr><th style=text-align:left>地区</th><th style=text-align:center>语言</th></tr></thead><tbody><tr><td style=text-align:left>西欧</td><td style=text-align:center>德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语</td></tr><tr><td style=text-align:left>东欧及中欧</td><td style=text-align:center>俄语、捷克语、波兰语</td></tr><tr><td style=text-align:left>中东</td><td style=text-align:center>阿拉伯语、波斯语、希伯来语、土耳其语</td></tr><tr><td style=text-align:left>东亚</td><td style=text-align:center>日语、韩语</td></tr><tr><td style=text-align:left>东南亚</td><td style=text-align:center>越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语</td></tr><tr><td style=text-align:left>南亚</td><td style=text-align:center>印地语、孟加拉语、乌尔都语</td></tr></tbody></table><p>此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。</p><h1 id=模型评测>模型评测<a hidden class=anchor aria-hidden=true href=#模型评测>#</a></h1><p>相比Qwen1.5，Qwen2在大规模模型实现了非常大幅度的效果提升。我们对Qwen2-72B进行了全方位的评测。在针对预训练语言模型的评估中，对比当前最优的开源模型，Qwen2-72B在包括自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型，如Llama-3-70B以及Qwen1.5最大的模型Qwen1.5-110B。这得益于其预训练数据及训练方法的优化。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b.jpg#center width=100%></figure><p>大规模预训练后，我们对模型进行精细的微调，以提升其智能水平，让其表现更接近人类。这个过程进一步提升了代码、数学、推理、指令遵循、多语言理解等能力。此外，模型学会对齐人类价值观，它也随之变得更加对人类有帮助、诚实以及安全。我们的微调过程遵循的原则是使训练尽可能规模化的同时并且尽可能减少人工标注。我们探索了如何采用多种自动方法以获取高质量、可靠、有创造力的指令和偏好数据，其中包括针对数学的<a href=https://arxiv.org/pdf/2308.01825>拒绝采样</a>、针对代码和指令遵循的代码执行反馈、针对创意写作的回译、针对角色扮演的<a href=https://arxiv.org/pdf/2401.12474>scalable oversight</a>、等等。在训练方面，我们结合了有监督微调、反馈模型训练以及在线DPO等方法。我们还采用了<a href=https://arxiv.org/pdf/2405.17931>在线模型合并</a>的方法减少对齐税。这些做法都大幅提升了模型的基础能力以及模型的智能水平。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b-instruct.jpg#center width=100%></figure><p>我们全面评估了Qwen2-72B-Instruct在16个基准测试中的表现。Qwen2-72B-Instruct在提升基础能力以及对齐人类价值观这两方面取得了较好的平衡。相比Qwen1.5的72B模型，Qwen2-72B-Instruct在所有评测中均大幅超越，并且了取得了匹敌Llama-3-70B-Instruct的表现。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>而在小模型方面，Qwen2系列模型基本能够超越同等规模的最优开源模型甚至更大规模的模型。相比近期推出的最好的模型，Qwen2-7B-Instruct依然能在多个评测上取得显著的优势，尤其是代码及中文理解上。<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-7b.jpg#center width=100%></figure><h1 id=亮点>亮点<a hidden class=anchor aria-hidden=true href=#亮点>#</a></h1><h2 id=代码--数学>代码 & 数学<a hidden class=anchor aria-hidden=true href=#代码--数学>#</a></h2><p>我们持续投入提升Qwen的代码及数学能力。在代码方面，我们成功将<a href=https://qwenlm.github.io/blog/codeqwen1.5/>CodeQwen1.5</a>的成功经验融入Qwen2的研发中，实现了在多种编程语言上的显著效果提升。而在数学方面，大规模且高质量的数据帮助Qwen2-72B-Instruct实现了数学解题能力的飞升。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-code-math.jpg#center width=100%></figure><h2 id=长文本处理>长文本处理<a hidden class=anchor aria-hidden=true href=#长文本处理>#</a></h2><p>Qwen2系列中的所有Instruct模型，均在32k上下文长度上进行训练，并通过<a href=https://arxiv.org/abs/2309.00071>YARN</a>或<a href=https://arxiv.org/abs/2402.17463>Dual Chunk Attention</a>等技术扩展至更长的上下文长度。</p><p>下图展示了我们在<a href=https://github.com/gkamradt/LLMTest_NeedleInAHaystack>Needle in a Haystack</a>测试集上的结果。值得注意的是，Qwen2-72B-Instruct能够完美处理128k上下文长度内的信息抽取任务。结合其本身强大的性能，只要有充足的算力，它一定能成为你处理长文本任务的首选！</p><p>此外，Qwen2系列中的其他模型的表现也十分突出：Qwen2-7B-Instruct几乎完美地处理长达128k的上下文；Qwen2-57B-A14B-Instruct则能处理64k的上下文长度；而该系列中的两个较小模型则支持32k的上下文长度。</p><p>除了长上下文模型，我们还开源了一个智能体解决方案，用于高效处理100万tokens级别的上下文。更多详细信息，请参见<a href=https://qwenlm.github.io/blog/qwen-agent-2405/>我们关于该主题的博客文章</a>。</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2_needle_in_haystack.png#center width=100%></figure><h2 id=安全>安全<a hidden class=anchor aria-hidden=true href=#安全>#</a></h2><p>下表展示了大型模型在四种多语言不安全查询类别（非法活动、欺诈、色情、隐私暴力）中生成有害响应的比例。测试数据来源于<a href=https://github.com/verazuo/jailbreak_llms/tree/main>Jailbreak</a>，并被翻译成多种语言进行评估。我们发现Llama-3在处理多语言提示方面表现不佳，因此没有将其纳入比较。通过显著性检验（P值），我们发现Qwen2-72B-Instruct模型在安全性方面与GPT-4的表现相当，并且显著优于Mistral-8x22B模型。</p><table><thead><tr><th>Language</th><th style=text-align:center></th><th style=text-align:center>Illegal Activity</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Fraud</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Pornography</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Privacy Violence</th><th style=text-align:center></th></tr></thead><tbody><tr><td></td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td></tr><tr><td>zh</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>17%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>43%</strong></td><td style=text-align:center>47%</td><td style=text-align:center>53%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>10%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>en</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>23%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>37%</strong></td><td style=text-align:center>67%</td><td style=text-align:center>63%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>27%</td><td style=text-align:center>3%</td></tr><tr><td>ar</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>15%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>15%</strong></td><td style=text-align:center>3%</td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>es</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>48%</strong></td><td style=text-align:center>64%</td><td style=text-align:center>50%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td></tr><tr><td>fr</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>19%</td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>27%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>ko</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>8%</td><td style=text-align:center>4%</td><td style=text-align:center>17%</td><td style=text-align:center>29%</td><td style=text-align:center><strong>10%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>26%</td><td style=text-align:center>4%</td></tr><tr><td>pt</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center><strong>47%</strong></td><td style=text-align:center>57%</td><td style=text-align:center><strong>47%</strong></td><td style=text-align:center><strong>4%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>4%</strong></td></tr><tr><td>th</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>10%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center>23%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>13%</td><td style=text-align:center>17%</td><td style=text-align:center><strong>10%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>7%</strong></td><td style=text-align:center><strong>7%</strong></td></tr><tr><td>vi</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center>11%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>22%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>22%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>Average</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>8%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center>11%</td><td style=text-align:center><strong>2%</strong></td><td style=text-align:center><strong>27%</strong></td><td style=text-align:center>39%</td><td style=text-align:center>31%</td><td style=text-align:center>3%</td><td style=text-align:center>16%</td><td style=text-align:center><strong>2%</strong></td></tr></tbody></table><h1 id=使用qwen2>使用Qwen2<a hidden class=anchor aria-hidden=true href=#使用qwen2>#</a></h1><p>现在，模型均已开源在Hugging Face和ModelScope上。欢迎查阅模型卡了解具体用法和更多关于模型的信息，如特性、指标等。</p><p>长时间以来，来自开源生态的朋友们一致支持着Qwen的发展，包括微调（<a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>、<a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>、<a href=https://github.com/yangjianxin1/Firefly>Firefly</a>、<a href=https://github.com/modelscope/swift>Swift</a>、<a href=https://github.com/InternLM/xtuner>XTuner</a>）、量化（<a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>、<a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>、<a href=https://github.com/intel/neural-compressor>Neural Compressor</a>）、部署（<a href=https://github.com/vllm-project/vllm>vLLM</a>、<a href=https://github.com/sgl-project/sglang>SGL</a>、<a href=https://github.com/skypilot-org/skypilot>SkyPilot</a>、<a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>、<a href=https://github.com/openvinotoolkit/openvino>OpenVino</a>、<a href=https://github.com/huggingface/text-generation-inference>TGI</a>）、本地运行（<a href=https://github.com/ml-explore/mlx>MLX</a>、<a href=https://github.com/ggerganov/llama.cpp>Llama.cpp</a>、<a href=https://ollama.com/>Ollama</a>、<a href=https://lmstudio.ai/>LM Studio</a>）、Agent及RAG（检索增强生成）框架（<a href=https://www.llamaindex.ai/>LlamaIndex</a>, <a href=https://www.crewai.com/>CrewAI</a>, <a href=https://github.com/OpenDevin/OpenDevin/>OpenDevin</a>）、评测（<a href=https://lmsys.org/>LMSys</a>, <a href=https://opencompass.org.cn/home>OpenCompass</a>, <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a>）、模型二次开发（<a href=https://huggingface.co/cognitivecomputations>Dolphin</a>, <a href=https://github.com/OpenBuddy/OpenBuddy>Openbuddy</a>）。想了解更多关于如何在三方框架中使用Qwen，欢迎阅读各项目的官方文档以及我们的<a href=https://qwen.readthedocs.io/en/latest/>官方文档</a>了解更多用法！</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/logo_v2.png#center width=80%></figure><p>当然，这里还有很多一直帮助我们的朋友们未被提及。我们真诚地感谢大家的支持，我们也希望社区的合作能够携手推动开源AI的发展。</p><h1 id=模型许可>模型许可<a hidden class=anchor aria-hidden=true href=#模型许可>#</a></h1><p>此次我们采用不同的模型许可。除了Qwen2-72B依旧使用此前的Qianwen License外，其余模型，包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B以及Qwen2-57B-A14B在内，均采用<strong>Apache 2.0</strong>的许可。我们希望本次开放程度的提升能够加速Qwen2在全球各地的落地及商业应用。</p><h1 id=qwen2的下一步是什么>Qwen2的下一步是什么？<a hidden class=anchor aria-hidden=true href=#qwen2的下一步是什么>#</a></h1><p>我们还在训练更大的模型，继续探索模型及数据的Scaling Law。此外，我们还将把Qwen2扩展成多模态模型，融入视觉及语音的理解。在不久的将来，我们还会继续开源新模型。敬请期待！</p><h1 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h1><p>如果你觉得我们的工作对你有帮助，欢迎引用！</p><pre tabindex=0><code>@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}
</code></pre><h1 id=附录>附录<a hidden class=anchor aria-hidden=true href=#附录>#</a></h1><h2 id=预训练语言模型评测>预训练语言模型评测<a hidden class=anchor aria-hidden=true href=#预训练语言模型评测>#</a></h2><p>对预训练模型的评估主要集中在自然语言理解、通用问题回答、代码、数学、科学知识、推理、多语言能力等能力上。</p><p>评测数据集包括</p><p><strong>英语任务</strong>: MMLU (5-shot)、MMLU-Pro (5-shot)、GPQA (5shot)、Theorem QA (5-shot)、BBH (3-shot)、HellaSwag (10-shot)、Winogrande (5-shot)、TruthfulQA (0-shot)以及ARC-C (25-shot)</p><p><strong>代码任务</strong>: EvalPlus (0-shot) (HumanEval、MBPP, HumanEval+、MBPP+)、MultiPL-E (0-shot) (Python、C++、JAVA、PHP、TypeScript、C#、Bash和JavaScript)</p><p><strong>数学任务</strong>: GSM8K (4-shot)、MATH (4-shot)</p><p><strong>中文任务</strong>: C-Eval(5-shot)、CMMLU (5-shot)</p><p><strong>多语言任务</strong>: Multi-Exam (M3Exam 5-shot、IndoMMLU 3-shot、ruMMLU 5-shot、mMMLU 5-shot)、Multi-Understanding (BELEBELE 5-shot、XCOPA 5-shot、XWinograd 5-shot、XStoryCloze 0-shot、PAWS-X 5-shot)、Multi-Mathematics (MGSM 8-shot)、Multi-Translation (Flores-101 5-shot)</p><h3 id=qwen2-72b>Qwen2-72B<a hidden class=anchor aria-hidden=true href=#qwen2-72b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>DeepSeek-V2</th><th style=text-align:center>Mixtral-8x22B</th><th style=text-align:center>Llama-3-70B</th><th style=text-align:center>Qwen1.5-72B</th><th style=text-align:center>Qwen1.5-110B</th><th style=text-align:center><strong>Qwen2-72B</strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>21B</td><td style=text-align:center>39B</td><td style=text-align:center>70B</td><td style=text-align:center>72B</td><td style=text-align:center>110B</td><td style=text-align:center>72B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>236B</td><td style=text-align:center>140B</td><td style=text-align:center>70B</td><td style=text-align:center>72B</td><td style=text-align:center>110B</td><td style=text-align:center>72B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>78.5</td><td style=text-align:center>77.8</td><td style=text-align:center>79.5</td><td style=text-align:center>77.5</td><td style=text-align:center>80.4</td><td style=text-align:center><strong>84.2</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>49.5</td><td style=text-align:center>52.8</td><td style=text-align:center>45.8</td><td style=text-align:center>49.4</td><td style=text-align:center><strong>55.6</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>34.3</td><td style=text-align:center>36.3</td><td style=text-align:center>36.3</td><td style=text-align:center>35.9</td><td style=text-align:center><strong>37.9</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>35.9</td><td style=text-align:center>32.3</td><td style=text-align:center>29.3</td><td style=text-align:center>34.9</td><td style=text-align:center><strong>43.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>78.9</td><td style=text-align:center>78.9</td><td style=text-align:center>81.0</td><td style=text-align:center>65.5</td><td style=text-align:center>74.8</td><td style=text-align:center><strong>82.4</strong></td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center>87.8</td><td style=text-align:center><strong>88.7</strong></td><td style=text-align:center>88.0</td><td style=text-align:center>86.0</td><td style=text-align:center>87.5</td><td style=text-align:center>87.6</td></tr><tr><td style=text-align:left>WindoGrande</td><td style=text-align:center>84.8</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>85.3</strong></td><td style=text-align:center>83.0</td><td style=text-align:center>83.5</td><td style=text-align:center>85.1</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>70.0</td><td style=text-align:center><strong>70.7</strong></td><td style=text-align:center>68.8</td><td style=text-align:center>65.9</td><td style=text-align:center>69.6</td><td style=text-align:center>68.9</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>42.2</td><td style=text-align:center>51.0</td><td style=text-align:center>45.6</td><td style=text-align:center><strong>59.6</strong></td><td style=text-align:center>49.6</td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>45.7</td><td style=text-align:center>46.3</td><td style=text-align:center>48.2</td><td style=text-align:center>46.3</td><td style=text-align:center>54.3</td><td style=text-align:center><strong>64.6</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>73.9</td><td style=text-align:center>71.7</td><td style=text-align:center>70.4</td><td style=text-align:center>66.9</td><td style=text-align:center>70.9</td><td style=text-align:center><strong>76.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>55.0</td><td style=text-align:center>54.1</td><td style=text-align:center>54.8</td><td style=text-align:center>52.9</td><td style=text-align:center>57.7</td><td style=text-align:center><strong>65.4</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>44.4</td><td style=text-align:center>46.7</td><td style=text-align:center>46.3</td><td style=text-align:center>41.8</td><td style=text-align:center>52.7</td><td style=text-align:center><strong>59.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>79.2</td><td style=text-align:center>83.7</td><td style=text-align:center>83.0</td><td style=text-align:center>79.5</td><td style=text-align:center>85.4</td><td style=text-align:center><strong>89.5</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>43.6</td><td style=text-align:center>41.7</td><td style=text-align:center>42.5</td><td style=text-align:center>34.1</td><td style=text-align:center>49.6</td><td style=text-align:center><strong>51.1</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>81.7</td><td style=text-align:center>54.6</td><td style=text-align:center>65.2</td><td style=text-align:center>84.1</td><td style=text-align:center>89.1</td><td style=text-align:center><strong>91.0</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>84.0</td><td style=text-align:center>53.4</td><td style=text-align:center>67.2</td><td style=text-align:center>83.5</td><td style=text-align:center>88.3</td><td style=text-align:center><strong>90.1</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Mulit-Exam</td><td style=text-align:center>67.5</td><td style=text-align:center>63.5</td><td style=text-align:center>70.0</td><td style=text-align:center>66.4</td><td style=text-align:center>75.6</td><td style=text-align:center><strong>76.6</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>77.0</td><td style=text-align:center>77.7</td><td style=text-align:center>79.9</td><td style=text-align:center>78.2</td><td style=text-align:center>78.2</td><td style=text-align:center><strong>80.7</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>58.8</td><td style=text-align:center>62.9</td><td style=text-align:center>67.1</td><td style=text-align:center>61.7</td><td style=text-align:center>64.4</td><td style=text-align:center><strong>76.0</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>36.0</td><td style=text-align:center>23.3</td><td style=text-align:center><strong>38.0</strong></td><td style=text-align:center>35.6</td><td style=text-align:center>36.2</td><td style=text-align:center>37.8</td></tr></tbody></table><h3 id=qwen2-57b-a14b>Qwen2-57B-A14B<a hidden class=anchor aria-hidden=true href=#qwen2-57b-a14b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Jamba</th><th style=text-align:center>Mixtral-8x7B</th><th style=text-align:center>Yi-1.5-34B</th><th style=text-align:center>Qwen1.5-32B</th><th style=text-align:center><strong><strong>Qwen2-57B-A14B</strong></strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>MoE</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>12B</td><td style=text-align:center>12B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>14B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>52B</td><td style=text-align:center>47B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>57B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>67.4</td><td style=text-align:center>71.8</td><td style=text-align:center><strong>77.1</strong></td><td style=text-align:center>74.3</td><td style=text-align:center>76.5</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>41.0</td><td style=text-align:center><strong>48.3</strong></td><td style=text-align:center>44.0</td><td style=text-align:center>43.0</td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>29.2</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>34.3</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>23.2</td><td style=text-align:center>-</td><td style=text-align:center>28.8</td><td style=text-align:center><strong>33.5</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>45.4</td><td style=text-align:center>50.3</td><td style=text-align:center><strong>76.4</strong></td><td style=text-align:center>66.8</td><td style=text-align:center>67.0</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>87.1</strong></td><td style=text-align:center>86.5</td><td style=text-align:center>85.9</td><td style=text-align:center>85.0</td><td style=text-align:center>85.2</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>82.5</td><td style=text-align:center>81.9</td><td style=text-align:center><strong>84.9</strong></td><td style=text-align:center>81.5</td><td style=text-align:center>79.5</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>64.4</td><td style=text-align:center><strong>66.0</strong></td><td style=text-align:center>65.6</td><td style=text-align:center>63.6</td><td style=text-align:center>64.1</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>46.4</td><td style=text-align:center>51.1</td><td style=text-align:center>53.9</td><td style=text-align:center>57.4</td><td style=text-align:center><strong>57.7</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>37.2</td><td style=text-align:center>46.3</td><td style=text-align:center>43.3</td><td style=text-align:center><strong>53.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>-</td><td style=text-align:center>63.9</td><td style=text-align:center>65.5</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>71.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>-</td><td style=text-align:center>46.4</td><td style=text-align:center>51.9</td><td style=text-align:center>50.4</td><td style=text-align:center><strong>57.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>-</td><td style=text-align:center>39.0</td><td style=text-align:center>39.5</td><td style=text-align:center>38.5</td><td style=text-align:center><strong>49.8</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>59.9</td><td style=text-align:center>62.5</td><td style=text-align:center><strong>82.7</strong></td><td style=text-align:center>76.8</td><td style=text-align:center>80.7</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center>41.7</td><td style=text-align:center>36.1</td><td style=text-align:center><strong>43.0</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>83.5</td><td style=text-align:center><strong>87.7</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>84.8</td><td style=text-align:center>82.3</td><td style=text-align:center><strong>88.5</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>-</td><td style=text-align:center>56.1</td><td style=text-align:center>58.3</td><td style=text-align:center>61.6</td><td style=text-align:center><strong>65.5</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>-</td><td style=text-align:center>70.7</td><td style=text-align:center>73.9</td><td style=text-align:center>76.5</td><td style=text-align:center><strong>77.0</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>-</td><td style=text-align:center>45.0</td><td style=text-align:center>49.3</td><td style=text-align:center>56.1</td><td style=text-align:center><strong>62.3</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>-</td><td style=text-align:center>29.8</td><td style=text-align:center>30.0</td><td style=text-align:center>33.5</td><td style=text-align:center><strong>34.5</strong></td></tr></tbody></table><h3 id=qwen2-7b>Qwen2-7B<a hidden class=anchor aria-hidden=true href=#qwen2-7b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mistral-7B</th><th style=text-align:center>Gemma-7B</th><th style=text-align:center>Llama-3-8B</th><th style=text-align:center>Qwen1.5-7B</th><th style=text-align:center>Qwen2-7B</th></tr></thead><tbody><tr><td style=text-align:left># Params</td><td style=text-align:center>7.2B</td><td style=text-align:center>8.5B</td><td style=text-align:center>8.0B</td><td style=text-align:center>7.7B</td><td style=text-align:center>7.6B</td></tr><tr><td style=text-align:left># Non-emb Params</td><td style=text-align:center>7.0B</td><td style=text-align:center>7.8B</td><td style=text-align:center>7.0B</td><td style=text-align:center>6.5B</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>64.2</td><td style=text-align:center>64.6</td><td style=text-align:center>66.6</td><td style=text-align:center>61.0</td><td style=text-align:center><strong>70.3</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>30.9</td><td style=text-align:center>33.7</td><td style=text-align:center>35.4</td><td style=text-align:center>29.9</td><td style=text-align:center><strong>40.0</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>24.7</td><td style=text-align:center>25.7</td><td style=text-align:center>25.8</td><td style=text-align:center>26.7</td><td style=text-align:center><strong>31.8</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>19.2</td><td style=text-align:center>21.5</td><td style=text-align:center>22.1</td><td style=text-align:center>14.2</td><td style=text-align:center><strong>31.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>56.1</td><td style=text-align:center>55.1</td><td style=text-align:center>57.7</td><td style=text-align:center>40.2</td><td style=text-align:center><strong>62.6</strong></td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>83.2</strong></td><td style=text-align:center>82.2</td><td style=text-align:center>82.1</td><td style=text-align:center>78.5</td><td style=text-align:center>80.7</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>78.4</td><td style=text-align:center><strong>79.0</strong></td><td style=text-align:center>77.4</td><td style=text-align:center>71.3</td><td style=text-align:center>77.0</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>60.0</td><td style=text-align:center><strong>61.1</strong></td><td style=text-align:center>59.3</td><td style=text-align:center>54.2</td><td style=text-align:center>60.6</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>42.2</td><td style=text-align:center>44.8</td><td style=text-align:center>44.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>54.2</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>37.2</td><td style=text-align:center>33.5</td><td style=text-align:center>36.0</td><td style=text-align:center><strong>51.2</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>51.1</td><td style=text-align:center>50.6</td><td style=text-align:center>53.9</td><td style=text-align:center>51.6</td><td style=text-align:center><strong>65.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>36.4</td><td style=text-align:center>39.6</td><td style=text-align:center>40.3</td><td style=text-align:center>40.0</td><td style=text-align:center><strong>54.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>29.4</td><td style=text-align:center>29.7</td><td style=text-align:center>22.6</td><td style=text-align:center>28.1</td><td style=text-align:center><strong>46.3</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>52.2</td><td style=text-align:center>46.4</td><td style=text-align:center>56.0</td><td style=text-align:center>62.5</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>13.1</td><td style=text-align:center>24.3</td><td style=text-align:center>20.5</td><td style=text-align:center>20.3</td><td style=text-align:center><strong>44.2</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>47.4</td><td style=text-align:center>43.6</td><td style=text-align:center>49.5</td><td style=text-align:center>74.1</td><td style=text-align:center><strong>83.2</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>50.8</td><td style=text-align:center>73.1</td><td style=text-align:center><strong>83.9</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>47.1</td><td style=text-align:center>42.7</td><td style=text-align:center>52.3</td><td style=text-align:center>47.7</td><td style=text-align:center><strong>59.2</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>63.3</td><td style=text-align:center>58.3</td><td style=text-align:center>68.6</td><td style=text-align:center>67.6</td><td style=text-align:center><strong>72.0</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>26.3</td><td style=text-align:center>39.1</td><td style=text-align:center>36.3</td><td style=text-align:center>37.3</td><td style=text-align:center><strong>57.5</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>23.3</td><td style=text-align:center>31.2</td><td style=text-align:center><strong>31.9</strong></td><td style=text-align:center>28.4</td><td style=text-align:center>31.5</td></tr></tbody></table><h3 id=qwen2-05b--qwen2-15b>Qwen2-0.5B & Qwen2-1.5B<a hidden class=anchor aria-hidden=true href=#qwen2-05b--qwen2-15b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Phi-2</th><th style=text-align:center>Gemma-2B</th><th style=text-align:center>MiniCPM</th><th style=text-align:center>Qwen1.5-1.8B</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center>Qwen2-1.5B</th></tr></thead><tbody><tr><td style=text-align:left>#Non-Emb Params</td><td style=text-align:center>2.5B</td><td style=text-align:center>2.0B</td><td style=text-align:center>2.4B</td><td style=text-align:center>1.3B</td><td style=text-align:center>0.35B</td><td style=text-align:center>1.3B</td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>52.7</td><td style=text-align:center>42.3</td><td style=text-align:center>53.5</td><td style=text-align:center>46.8</td><td style=text-align:center>45.4</td><td style=text-align:center><strong>56.5</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>15.9</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>14.7</td><td style=text-align:center>21.8</td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>8.9</td><td style=text-align:center><strong>15.0</strong></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>47.6</td><td style=text-align:center>22.0</td><td style=text-align:center><strong>50.0</strong></td><td style=text-align:center>20.1</td><td style=text-align:center>22.0</td><td style=text-align:center>31.1</td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>55.0</strong></td><td style=text-align:center>29.2</td><td style=text-align:center>47.3</td><td style=text-align:center>18.0</td><td style=text-align:center>22.0</td><td style=text-align:center>37.4</td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>57.2</td><td style=text-align:center>17.7</td><td style=text-align:center>53.8</td><td style=text-align:center>38.4</td><td style=text-align:center>36.5</td><td style=text-align:center><strong>58.5</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>3.5</td><td style=text-align:center>11.8</td><td style=text-align:center>10.2</td><td style=text-align:center>10.1</td><td style=text-align:center>10.7</td><td style=text-align:center><strong>21.7</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center><strong>43.4</strong></td><td style=text-align:center>35.2</td><td style=text-align:center>36.9</td><td style=text-align:center>24.2</td><td style=text-align:center>28.4</td><td style=text-align:center>37.2</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>73.1</strong></td><td style=text-align:center>71.4</td><td style=text-align:center>68.3</td><td style=text-align:center>61.4</td><td style=text-align:center>49.3</td><td style=text-align:center>66.6</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center><strong>74.4</strong></td><td style=text-align:center>66.8</td><td style=text-align:center>-</td><td style=text-align:center>60.3</td><td style=text-align:center>56.8</td><td style=text-align:center>66.2</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center><strong>61.1</strong></td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>37.9</td><td style=text-align:center>31.5</td><td style=text-align:center>43.9</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>44.5</td><td style=text-align:center>33.1</td><td style=text-align:center>-</td><td style=text-align:center>39.4</td><td style=text-align:center>39.7</td><td style=text-align:center><strong>45.9</strong></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>23.4</td><td style=text-align:center>28.0</td><td style=text-align:center>51.1</td><td style=text-align:center>59.7</td><td style=text-align:center>58.2</td><td style=text-align:center><strong>70.6</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>24.2</td><td style=text-align:center>-</td><td style=text-align:center>51.1</td><td style=text-align:center>57.8</td><td style=text-align:center>55.1</td><td style=text-align:center><strong>70.3</strong></td></tr></tbody></table><h2 id=指令微调模型评测1>指令微调模型评测<sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><a hidden class=anchor aria-hidden=true href=#指令微调模型评测1>#</a></h2><h3 id=qwen2-72b-instruct>Qwen2-72B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-72b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-70B-Instruct</th><th style=text-align:center>Qwen1.5-72B-Chat</th><th style=text-align:center><strong>Qwen2-72B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>82.0</td><td style=text-align:center>75.6</td><td style=text-align:center><strong>82.3</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>56.2</td><td style=text-align:center>51.7</td><td style=text-align:center><strong>64.4</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>41.9</td><td style=text-align:center>39.4</td><td style=text-align:center><strong>42.4</strong></td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>42.5</td><td style=text-align:center>28.8</td><td style=text-align:center><strong>44.4</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.95</td><td style=text-align:center>8.61</td><td style=text-align:center><strong>9.12</strong></td></tr><tr><td style=text-align:left>Arena-Hard</td><td style=text-align:center>41.1</td><td style=text-align:center>36.1</td><td style=text-align:center><strong>48.1</strong></td></tr><tr><td style=text-align:left>IFEval (Prompt Strict-Acc.)</td><td style=text-align:center>77.3</td><td style=text-align:center>55.8</td><td style=text-align:center><strong>77.6</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>81.7</td><td style=text-align:center>71.3</td><td style=text-align:center><strong>86.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>82.3</strong></td><td style=text-align:center>71.9</td><td style=text-align:center>80.2</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>63.4</td><td style=text-align:center>48.1</td><td style=text-align:center><strong>69.2</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>75.2</td><td style=text-align:center>66.9</td><td style=text-align:center><strong>79.0</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>29.3</td><td style=text-align:center>17.9</td><td style=text-align:center><strong>35.7</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center><strong>93.0</strong></td><td style=text-align:center>82.7</td><td style=text-align:center>91.1</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>50.4</td><td style=text-align:center>42.5</td><td style=text-align:center><strong>59.7</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>61.6</td><td style=text-align:center>76.1</td><td style=text-align:center><strong>83.8</strong></td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>7.42</td><td style=text-align:center>7.28</td><td style=text-align:center><strong>8.27</strong></td></tr></tbody></table><h3 id=qwen2-57b-a14b-instruct>Qwen2-57B-A14B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-57b-a14b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mixtral-8x7B-Instruct-v0.1</th><th style=text-align:center>Yi-1.5-34B-Chat</th><th style=text-align:center>Qwen1.5-32B-Chat</th><th style=text-align:center><strong>Qwen2-57B-A14B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>MoE</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>12B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>14B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>47B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>57B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>71.4</td><td style=text-align:center><strong>76.8</strong></td><td style=text-align:center>74.8</td><td style=text-align:center>75.4</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>43.3</td><td style=text-align:center>52.3</td><td style=text-align:center>46.4</td><td style=text-align:center><strong>52.8</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>34.3</strong></td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>30.9</td><td style=text-align:center><strong>33.1</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.30</td><td style=text-align:center>8.50</td><td style=text-align:center>8.30</td><td style=text-align:center><strong>8.55</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>45.1</td><td style=text-align:center>75.2</td><td style=text-align:center>68.3</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>59.5</td><td style=text-align:center><strong>74.6</strong></td><td style=text-align:center>67.9</td><td style=text-align:center>70.9</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>50.7</td><td style=text-align:center><strong>66.4</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>63.6</td><td style=text-align:center><strong>71.6</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>12.3</td><td style=text-align:center>-</td><td style=text-align:center>15.2</td><td style=text-align:center><strong>25.5</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>65.7</td><td style=text-align:center><strong>90.2</strong></td><td style=text-align:center>83.6</td><td style=text-align:center>79.6</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>30.7</td><td style=text-align:center><strong>50.1</strong></td><td style=text-align:center>42.4</td><td style=text-align:center>49.1</td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>76.7</td><td style=text-align:center>80.5</td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>5.70</td><td style=text-align:center>7.20</td><td style=text-align:center>7.19</td><td style=text-align:center><strong>7.36</strong></td></tr></tbody></table><h3 id=qwen2-7b-instruct>Qwen2-7B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-7b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-8B-Instruct</th><th style=text-align:center>Yi-1.5-9B-Chat</th><th style=text-align:center>GLM-4-9B-Chat</th><th style=text-align:center>Qwen1.5-7B-Chat</th><th style=text-align:center>Qwen2-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>68.4</td><td style=text-align:center>69.5</td><td style=text-align:center><strong>72.4</strong></td><td style=text-align:center>59.5</td><td style=text-align:center>70.5</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>41.0</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>29.1</td><td style=text-align:center><strong>44.1</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center><strong>34.2</strong></td><td style=text-align:center>-</td><td style=text-align:center><strong>-</strong></td><td style=text-align:center>27.8</td><td style=text-align:center>25.3</td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>23.0</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>14.1</td><td style=text-align:center><strong>25.3</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.05</td><td style=text-align:center>8.20</td><td style=text-align:center>8.35</td><td style=text-align:center>7.60</td><td style=text-align:center><strong>8.41</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Humaneval</td><td style=text-align:center>62.2</td><td style=text-align:center>66.5</td><td style=text-align:center>71.8</td><td style=text-align:center>46.3</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>67.9</strong></td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>48.9</td><td style=text-align:center>67.2</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>27.2</td><td style=text-align:center><strong>59.1</strong></td></tr><tr><td style=text-align:left>Evalplus</td><td style=text-align:center>60.9</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>44.8</td><td style=text-align:center><strong>70.3</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>17.3</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>6.0</td><td style=text-align:center><strong>26.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>79.6</td><td style=text-align:center><strong>84.8</strong></td><td style=text-align:center>79.6</td><td style=text-align:center>60.3</td><td style=text-align:center>82.3</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>30.0</td><td style=text-align:center>47.7</td><td style=text-align:center><strong>50.6</strong></td><td style=text-align:center>23.2</td><td style=text-align:center>49.6</td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>45.9</td><td style=text-align:center>-</td><td style=text-align:center>75.6</td><td style=text-align:center>67.3</td><td style=text-align:center><strong>77.2</strong></td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>6.20</td><td style=text-align:center>6.90</td><td style=text-align:center>7.01</td><td style=text-align:center>6.20</td><td style=text-align:center><strong>7.21</strong></td></tr></tbody></table><h3 id=qwen2-05b-instruct--qwen2-15b-instruct>Qwen2-0.5B-Instruct & Qwen2-1.5B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-05b-instruct--qwen2-15b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen1.5-0.5B-Chat</th><th style=text-align:center><strong>Qwen2-0.5B-Instruct</strong></th><th style=text-align:center>Qwen1.5-1.8B-Chat</th><th style=text-align:center><strong>Qwen2-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left>MMLU</td><td style=text-align:center>35.0</td><td style=text-align:center><strong>37.9</strong></td><td style=text-align:center>43.7</td><td style=text-align:center><strong>52.4</strong></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>9.1</td><td style=text-align:center><strong>17.1</strong></td><td style=text-align:center>25.0</td><td style=text-align:center><strong>37.8</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>11.3</td><td style=text-align:center><strong>40.1</strong></td><td style=text-align:center>35.3</td><td style=text-align:center><strong>61.6</strong></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>37.2</td><td style=text-align:center><strong>45.2</strong></td><td style=text-align:center>55.3</td><td style=text-align:center><strong>63.8</strong></td></tr><tr><td style=text-align:left>IFEval (Prompt Strict-Acc.)</td><td style=text-align:center>14.6</td><td style=text-align:center><strong>20.0</strong></td><td style=text-align:center>16.8</td><td style=text-align:center><strong>29.0</strong></td></tr></tbody></table><h2 id=多语言能力评测>多语言能力评测<a hidden class=anchor aria-hidden=true href=#多语言能力评测>#</a></h2><p>我们通过多个跨语言开放基准测试以及人工评估，比较了Qwen2指令微调模型与其他近期的大型语言模型。对于基准测试，我们展示了在2个评估数据集上的结果：</p><ul><li><a href=https://github.com/nlp-uoregon/mlmm-evaluation>M-MMLU</a>： 来自Okapi的多语言常识理解数据集（我们在阿、德、西、法、意、荷、俄、乌、越、中这几个子集进行测试）</li></ul><ul><li><a href=https://arxiv.org/abs/2210.03057>MGSM</a>：包含德、英、西、法、日、俄、泰、中和孟在内的数学评测。</li></ul><p>结果如下所示：</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>M-MMLU (5-shot)</th><th style=text-align:center>MGSM (0-shot, CoT)</th></tr></thead><tbody><tr><td style=text-align:left><strong><em>Proprietary LLMs</em></strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>78.0</td><td style=text-align:center>87.0</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>79.3</td><td style=text-align:center>90.5</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>83.2</td><td style=text-align:center>89.6</td></tr><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>80.1</td><td style=text-align:center>91.0</td></tr><tr><td style=text-align:left>Claude-3-Sonnet-20240229</td><td style=text-align:center>71.0</td><td style=text-align:center>85.6</td></tr><tr><td style=text-align:left><strong><em>Open-source LLMs</em></strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>command-r-plus-110b</td><td style=text-align:center>65.5</td><td style=text-align:center>63.5</td></tr><tr><td style=text-align:left>Qwen1.5-7B-Chat</td><td style=text-align:center>50.0</td><td style=text-align:center>37.0</td></tr><tr><td style=text-align:left>Qwen1.5-32B-Chat</td><td style=text-align:center>65.0</td><td style=text-align:center>65.0</td></tr><tr><td style=text-align:left>Qwen1.5-72B-Chat</td><td style=text-align:center>68.4</td><td style=text-align:center>71.7</td></tr><tr><td style=text-align:left><strong>Qwen2-7B-Instruct</strong></td><td style=text-align:center><strong>60.0</strong></td><td style=text-align:center><strong>57.0</strong></td></tr><tr><td style=text-align:left><strong>Qwen2-57B-A14B-Instruct</strong></td><td style=text-align:center><strong>68.0</strong></td><td style=text-align:center><strong>74.0</strong></td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center><strong>78.0</strong></td><td style=text-align:center><strong>86.6</strong></td></tr></tbody></table><p>针对人工评测，我们使用内部评估集比较了Qwen2-72B-Instruct与GPT3.5、GPT4和Claude-3-Opus，该评测集包括10种语言：ar（阿拉伯语）、es（西班牙语）、fr（法语）、ko（韩语）、th（泰语）、vi（越南语）、pt（葡萄牙语）、id（印度尼西亚语）、ja（日语）和ru（俄语）。</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>ar</th><th style=text-align:center>es</th><th style=text-align:center>fr</th><th style=text-align:center>ko</th><th style=text-align:center>th</th><th style=text-align:center>vi</th><th style=text-align:center>pt</th><th style=text-align:center>id</th><th style=text-align:center>ja</th><th style=text-align:center>ru</th><th style=text-align:center>Average</th></tr></thead><tbody><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>4.15</td><td style=text-align:center>4.31</td><td style=text-align:center>4.23</td><td style=text-align:center>4.23</td><td style=text-align:center>4.01</td><td style=text-align:center>3.98</td><td style=text-align:center>4.09</td><td style=text-align:center>4.40</td><td style=text-align:center>3.85</td><td style=text-align:center>4.25</td><td style=text-align:center>4.15</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>3.55</td><td style=text-align:center>4.26</td><td style=text-align:center>4.16</td><td style=text-align:center>4.40</td><td style=text-align:center>4.09</td><td style=text-align:center>4.14</td><td style=text-align:center>3.89</td><td style=text-align:center>4.39</td><td style=text-align:center>3.72</td><td style=text-align:center>4.32</td><td style=text-align:center>4.09</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>3.44</td><td style=text-align:center>4.08</td><td style=text-align:center>4.19</td><td style=text-align:center>4.24</td><td style=text-align:center>4.11</td><td style=text-align:center>3.84</td><td style=text-align:center>3.86</td><td style=text-align:center>4.09</td><td style=text-align:center>3.68</td><td style=text-align:center>4.27</td><td style=text-align:center>3.98</td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center>3.86</td><td style=text-align:center>4.10</td><td style=text-align:center>4.01</td><td style=text-align:center>4.14</td><td style=text-align:center>3.75</td><td style=text-align:center>3.91</td><td style=text-align:center>3.97</td><td style=text-align:center>3.83</td><td style=text-align:center>3.63</td><td style=text-align:center>4.15</td><td style=text-align:center>3.93</td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>3.55</td><td style=text-align:center>3.92</td><td style=text-align:center>3.94</td><td style=text-align:center>3.87</td><td style=text-align:center>3.83</td><td style=text-align:center>3.95</td><td style=text-align:center>3.55</td><td style=text-align:center>3.77</td><td style=text-align:center>3.06</td><td style=text-align:center>3.63</td><td style=text-align:center>3.71</td></tr><tr><td style=text-align:left>GPT-3.5-Turbo-1106</td><td style=text-align:center>2.52</td><td style=text-align:center>4.07</td><td style=text-align:center>3.47</td><td style=text-align:center>2.37</td><td style=text-align:center>3.38</td><td style=text-align:center>2.90</td><td style=text-align:center>3.37</td><td style=text-align:center>3.56</td><td style=text-align:center>2.75</td><td style=text-align:center>3.24</td><td style=text-align:center>3.16</td></tr></tbody></table><p>将上述结果分类求平均后，结果如下所示：</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Knowledge</th><th style=text-align:center>Understanding</th><th style=text-align:center>Creation</th><th style=text-align:center>Math</th></tr></thead><tbody><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>3.64</td><td style=text-align:center>4.45</td><td style=text-align:center>4.42</td><td style=text-align:center>3.81</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>3.76</td><td style=text-align:center>4.35</td><td style=text-align:center>4.45</td><td style=text-align:center>3.53</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>3.42</td><td style=text-align:center>4.29</td><td style=text-align:center>4.35</td><td style=text-align:center>3.58</td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center>3.41</td><td style=text-align:center>4.07</td><td style=text-align:center>4.36</td><td style=text-align:center>3.61</td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>3.42</td><td style=text-align:center>4.09</td><td style=text-align:center>4.10</td><td style=text-align:center>3.32</td></tr><tr><td style=text-align:left>GPT-3.5-Turbo-1106</td><td style=text-align:center>3.37</td><td style=text-align:center>3.67</td><td style=text-align:center>3.89</td><td style=text-align:center>2.97</td></tr></tbody></table><p>以上结果均反映了Qwen2指令微调模型突出的多语言能力。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>2024-07-16 更新: 指令微调模型结果可能与技术报告存在差异；如有不同，以技术报告为准。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>