<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>QVQ-Max：有依据地思考 | Qwen</title><meta name=keywords content><meta name=description content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。
MathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。
接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。
为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。
举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。
我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。
核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。
细致观察：抓住每一个细节
QVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。
深入推理：不只是“看到”，还要“想到”
仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。
灵活应用：从解答问题到创作
除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。
样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。
职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image RecognitionNextQVQ-Max-PreviewMathematical ReasoningNextQVQ-Max-PreviewInterpreting Palm Readings (For Reference Only)NextQVQ-Max-PreviewVideo UnderstandingNextQVQ-Max-PreviewLearn to code by watching videosNextQVQ-Max-Preview下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向："><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qvq-max-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.25451dd4678157e0fb2e84a2fba5ad7861ab458e1168319a052575d04324b785.css integrity="sha256-JUUd1GeBV+D7LoSi+6WteGGrRY4RaDGaBSV10EMkt4U=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qvq-max-preview/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qvq-max-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="QVQ-Max：有依据地思考"><meta property="og:description" content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。
MathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。
接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。
为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。
举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。
我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。
核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。
细致观察：抓住每一个细节
QVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。
深入推理：不只是“看到”，还要“想到”
仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。
灵活应用：从解答问题到创作
除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。
样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。
职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image RecognitionNextQVQ-Max-PreviewMathematical ReasoningNextQVQ-Max-PreviewInterpreting Palm Readings (For Reference Only)NextQVQ-Max-PreviewVideo UnderstandingNextQVQ-Max-PreviewLearn to code by watching videosNextQVQ-Max-Preview下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向："><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qvq-max-preview/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-28T00:00:04+08:00"><meta property="article:modified_time" content="2025-03-28T00:00:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="QVQ-Max：有依据地思考"><meta name=twitter:description content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。
MathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。
接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。
为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。
举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。
我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。
核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。
细致观察：抓住每一个细节
QVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。
深入推理：不只是“看到”，还要“想到”
仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。
灵活应用：从解答问题到创作
除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。
样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。
职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image RecognitionNextQVQ-Max-PreviewMathematical ReasoningNextQVQ-Max-PreviewInterpreting Palm Readings (For Reference Only)NextQVQ-Max-PreviewVideo UnderstandingNextQVQ-Max-PreviewLearn to code by watching videosNextQVQ-Max-Preview下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"QVQ-Max：有依据地思考","item":"https://qwenlm.github.io/zh/blog/qvq-max-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"QVQ-Max：有依据地思考","name":"QVQ-Max：有依据地思考","description":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\n介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。\nMathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。\n接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。\n为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。\n举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。\n我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。\n核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。\n细致观察：抓住每一个细节\nQVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。\n深入推理：不只是“看到”，还要“想到”\n仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。\n灵活应用：从解答问题到创作\n除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。\n样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。\n职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image Recognition\rNext\rQVQ-Max-Preview\rMathematical Reasoning\rNext\rQVQ-Max-Preview\rInterpreting Palm Readings (For Reference Only)\rNext\rQVQ-Max-Preview\rVideo Understanding\rNext\rQVQ-Max-Preview\rLearn to code by watching videos\rNext\rQVQ-Max-Preview\r下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向：","keywords":[],"articleBody":"\rQWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\n介绍 去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。\nMathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。\n接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。\n为什么需要视觉推理？ 传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。\n举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。\n我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。\n核心能力：从观察到推理 QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。\n细致观察：抓住每一个细节\nQVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。\n深入推理：不只是“看到”，还要“想到”\n仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。\n灵活应用：从解答问题到创作\n除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。\n样例 QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。\n职场工具：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。 学习助手：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。 生活小帮手：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。 Multi-image Recognition\rNext\rQVQ-Max-Preview\rMathematical Reasoning\rNext\rQVQ-Max-Preview\rInterpreting Palm Readings (For Reference Only)\rNext\rQVQ-Max-Preview\rVideo Understanding\rNext\rQVQ-Max-Preview\rLearn to code by watching videos\rNext\rQVQ-Max-Preview\r下一步 目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向：\n更准确地观察：通过视觉内容的校验，如grounding来检查观察内容的准确性提高识别能力。 视觉Agent：提升模型在处理多步和更复杂的任务，如手机电脑操控，玩游戏。 更好的交互：让模型在思考和交互中不局限于文字，还可以涵盖更多的模态，比如工具校验，视觉生成等。 QVQ-Max 是一款既有“眼力”又有“脑力”的视觉推理模型。它不仅能识别图片里的内容，还能结合这些信息进行分析和推理，甚至完成一些创造性的任务。虽然它还在成长阶段，但已经展现出了很大的潜力。我们希望通过不断的优化，让 QVQ-Max 成为一款真正实用的视觉 Agent，帮助大家解决实际问题。\n","wordCount":"92","inLanguage":"zh","datePublished":"2025-03-28T00:00:04+08:00","dateModified":"2025-03-28T00:00:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qvq-max-preview/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>QVQ-Max：有依据地思考</h1><div class=post-meta><span title='2025-03-28 00:00:04 +0800 +0800'>2025年3月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;92 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qvq-max-preview/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><video loop src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/head.mov autoplay></video></figure><p><a href=https://chat.qwenlm.ai class="btn external" target=_blank>QWEN CHAT</a>
<a href=https://github.com/QwenLM/Qwen2.5-VL class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5 class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/collections/Qwen25-VL-58fbb5d31f1d47 class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h3 id=介绍><strong>介绍</strong><a hidden class=anchor aria-hidden=true href=#介绍>#</a></h3><p>去年12月，我们推出了 QVQ-72B-Preview, 作为一个探索模型，它存在很多问题。今天，我们正式推出 QVQ-Max 视觉推理模型的第一版。这款模型的特点是，它不仅能够“看懂”图片和视频里的内容，还能结合这些信息进行分析、推理，甚至给出解决方案。从数学题到生活小问题，从编程代码到艺术创作，QVQ-Max 都表现出了不俗的能力。虽然这只是我们的第一个版本，但它的潜力已经让人眼前一亮。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/test_time.png#center width=80%></figure><p>MathVision 是汇集各类困难多模态数学的 benchmark，我们通过模型在上面的表现来评估模型解决复杂数学问题的能力。如图所示，通过调整模型 thinking 的最大长度，我们会发现模型在 MathVision 的准确率会持续提升，这展现了模型巨大的潜力。</p><p>接下来，我们就来聊聊 QVQ-Max 的设计初衷、实际能力以及它能为大家做些什么。</p><h3 id=为什么需要视觉推理><strong>为什么需要视觉推理？</strong><a hidden class=anchor aria-hidden=true href=#为什么需要视觉推理>#</a></h3><p>传统的AI模型大多依赖文字输入，比如回答问题、写文章或者生成代码。但现实生活中，很多信息并不是用文字表达的，而是以图片、图表甚至视频的形式存在。一张图片可能包含丰富的细节，比如颜色、形状、位置关系等，而这些信息往往比文字更直观、也更复杂。</p><p>举个例子，如果你想知道一张建筑图纸是否合理，光靠描述可能很难判断，但如果能看到图纸并结合专业知识去分析，就会容易得多。这就是视觉推理的意义——它让 AI 不仅能“看”，还能“理解”并“思考”。</p><p>我们设计 QVQ-Max 的目标很简单：让它成为一个既“眼尖”又“脑快”的助手，帮助用户解决各种实际问题。</p><hr><h3 id=核心能力从观察到推理><strong>核心能力：从观察到推理</strong><a hidden class=anchor aria-hidden=true href=#核心能力从观察到推理>#</a></h3><p>QVQ-Max的能力可以总结为三个方面：细致观察、深入推理和灵活应用。下面分别来说说它在这些方面的表现。</p><ol><li><p><strong>细致观察：抓住每一个细节</strong><br>QVQ-Max 对图片的解析能力非常强，无论是复杂的图表还是日常生活中随手拍的照片，它都能快速识别出关键元素。比如，它可以告诉你一张照片里有哪些物品、有什么文字标识，甚至还能指出一些你可能忽略的小细节。</p></li><li><p><strong>深入推理：不只是“看到”，还要“想到”</strong><br>仅仅识别出图片里的内容还不够，QVQ-Max 还能进一步分析这些信息，并结合背景知识得出结论。例如，在一道几何题中，它可以根据题目附带的图形推导出答案；在一段视频里，它能根据画面内容推测出接下来可能发生的情节。</p></li><li><p><strong>灵活应用：从解答问题到创作</strong><br>除了分析和推理，QVQ-Max 还能做一些有趣的事情，比如帮你设计插画、生成短视频脚本，甚至根据你的需求创作角色扮演的内容。如果你上传一幅草稿，它可能会帮你完善成一幅完整的作品；上传一个日常照片，它可以化身犀利的评论家，占卜师。</p></li></ol><h2 id=样例>样例<a hidden class=anchor aria-hidden=true href=#样例>#</a></h2><p>QVQ-Max 的应用范围很广，无论是在学习、工作还是日常生活中，它都能派上用场。</p><ul><li><strong>职场工具</strong>：在工作中，QVQ-Max 可以协助完成数据分析、信息整理、编程写代码等任务。</li><li><strong>学习助手</strong>：对于学生来说，QVQ-Max 可以帮助解答数学、物理等科目的难题，尤其是那些配有图表的题目。它还能通过直观的方式讲解复杂概念，让学习变得更轻松。</li><li><strong>生活小帮手</strong>：在生活中，QVQ-Max 也能提供不少实用建议。比如，它可以根据你的衣柜照片推荐穿搭方案，或者根据食谱图片指导你如何烹饪一道新菜。</li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Multi-image Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls loop src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/travel.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Mathematical Reasoning</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls loop src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/math.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Interpreting Palm Readings (For Reference Only)</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls loop src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/Divination.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Video Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls loop src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/video.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Learn to code by watching videos</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls loop src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/video_and_code.mov autoplay></video></figure></div></div></div></div><hr><h2 id=下一步><strong>下一步</strong><a hidden class=anchor aria-hidden=true href=#下一步>#</a></h2><p>目前的 QVQ-Max 只是第一版，还有很多可以提升的空间。接下来，我们会重点关注以下几个方向：</p><ol><li><strong>更准确地观察</strong>：通过视觉内容的校验，如grounding来检查观察内容的准确性提高识别能力。</li><li><strong>视觉Agent</strong>：提升模型在处理多步和更复杂的任务，如手机电脑操控，玩游戏。</li><li><strong>更好的交互</strong>：让模型在思考和交互中不局限于文字，还可以涵盖更多的模态，比如工具校验，视觉生成等。</li></ol><p>QVQ-Max 是一款既有“眼力”又有“脑力”的视觉推理模型。它不仅能识别图片里的内容，还能结合这些信息进行分析和推理，甚至完成一些创造性的任务。虽然它还在成长阶段，但已经展现出了很大的潜力。我们希望通过不断的优化，让 QVQ-Max 成为一款真正实用的视觉 Agent，帮助大家解决实际问题。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>