<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-Max：探索大规模 MoE 模型的智能 | Qwen</title><meta name=keywords content><meta name=description content="QWEN CHAT API DEMO DISCORD
过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！
性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。
首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。
在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。
在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。
我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2.5-max/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-max/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-max/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-Max：探索大规模 MoE 模型的智能"><meta property="og:description" content="QWEN CHAT API DEMO DISCORD
过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！
性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。
首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。
在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。
在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。
我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2.5-max/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-28T23:00:04+08:00"><meta property="article:modified_time" content="2025-01-28T23:00:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-Max：探索大规模 MoE 模型的智能"><meta name=twitter:description content="QWEN CHAT API DEMO DISCORD
过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！
性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。
首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。
在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。
在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。
我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Qwen2.5-Max：探索大规模 MoE 模型的智能","item":"https://qwenlm.github.io/zh/blog/qwen2.5-max/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-Max：探索大规模 MoE 模型的智能","name":"Qwen2.5-Max：探索大规模 MoE 模型的智能","description":"QWEN CHAT API DEMO DISCORD\n过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！\n性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。\n首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。\n在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。\n在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。\n我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2.","keywords":[],"articleBody":" QWEN CHAT API DEMO DISCORD\n过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 API 的方式进行访问，也可以登录 Qwen Chat 进行体验！\n性能 我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。\n首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。\n在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。\n在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。\n我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2.5-Max 将会达到更高的水平。\n使用 Qwen2.5-Max 现在您可以在 Qwen Chat 中使用 Qwen2.5-Max，直接与模型对话，或者使用 artifacts、搜索等功能。\nQwen2.5-Max 的 API（模型名称为 qwen-max-2025-01-25）现已开放使用。您可以先注册阿里云账号并开通阿里云大模型服务平台，然后在控制台创建 API 密钥。\n由于 Qwen 的 API 与 OpenAI API 兼容，我们可以直接按照使用 OpenAI API 的常规方式进行调用。以下是使用 Python 调用 Qwen2.5-Max 的示例：\nfrom openai import OpenAI import os client = OpenAI( api_key=os.getenv(\"API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", ) completion = client.chat.completions.create( model=\"qwen-max-2025-01-25\", messages=[ {'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Which number is larger, 9.11 or 9.8?'} ] ) print(completion.choices[0].message) 未来展望 持续提升数据规模和模型参数规模能够有效提升模型的智能水平。接下来，我们将持续探索，除了在 pretraining 的 scaling 继续探索外，将大力投入强化学习的 scaling，希望能实现超越人类的智能，驱动 AI 探索未知之境。\n引用 如果您觉得 Qwen2.5 对您有帮助，欢迎引用以下论文。\n@article{qwen25, title={Qwen2.5 technical report}, author={Qwen Team}, journal={arXiv preprint arXiv:2412.15115}, year={2024} } ","wordCount":"162","inLanguage":"zh","datePublished":"2025-01-28T23:00:04+08:00","dateModified":"2025-01-28T23:00:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2.5-max/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-Max：探索大规模 MoE 模型的智能</h1><div class=post-meta><span title='2025-01-28 23:00:04 +0800 +0800'>2025年1月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;162 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2.5-max/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-max-banner.png width=100%></figure><p><a href=https://chat.qwenlm.ai class="btn external" target=_blank>QWEN CHAT</a>
<a href="https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen?spm=a2c63.p38356.help-menu-2400256.d_0_1_0.1f6574a72ddbKE" class="btn external" target=_blank>API</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>过去有一种观点认为，持续地增长数据规模和模型参数规模是一种通向 AGI 的可能的路径。然而，整个大模型社区对于训练超大规模的模型的经验都相对匮乏，不论是稠密模型还是 MoE 模型。近期，DeepSeek V3 的发布让大家了解到超大规模 MoE 模型的效果及实现方法，而同期，Qwen 也在研发超大规模的 MoE 模型 Qwen2.5-Max，使用超过 20 万亿 token 的预训练数据及精心设计的后训练方案进行训练。今天，我们很高兴能给大家分享 Qwen2.5-Max 目前所取得的成果。大家可以通过 <a href="https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen?spm=a2c63.p38356.help-menu-2400256.d_0_1_0.1f6574a72ddbKE">API</a> 的方式进行访问，也可以登录 <a href=https://chat.qwenlm.ai>Qwen Chat</a> 进行体验！</p><h2 id=性能>性能<a hidden class=anchor aria-hidden=true href=#性能>#</a></h2><p>我们将 Qwen2.5-Max 与业界领先的模型（无论是闭源还是开源）在一系列广受关注的基准测试上进行了对比评估。这些基准测试包括测试大学水平知识的 MMLU-Pro、评估编程能力的 LiveCodeBench，全面评估综合能力的 LiveBench，以及近似人类偏好的 Arena-Hard。我们的评估结果涵盖了基座模型和指令模型的性能得分。</p><p>首先，我们直接对比了指令模型的性能表现。指令模型即我们平常使用的可以直接对话的模型。我们将 Qwen2.5-Max 与业界领先的模型（包括 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的性能结果进行了对比。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-max-instruct.jpg width=100%></figure><p>在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等基准测试中，Qwen2.5-Max 的表现超越了 DeepSeek V3。同时在 MMLU-Pro 等其他评估中也展现出了极具竞争力的成绩。</p><p>在基座模型的对比中，由于无法访问 GPT-4o 和 Claude-3.5-Sonnet 等闭源模型的基座模型，我们将 Qwen2.5-Max 与目前领先的开源 MoE 模型 DeepSeek V3、最大的开源稠密模型 Llama-3.1-405B，以及同样位列开源稠密模型前列的 Qwen2.5-72B 进行了对比。对比结果如下图所示。</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Max.jpeg width=100%></figure><p>我们的基座模型在大多数基准测试中都展现出了显著的优势。我们相信，随着后训练技术的不断进步，下一个版本的 Qwen2.5-Max 将会达到更高的水平。</p><h2 id=使用-qwen25-max>使用 Qwen2.5-Max<a hidden class=anchor aria-hidden=true href=#使用-qwen25-max>#</a></h2><p>现在您可以在 Qwen Chat 中使用 Qwen2.5-Max，直接与模型对话，或者使用 artifacts、搜索等功能。</p><video width=100% autoplay loop muted playsinline>
<source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/qwen-max.mp4 type=video/mp4></video><p>Qwen2.5-Max 的 API（模型名称为 <code>qwen-max-2025-01-25</code>）现已开放使用。您可以先<a href=https://account.alibabacloud.com/register/intl_register.htm>注册阿里云账号</a>并开通阿里云大模型服务平台，然后在控制台创建 API 密钥。</p><p>由于 Qwen 的 API 与 OpenAI API 兼容，我们可以直接按照使用 OpenAI API 的常规方式进行调用。以下是使用 Python 调用 Qwen2.5-Max 的示例：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;API_KEY&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-max-2025-01-25&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;system&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;You are a helpful assistant.&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;Which number is larger, 9.11 or 9.8?&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=未来展望>未来展望<a hidden class=anchor aria-hidden=true href=#未来展望>#</a></h2><p>持续提升数据规模和模型参数规模能够有效提升模型的智能水平。接下来，我们将持续探索，除了在 pretraining 的 scaling 继续探索外，将大力投入强化学习的 scaling，希望能实现超越人类的智能，驱动 AI 探索未知之境。</p><h1 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h1><p>如果您觉得 Qwen2.5 对您有帮助，欢迎引用以下论文。</p><pre tabindex=0><code>@article{qwen25,
  title={Qwen2.5 technical report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>