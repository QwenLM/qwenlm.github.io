<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2-VL: 更清晰地看世界 | Qwen</title><meta name=keywords content><meta name=description content="DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：
读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！
模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。
Example: Multi-object RecognitionNextUserOutput the color and number of each box."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/zh/blog/qwen2-vl/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2-vl/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2-vl/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2-VL: 更清晰地看世界"><meta property="og:description" content="DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：
读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！
模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。
Example: Multi-object RecognitionNextUserOutput the color and number of each box."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/zh/blog/qwen2-vl/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-29T00:24:00+08:00"><meta property="article:modified_time" content="2024-08-29T00:24:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2-VL: 更清晰地看世界"><meta name=twitter:description content="DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：
读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！
模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。
Example: Multi-object RecognitionNextUserOutput the color and number of each box."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2-VL: 更清晰地看世界","item":"https://qwenlm.github.io/zh/blog/qwen2-vl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2-VL: 更清晰地看世界","name":"Qwen2-VL: 更清晰地看世界","description":"DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD\n经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：\n读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。\n理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。\n能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。\n多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！\n模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。\nExample: Multi-object Recognition\rNext\rUser\rOutput the color and number of each box.","keywords":[],"articleBody":" DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD\n经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：\n读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。\n理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。\n能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。\n多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！\n模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。\nExample: Multi-object Recognition\rNext\rUser\rOutput the color and number of each box. Qwen2-VL\rThe image shows a stack of colorful blocks with numbers on them. Here is the color and number of each block from top to bottom:\nTop row (single block): Blue with the number 9 Second row (two blocks): Light blue with the number 7, and green with the number 8 Third row (three blocks): Purple with the number 4, pink with the number 5, and light green with the number 6 Bottom row (four blocks): Red with the number 0, orange with the number 1, yellow with the number 2, and light green with the number 3 Example: Plant identification\rNext\rUser\rWhat kind of flower is this? Qwen2-VL\rThe flowers in the picture are forget-me-nots (Myosotis). These small, delicate flowers are known for their vibrant colors, typically blue, pink, or white, and are often used in bouquets and gardens for their charming appearance.\rExample: Document Parsing with Dense Formulas\rNext\rUser\rConvert the content in the image to Markdown Qwen2-VL\rIf $i \u003e 0$, let $\\underline{d}’’ := (i-1, d-i-l+1, l)$. Let $V_{\\underline{d}}$ and $V_{\\underline{d}’’}$ be $r+1$-dimensional subspaces of $H^0(\\mathcal{L}_{\\underline{d}})$ and $H^0(\\mathcal{L}_{\\underline{d}’’})$, respectively, such that $\\varphi_{\\underline{d}, \\underline{d}’’}(V_{\\underline{d}}) = V_{\\underline{d}’’}^{X_1, 0}$. Then the distributivity holds in $V_{\\underline{d}’’}$ if and only if $$ \\dim (V_{\\underline{d}}^{X_2, 0} + V_{\\underline{d}}^{X_3, 0}) - \\dim (V_{\\underline{d}’’}^{X_2, 0} + V_{\\underline{d}’’}^{X_3, 0}) = r + 1 - \\dim (V_{\\underline{d}’’}^{X_1, 0} + V_{\\underline{d}’’}^{X_2, 0} + V_{\\underline{d}’’}^{X_3, 0}) $$\nIf $i + l \\leq d - 1$, let $\\underline{d}’’’ := (i, d-i-l-1, l+1)$. Let $V_{\\underline{d}}$ and $V_{\\underline{d}’’’}$ be $r+1$-dimensional subspaces of $H^0(\\mathcal{L}_{\\underline{d}})$ and $H^0(\\mathcal{L}_{\\underline{d}’’’})$, respectively, such that $\\varphi_{\\underline{d}’’’, \\underline{d}}(V_{\\underline{d}’’’}) = V_{\\underline{d}}^{X_3, 0}$. Then the distributivity holds in $V_{\\underline{d}}$ if and only if $$ \\dim (V_{\\underline{d}’’’}^{X_1, 0} + V_{\\underline{d}’’’}^{X_2, 0}) - \\dim (V_{\\underline{d}}^{X_1, 0} + V_{\\underline{d}}^{X_2, 0}) = r + 1 - \\dim (V_{\\underline{d}}^{X_1, 0} + V_{\\underline{d}}^{X_2, 0} + V_{\\underline{d}}^{X_3, 0}), $$ where the maps $\\varphi_{\\underline{d}, \\underline{d}’’}$ and $\\varphi_{\\underline{d}’’’, \\underline{d}}$ in Proposition 3.14 are the maps linking the corresponding sheaves. Another important result is Proposition 3.16, which establishes an inequality for any exact limit linear series. Specifically, our Proposition 3.16 says:\nLet ${(\\mathcal{L}_{\\underline{d}}, V_{\\underline{d}})}_{\\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then $$ \\sum_{\\underline{d}} \\dim \\left( \\frac{V_{\\underline{d}}}{V_{\\underline{d}}^{X_1, 0} + V_{\\underline{d}}^{X_2, 0} + V_{\\underline{d}}^{X_3, 0}} \\right) \\geq r + 1. $$\nAs a consequence of Proposition 3.14 and Proposition 3.16, in Corollary 3.17, we get the following characterization of exact limit linear series satisfying the distributivity at each multidegree:\nLet ${(\\mathcal{L}_{\\underline{d}}, V_{\\underline{d}})}_{\\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then $$ \\sum_{\\underline{d}} \\dim \\left( \\frac{V_{\\underline{d}}}{V_{\\underline{d}}^{X_1, 0} + V_{\\underline{d}}^{X_2, 0} + V_{\\underline{d}}^{X_3, 0}} \\right) = r + 1 $$ if and only if the distributivity holds in $V_{\\underline{d}}$ for any $\\underline{d}$.\nIn Section 3, we also study simple limit linear series. In Proposition 3.18, we characterize the multidegrees which determine all simple bases. In Proposition 3.20, following the construction in Muñoz [5], Proposition 4.4, we present a certain structure of linearly independent sections for any exact limit linear series satisfying the distributivity property. In propositions 3.21 and 3.22, we obtain some properties of that structure. These properties imply the existence of a simple basis (we give a proof of that in Proposition 3.23).\nWe conclude Section 3 with a characterization of simple limit linear series among the exact limit linear series. Specifically, our Corollary 3.24 says:\nLet ${(\\mathcal{L}_{\\underline{d}}, V_{\\underline{d}})}_{\\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then the following statements are equivalent:\n${(\\mathcal{L}_{\\underline{d}}, V_{\\underline{d}})}_{\\underline{d}}$ is simple. $\\sum_{\\underline{d}} \\dim \\left( \\frac{V_{\\underline{d}}}{V_{\\underline{d}}^{X_1, 0} + V_{\\underline{d}}^{X_2, 0} + V_{\\underline{d}}^{X_3, 0}} \\right) = r + 1$. The distributivity holds in $V_{\\underline{d}}$ for any $\\underline{d}$. In a subsequent work, given a refined limit linear series, we will construct all simple extensions by combining our Proposition 3.14 and the method of Muñoz [6] for the construction of all exact extensions.\nExample: Multilingual Text Recognition\rNext\rUser\r转录图中内容，并识别图中每段内容的语言。 Qwen2-VL\r汉语，也称为“华语”。是中国使用人数最多的语言，也是世界上作为第一语言使用人数最多的语言。是中华优秀传统文化的重要载体。繁体字，又稱為“繁體中文”，與“簡化字”/“簡體字”相對。一般是指漢字簡化運動被簡化字所代替的漢字。\n语言：中文 日本語は地方ごとに多様な方言があり、とりわけ琉球諸島で方言差が著しい。\n语言：日语 한국어(韓國語), 조선말(朝鮮말)는 대한민국과 조선민주주의인민공화국의 공용어이다. 둘은 표기나 문법에서는 차이가 없지만 동사 어미나 표현에서 차이가 있다.\n语言：韩语 Le français est une langue indo-européenne de la famille des langues romanes dont les locuteurs sont appelés « francophones ».\n语言：法语 El español o castellano es una lengua romance procedente del latín hablado, perteneciente a la familia de lenguas indoeuropeas.\n语言：西班牙语 A língua portuguesa, também designada português, é uma língua indo-europeia românica flexiva ocidental originada no galego-português falado no Reino da Galiza e no norte de Portugal.\n语言：葡萄牙语 Is ceann de na teangacha Ceilteacha í an Ghaeilge (nó Gaeilge na hÉireann mar a thugtar uirthi corruair), agus ceann de na trí cinn de theangacha Ceilteacha ar a dtugtar na teangacha Gaelacha (Gaeilge, Gaeilge Mhanann agus Gaeilge na hAlban) go háirithe.\n语言：爱尔兰语 English is a West Germanic language in the Indo-European language family, whose speakers, called Anglophones, originated in early medieval England on the island of Great Britain.\n语言：英语 Die deutsche Sprache bzw. Deutsch ist eine westgermanische Sprache, die weltweit etwa 90 bis 105 Millionen Menschen als Muttersprache und weiteren rund 80 Millionen als Zweit- oder Fremdsprache dient.\n语言：德语 Język polski, polszczyzna — język z grupy zachodniosłowiańskiej (do której należą również czeski, kaszubski, słowacki, języki łużyckie czy wymarły język drzewiański), stanowiącej część rodziny indoeuropejskiej.\n语言：波兰语 Η ελληνική γλώσσα ανήκει στην ινδοευρωπαϊκή οικογένεια και αποτελεί το μοναδικό μέλος του ελληνικού κλάδου.\n语言：希腊语 Tiếng Việt hay Việt ngữ là một ngôn ngữ thuộc ngữ hệ Nam Á, được công nhận là ngôn ngữ chính thức tại Việt Nam.\n语言：越南语 Монгол хэл нь Монгол улсын албан ёсны хэл юм.\n语言：蒙古语 Число владеющих русским языком в России составляет 137,5 млн человек (2010). Всего в мире на русском говорят 258,2 млн человек (2022)\n语言：俄语 हिन्दी विश्व की एक प्रमुख भाषा है और भारत की एक राजभाषा है। केन्द्रीय स्तर पर भारत में सह-आधिकारिक भाषा अंग्रेज़ी है।\n语言：印地语 Kiswahili ni lugha ya Kibantu yenye misamiati mingi ya Kiarabu (35%), lakini sasa ya Kiingereza pia (10%), inayozungumzwa katika eneo kubwa la Afrika ya Mashariki.\n语言：斯瓦希里语 Example: Handwritten Text Recognition\rNext\rUser\rWhat is the text? Qwen2-VL\rThe text on the wall is in Portuguese and reads: “não é normal ter medo de andar sozinha na rua!!!” This translates to: “It is not normal to be afraid to walk alone on the street!!!”\rExample: Handwritten Text Recognition\rNext\rUser\r识别图中所有文字，无需添加标点。 Qwen2-VL\r尊敬的老师\n展信舒颜\n气温骤降\n树叶无声无息的掉落\n教诲如春风\n师恩深似海\n愿温馨、喜悦与幸福\n如同吐露芬芳的花朵\n永远洋溢在您幸福的时光里\n2. 视觉推理：解决现实世界问题 在此版本中，Qwen2-VL 的数学和编程能力得到了显著增强。模型能够通过分析图片解决问题，还可以解读复杂的数学问题，通过图表进行分析，甚至极端长宽比的图片也能正确解读。除此之外，我们还强化了从真实世界图像和图表中提取信息的能力，并改进了其指令跟随的能力。这种视觉感知与逻辑推理的结合，赋予模型解决实际问题的能力，让模型能够扮演生活中的小助手。\nExample: Code\rNext\rUser\rSolve the algorithmic problem in the web page. Qwen2-VL\rTo solve the “Snake in Matrix” problem, we need to implement a function that takes the size of the matrix n and a list of commands, and returns the final position of the snake after executing all the commands. Here’s a step-by-step approach to solve this problem:\nInitialize Variables:\nInitialize the current position of the snake as 0. Initialize the direction of the snake as “RIGHT” (initially, the snake moves to the right). Define Movement Functions:\nCreate a function to move the snake based on the current direction and the size of the matrix n. Update the position of the snake accordingly. Process Commands:\nIterate through each command in the list of commands. For each command, update the direction and position of the snake. Return Final Position:\nAfter processing all commands, return the final position of the snake. Here’s the Python code to implement the solution:\nclass Solution: def finalPositionOfSnake(self, n: int, commands: List[str]) -\u003e int: # Initialize variables position = 0 direction = \"RIGHT\" # Define movement functions def move_right(): nonlocal position position += 1 def move_down(): nonlocal position position += n def move_left(): nonlocal position position -= 1 def move_up(): nonlocal position position -= n # Process commands for command in commands: if command == \"RIGHT\": move_right() elif command == \"DOWN\": move_down() elif command == \"LEFT\": move_left() elif command == \"UP\": move_up() # Return final position return position This code defines a class Solution with a method finalPositionOfSnake that takes the size of the matrix n and a list of commands, and returns the final position of the snake after executing all the commands. The movement functions (move_right, move_down, move_left, move_up) are used to update the position of the snake based on the current direction.\nExample: Math\rNext\rUser\rFind the length of AC in the isosceles triangle ABC. Qwen2-VL\rSince triangle ABC is isosceles, we know that AC = BC. Therefore, we can set the expressions for AC and BC equal to each other and solve for x:\n4x + 1 = 5x - 0.5\nSubtracting 4x from both sides, we get:\n1 = x - 0.5\nAdding 0.5 to both sides, we get:\n1.5 = x\nNow that we know the value of x, we can substitute it back into the expression for AC to find its length:\nAC = 4x + 1\nAC = 4(1.5) + 1\nAC = 6 + 1\nAC = 7\nTherefore, the length of AC is 7.\nExample: Information Extraction of High Resolution Images\rNext\rUser\rExtract all page titles in the screenshot of search result. Qwen2-VL\rThe page titles in the screenshot of the search result are:\n你好，Qwen2 QwenLM/Qwen2 Qwen2 - a Qwen Collection 快速开始 Qwen2-7B Qwen2-Math，新一代数学模型 Qwen Qwen/Qwen2-7B-Instruct [2407.10671] Qwen2 Technical Report qwen2 Example: Math\rNext\rUser\rPlease tell me how many days are between Ruka’s birthday and Yuki’s birthday? Think carefully step by step. Qwen2-VL\rTo determine the number of days between Ruka’s and Yuki’s birthdays, we need to calculate the difference between their respective birth dates.\nRuka’s birthday is on June 24. Yuki’s birthday is on September 17. Let’s break it down step by step:\nFrom June 24 to June 30:\nJune 24 to June 30 is 6 days. From July 1 to August 31:\nJuly has 31 days. August has 31 days. Total days in July and August: 31 + 31 = 62 days. From September 1 to September 17:\nSeptember 1 to September 17 is 17 days. Now, we add all these days together:\nJune 24 to June 30: 6 days July 1 to August 31: 62 days September 1 to September 17: 17 days Total days = 6 + 62 + 17 = 85 days\nTherefore, there are 85 days between Ruka’s and Yuki’s birthdays.\nExample: Information Extraction\rNext\rUser\rWhat are the Linux versions and their release dates in the picture? Return results as a JSON list. Qwen2-VL\r[ { \"version\": \"6.11-rc3\", \"release_date\": \"2024-08-11\" }, { \"version\": \"6.10.4\", \"release_date\": \"2024-08-11\" }, { \"version\": \"6.9.12 [EOL]\", \"release_date\": \"2024-07-27\" }, { \"version\": \"6.6.45\", \"release_date\": \"2024-08-11\" }, { \"version\": \"6.1.104\", \"release_date\": \"2024-08-11\" }, { \"version\": \"5.15.164\", \"release_date\": \"2024-07-27\" }, { \"version\": \"5.10.223\", \"release_date\": \"2024-07-27\" }, { \"version\": \"5.4.281\", \"release_date\": \"2024-07-27\" }, { \"version\": \"4.19.319\", \"release_date\": \"2024-07-27\" }, { \"version\": \"next-20240812\", \"release_date\": \"2024-08-12\" } ] Example: Information Extraction and Organization\rNext\rUser\r将图中的每日的最高气温和最低气温整理成表格。 Qwen2-VL\r以下是图中每日的最高气温和最低气温整理成的表格：\n日期 最高气温 最低气温 08/12 31°C 23°C 08/13 30°C 22°C 08/14 30°C 23°C 08/15 31°C 24°C 08/16 31°C 24°C 08/17 30°C 24°C 08/18 30°C 23°C 08/19 30°C 22°C 希望这对你有帮助！\nExample: Information Extraction of High Resolution Images with Extreme Aspect Ratio\rNext\rUser\rqwen-vl-max-0809对视频内容的理解功能存在哪些限制？ Qwen2-VL\rqwen-vl-max-0809对视频内容的理解功能存在以下限制：视频文件大小最大为150MB，视频文件格式支持MP4、AVI、MKV、MOV、FLV、WMV等，视频尺寸无限制，但视频文件会被调整到约600K像素数，更大尺寸的视频文件不会有更好的理解效果。\r3. 视频理解与实时聊天 除了静态图像，Qwen2-VL 的能力已扩展到视频内容分析。它能够总结视频要点、即时回答相关问题，并维持连贯对话，仿佛一位随时待命的个人助手，帮助用户从视频中获取有价值的信息。这一特性无疑拓宽了AI应用的边界，为用户提供更加直观且即时的帮助。\nExample: Video Understanding\rNext\rUser\rPlease describe the video in detail. Qwen2-VL\rThe video begins with a man speaking to the camera, followed by a group of people sitting in a control room. The camera then cuts to two men floating inside a space station, where they are seen speaking to the camera. The men appear to be astronauts, and they are wearing space suits. The space station is filled with various equipment and machinery, and the camera pans around to show the different areas of the station. The men continue to speak to the camera, and they appear to be discussing their mission and the various tasks they are performing. Overall, the video provides a fascinating glimpse into the world of space exploration and the daily lives of astronauts.\rUser\rWhat is the color of the clothes the two astronauts wearing?\rQwen2-VL\rThe two astronauts are wearing blue and black clothes.\rExample: Multi-Video Understanding\rNext\rUser\r视频中的人在做什么？ Qwen2-VL\r视频中的人正在使用吸尘器清理地板。\rUser\r如果要进一步清洁地面的话，接下来该怎么做？\rQwen2-VL\r如果要进一步清洁地面，可以使用拖把或湿布擦拭地面，以去除吸尘器无法清除的污渍和灰尘。\rUser\r已经用拖把清洁了地面，评价一下清洁工作？ Qwen2-VL\r从视频中可以看出，清洁工作已经完成，地面已经变得干净整洁。\rUser\r通过你看到的，接下来还能做些什么？\rQwen2-VL\r接下来可以将拖把清洗干净，晾干备用。\rExample: Live chat\rNext\rLive chat\rExample: Screen sharing chat\rNext\rScreen sharing chat\r4. Visual Agent 能力: 函数调用和视觉交互. Qwen2-VL 在作为视觉代理方面展示了强大的潜力，已经初步具备一些能够利用视觉能力完成一些自动化的工具调用和交互。\nFunction Calling: 模型支持函数调用，使其能够利用外部工具进行实时数据检索——无论是航班状态、天气预报还是包裹追踪——通过解读视觉线索。视觉解读与功能执行的整合提升了模型的实用性，使其成为信息管理和决策的有力工具。 Example: Function Calling\rNext\rAsk about the weather\rExample: Code Interpreter\rNext\rWrite code based on the structure diagram.\rExample: Code Interpreter\rNext\rVisualize Charts\rExample: Code Interpreter\rNext\rImplement the formula in code\rVisual Interactions: 视觉交互对多模态大模型是更高的感知能力要求。我们初步做了一些简单的探索，让模型能够更像人一样和环境交互。使得 Qwen2-VL 不仅作为观察者，而是能有代替人做更多的执行者的可能。 Example: UI Interactions\rNext\rOperate a Mobile Phone\rExample: Play a Game\rNext\r21 points\rExample: Visual Interactions\rNext\rOperate a Robotic Arm\rExample: Action and Reasoning\rNext\rPut a cold mug in the microwave\r当然模型并不完美，仍然存在一些局限性，比如无法从视频中提取音频、知识仅更新至 2023 年 6 月等。此外，模型在复杂的指令和复杂场景下不能保证完全正确，在计数、人物识别和3D空间感知方面也相对薄弱。\n模型架构 整体上我们仍然延续了 Qwen-VL 中 ViT 加 Qwen2 的串联结构，在三个不同尺度的模型上，我们都采用 600M 规模大小的 ViT，并且支持图像和视频统一输入。为了让模型更清楚地感知视觉信息和理解视频，我们还进行了以下升级：\nQwen2-VL 在架构上的一大改进是实现了对原生动态分辨率的全面支持。与上一代模型相比，Qwen2-VL 能够处理任意分辨率的图像输入，不同大小图片被转换为动态数量的 tokens，最小只占 4 个 tokens。这种设计不仅确保了模型输入与图像原始信息之间的高度一致性，更是模拟了人类视觉感知的自然方式，赋予模型处理任意尺寸图像的强大能力，使其在图像处理领域展现出更加灵活和高效的表现。 Qwen2-VL 在架构上的另一重要创新则是多模态旋转位置嵌入（M-ROPE）。传统的旋转位置嵌入只能捕捉一维序列的位置信息，而 M-ROPE 通过将原始旋转嵌入分解为代表时间、高度和宽度的三个部分，使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息。这一创新赋予了语言模型强大的多模态处理和推理能力，能够更好地理解和建模复杂的多模态数据。 玩转 Qwen2-VL 如需使用 Qwen2-VL 系列的最强模型 Qwen2-VL-72B，目前您可以通过我们的官方 API （登录 DashScope 注册并获取API Key）来体验该模型：\nfrom openai import OpenAI import os import base64 def encode_image(image_path): with open(image_path, \"rb\") as image_file: return base64.b64encode(image_file.read()).decode(\"utf-8\") # Path to your image image_path = \"dog_and_girl.jpeg\" # Getting the base64 string base64_image = encode_image(image_path) def get_response(): client = OpenAI( api_key=os.getenv(\"DASHSCOPE_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", ) completion = client.chat.completions.create( model=\"qwen-vl-max-0809\", messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"What is this?\"}, { \"type\": \"image_url\", \"image_url\": { \"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\" }, }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}, }, ], } ], top_p=0.8, stream=True, stream_options={\"include_usage\": True}, ) for chunk in completion: print(chunk.model_dump_json()) if __name__ == \"__main__\": get_response() Qwen2-VL 系列的 2B 和 7B 模型及其量化模型已经开源，您可以在Hugging Face和ModelScope上找到它们，通过 Model Card 深入了解使用指南、特色功能及性能指标。下面我们展示一个基于 HF Transformers 的最简用例。\n请先确保您已通过此命令 pip install git+https://github.com/huggingface/transformers 从源安装了 transformers。如果您没有通过源安装，您可能遇到如下报错：\nKeyError: 'qwen2_vl' 我们提供了帮助您处理不同类型的视觉输入的工具。它支持 base64、URL、穿插的图文与视频等。您可以通过如下命令安装：\npip install qwen-vl-utils 下面是一段示例代码。我们推荐您使用 flash attention 2 来实现加速和节省显存。\n示例代码 from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor from qwen_vl_utils import process_vision_info # default: Load the model on the available device(s) model = Qwen2VLForConditionalGeneration.from_pretrained( \"Qwen/Qwen2-VL-7B-Instruct\", device_map=\"auto\" ) # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios. # model = Qwen2VLForConditionalGeneration.from_pretrained( # \"Qwen/Qwen2-VL-7B-Instruct\", # torch_dtype=torch.bfloat16, # attn_implementation=\"flash_attention_2\", # device_map=\"auto\", # ) # default processer processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\") # The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage. # min_pixels = 256*28*28 # max_pixels = 1280*28*28 # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels) messages = [ { \"role\": \"user\", \"content\": [ { \"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\", }, {\"type\": \"text\", \"text\": \"Describe this image.\"}, ], } ] # Preparation for inference text = processor.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) image_inputs, video_inputs = process_vision_info(messages) inputs = processor( text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\", ) # Inference: Generation of the output generated_ids = model.generate(**inputs, max_new_tokens=128) generated_ids_trimmed = [ out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False ) print(output_text) 为了方便大家更好地使用我们的模型，我们提供了开源生态的多个三方框架的支持，其中包括量化工具（AutoGPTQ, AutoAWQ）、部署工具（vLLM）、以及微调工具（Llama-Factory）等。\nLicense Qwen2-VL-2B 和 Qwen2-VL-7B 均使用 Apache 2.0 License。\n下一步 我们希望得到您的反馈以及了解您如何应用 Qwen2-VL。在接下来的时间里，我们将进一步基于下一代语言模型打造更强的视觉语言模型，同时将研究结合更多模态以实现一个真正意义的统一模型！\n","wordCount":"2549","inLanguage":"zh","datePublished":"2024-08-29T00:24:00+08:00","dateModified":"2024-08-29T00:24:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/zh/blog/qwen2-vl/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2-VL: 更清晰地看世界</h1><div class=post-meta><span title='2024-08-29 00:24:00 +0800 +0800'>2024年8月29日</span>&nbsp;·&nbsp;12 分钟&nbsp;·&nbsp;2549 字&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://qwenlm.github.io/blog/qwen2-vl/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen2-VL/qwen2vl-head.jpeg#center width=100%></figure><p><a href=https://huggingface.co/spaces/Qwen/Qwen2-VL class="btn external" target=_blank>DEMO</a>
<a href=https://github.com/QwenLM/Qwen2-VL class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api class="btn external" target=_blank>API</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：<strong>Qwen2-VL</strong> ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：</p><ol><li><p><strong>读懂不同分辨率和不同长宽比的图片</strong>：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。</p></li><li><p><strong>理解20分钟以上的长视频</strong>：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。</p></li><li><p><strong>能够操作手机和机器人的视觉智能体</strong>：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。</p></li><li><p><strong>多语言支持</strong>：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。<br><br></p></li></ol><p>我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！</p><h2 id=模型性能>模型性能<a hidden class=anchor aria-hidden=true href=#模型性能>#</a></h2><p>我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。<figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-72b.jpg#center width=100%></figure>在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。<figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-7b.jpg#center width=100%></figure>除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。<figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-2b.jpg#center width=100%></figure></p><h2 id=模型能力案例>模型能力案例<a hidden class=anchor aria-hidden=true href=#模型能力案例>#</a></h2><h3 id=1-更细节的识别理解>1. 更细节的识别理解<a hidden class=anchor aria-hidden=true href=#1-更细节的识别理解>#</a></h3><p>Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Multi-object Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Output the color and number of each box.
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/boxes.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>The image shows a stack of colorful blocks with numbers on them. Here is the color and number of each block from top to bottom:</p><ul><li>Top row (single block): Blue with the number 9</li><li>Second row (two blocks): Light blue with the number 7, and green with the number 8</li><li>Third row (three blocks): Purple with the number 4, pink with the number 5, and light green with the number 6</li><li>Bottom row (four blocks): Red with the number 0, orange with the number 1, yellow with the number 2, and light green with the number 3</li></ul></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Plant identification</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What kind of flower is this?
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/flowers.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content>The flowers in the picture are forget-me-nots (Myosotis). These small, delicate flowers are known for their vibrant colors, typically blue, pink, or white, and are often used in bouquets and gardens for their charming appearance.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing with Dense Formulas</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Convert the content in the image to Markdown
<img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/arxiv_sample.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><ol><li><p>If $i > 0$, let $\underline{d}&rsquo;&rsquo; := (i-1, d-i-l+1, l)$. Let $V_{\underline{d}}$ and $V_{\underline{d}&rsquo;&rsquo;}$ be $r+1$-dimensional subspaces of $H^0(\mathcal{L}_{\underline{d}})$ and $H^0(\mathcal{L}_{\underline{d}&rsquo;&rsquo;})$, respectively, such that $\varphi_{\underline{d}, \underline{d}&rsquo;&rsquo;}(V_{\underline{d}}) = V_{\underline{d}&rsquo;&rsquo;}^{X_1, 0}$. Then the distributivity holds in $V_{\underline{d}&rsquo;&rsquo;}$ if and only if
$$
\dim (V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}) - \dim (V_{\underline{d}&rsquo;&rsquo;}^{X_2, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_3, 0}) = r + 1 - \dim (V_{\underline{d}&rsquo;&rsquo;}^{X_1, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_2, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_3, 0})
$$</p></li><li><p>If $i + l \leq d - 1$, let $\underline{d}&rsquo;&rsquo;&rsquo; := (i, d-i-l-1, l+1)$. Let $V_{\underline{d}}$ and $V_{\underline{d}&rsquo;&rsquo;&rsquo;}$ be $r+1$-dimensional subspaces of $H^0(\mathcal{L}_{\underline{d}})$ and $H^0(\mathcal{L}_{\underline{d}&rsquo;&rsquo;&rsquo;})$, respectively, such that $\varphi_{\underline{d}&rsquo;&rsquo;&rsquo;, \underline{d}}(V_{\underline{d}&rsquo;&rsquo;&rsquo;}) = V_{\underline{d}}^{X_3, 0}$. Then the distributivity holds in $V_{\underline{d}}$ if and only if
$$
\dim (V_{\underline{d}&rsquo;&rsquo;&rsquo;}^{X_1, 0} + V_{\underline{d}&rsquo;&rsquo;&rsquo;}^{X_2, 0}) - \dim (V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0}) = r + 1 - \dim (V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}),
$$
where the maps $\varphi_{\underline{d}, \underline{d}&rsquo;&rsquo;}$ and $\varphi_{\underline{d}&rsquo;&rsquo;&rsquo;, \underline{d}}$ in Proposition 3.14 are the maps linking the corresponding sheaves. Another important result is Proposition 3.16, which establishes an inequality for any exact limit linear series. Specifically, our Proposition 3.16 says:</p></li></ol><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then
$$
\sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) \geq r + 1.
$$</p><p>As a consequence of Proposition 3.14 and Proposition 3.16, in Corollary 3.17, we get the following characterization of exact limit linear series satisfying the distributivity at each multidegree:</p><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then
$$
\sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) = r + 1
$$
if and only if the distributivity holds in $V_{\underline{d}}$ for any $\underline{d}$.</p><p>In Section 3, we also study simple limit linear series. In Proposition 3.18, we characterize the multidegrees which determine all simple bases. In Proposition 3.20, following the construction in Muñoz [5], Proposition 4.4, we present a certain structure of linearly independent sections for any exact limit linear series satisfying the distributivity property. In propositions 3.21 and 3.22, we obtain some properties of that structure. These properties imply the existence of a simple basis (we give a proof of that in Proposition 3.23).</p><p>We conclude Section 3 with a characterization of simple limit linear series among the exact limit linear series. Specifically, our Corollary 3.24 says:</p><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then the following statements are equivalent:</p><ol><li>${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ is simple.</li><li>$\sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) = r + 1$.</li><li>The distributivity holds in $V_{\underline{d}}$ for any $\underline{d}$.</li></ol><p>In a subsequent work, given a refined limit linear series, we will construct all simple extensions by combining our Proposition 3.14 and the method of Muñoz [6] for the construction of all exact extensions.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multilingual Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>转录图中内容，并识别图中每段内容的语言。
<img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot_multilingual4.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><ol><li><p>汉语，也称为“华语”。是中国使用人数最多的语言，也是世界上作为第一语言使用人数最多的语言。是中华优秀传统文化的重要载体。繁体字，又稱為“繁體中文”，與“簡化字”/“簡體字”相對。一般是指漢字簡化運動被簡化字所代替的漢字。</p><ul><li>语言：中文</li></ul></li><li><p>日本語は地方ごとに多様な方言があり、とりわけ琉球諸島で方言差が著しい。</p><ul><li>语言：日语</li></ul></li><li><p>한국어(韓國語), 조선말(朝鮮말)는 대한민국과 조선민주주의인민공화국의 공용어이다. 둘은 표기나 문법에서는 차이가 없지만 동사 어미나 표현에서 차이가 있다.</p><ul><li>语言：韩语</li></ul></li><li><p>Le français est une langue indo-européenne de la famille des langues romanes dont les locuteurs sont appelés « francophones ».</p><ul><li>语言：法语</li></ul></li><li><p>El español o castellano es una lengua romance procedente del latín hablado, perteneciente a la familia de lenguas indoeuropeas.</p><ul><li>语言：西班牙语</li></ul></li><li><p>A língua portuguesa, também designada português, é uma língua indo-europeia românica flexiva ocidental originada no galego-português falado no Reino da Galiza e no norte de Portugal.</p><ul><li>语言：葡萄牙语</li></ul></li><li><p>Is ceann de na teangacha Ceilteacha í an Ghaeilge (nó Gaeilge na hÉireann mar a thugtar uirthi corruair), agus ceann de na trí cinn de theangacha Ceilteacha ar a dtugtar na teangacha Gaelacha (Gaeilge, Gaeilge Mhanann agus Gaeilge na hAlban) go háirithe.</p><ul><li>语言：爱尔兰语</li></ul></li><li><p>English is a West Germanic language in the Indo-European language family, whose speakers, called Anglophones, originated in early medieval England on the island of Great Britain.</p><ul><li>语言：英语</li></ul></li><li><p>Die deutsche Sprache bzw. Deutsch ist eine westgermanische Sprache, die weltweit etwa 90 bis 105 Millionen Menschen als Muttersprache und weiteren rund 80 Millionen als Zweit- oder Fremdsprache dient.</p><ul><li>语言：德语</li></ul></li><li><p>Język polski, polszczyzna — język z grupy zachodniosłowiańskiej (do której należą również czeski, kaszubski, słowacki, języki łużyckie czy wymarły język drzewiański), stanowiącej część rodziny indoeuropejskiej.</p><ul><li>语言：波兰语</li></ul></li><li><p>Η ελληνική γλώσσα ανήκει στην ινδοευρωπαϊκή οικογένεια και αποτελεί το μοναδικό μέλος του ελληνικού κλάδου.</p><ul><li>语言：希腊语</li></ul></li><li><p>Tiếng Việt hay Việt ngữ là một ngôn ngữ thuộc ngữ hệ Nam Á, được công nhận là ngôn ngữ chính thức tại Việt Nam.</p><ul><li>语言：越南语</li></ul></li><li><p>Монгол хэл нь Монгол улсын албан ёсны хэл юм.</p><ul><li>语言：蒙古语</li></ul></li><li><p>Число владеющих русским языком в России составляет 137,5 млн человек (2010). Всего в мире на русском говорят 258,2 млн человек (2022)</p><ul><li>语言：俄语</li></ul></li><li><p>हिन्दी विश्व की एक प्रमुख भाषा है और भारत की एक राजभाषा है। केन्द्रीय स्तर पर भारत में सह-आधिकारिक भाषा अंग्रेज़ी है।</p><ul><li>语言：印地语</li></ul></li><li><p>Kiswahili ni lugha ya Kibantu yenye misamiati mingi ya Kiarabu (35%), lakini sasa ya Kiingereza pia (10%), inayozungumzwa katika eneo kubwa la Afrika ya Mashariki.</p><ul><li>语言：斯瓦希里语</li></ul></li></ol></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Handwritten Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What is the text?
<img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/multilingual_2.png alt=image></div><div class=role>Qwen2-VL</div><div class=content>The text on the wall is in Portuguese and reads: &ldquo;não é normal ter medo de andar sozinha na rua!!!&rdquo; This translates to: &ldquo;It is not normal to be afraid to walk alone on the street!!!&rdquo;</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Handwritten Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>识别图中所有文字，无需添加标点。
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Handwritten_chinese.jpeg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>尊敬的老师</p><p>展信舒颜</p><p>气温骤降</p><p>树叶无声无息的掉落</p><p>教诲如春风</p><p>师恩深似海</p><p>愿温馨、喜悦与幸福</p><p>如同吐露芬芳的花朵</p><p>永远洋溢在您幸福的时光里</p></div></div></div></div><h3 id=2-视觉推理解决现实世界问题>2. 视觉推理：解决现实世界问题<a hidden class=anchor aria-hidden=true href=#2-视觉推理解决现实世界问题>#</a></h3><p>在此版本中，Qwen2-VL 的数学和编程能力得到了显著增强。模型能够通过分析图片解决问题，还可以解读复杂的数学问题，通过图表进行分析，甚至极端长宽比的图片也能正确解读。除此之外，我们还强化了从真实世界图像和图表中提取信息的能力，并改进了其指令跟随的能力。这种视觉感知与逻辑推理的结合，赋予模型解决实际问题的能力，让模型能够扮演生活中的小助手。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Code</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Solve the algorithmic problem in the web page.
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code.jpeg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>To solve the &ldquo;Snake in Matrix&rdquo; problem, we need to implement a function that takes the size of the matrix <code>n</code> and a list of commands, and returns the final position of the snake after executing all the commands. Here&rsquo;s a step-by-step approach to solve this problem:</p><ol><li><p><strong>Initialize Variables</strong>:</p><ul><li>Initialize the current position of the snake as <code>0</code>.</li><li>Initialize the direction of the snake as &ldquo;RIGHT&rdquo; (initially, the snake moves to the right).</li></ul></li><li><p><strong>Define Movement Functions</strong>:</p><ul><li>Create a function to move the snake based on the current direction and the size of the matrix <code>n</code>.</li><li>Update the position of the snake accordingly.</li></ul></li><li><p><strong>Process Commands</strong>:</p><ul><li>Iterate through each command in the list of commands.</li><li>For each command, update the direction and position of the snake.</li></ul></li><li><p><strong>Return Final Position</strong>:</p><ul><li>After processing all commands, return the final position of the snake.</li></ul></li></ol><p>Here&rsquo;s the Python code to implement the solution:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Solution</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>finalPositionOfSnake</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>commands</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Initialize variables</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>direction</span> <span class=o>=</span> <span class=s2>&#34;RIGHT&#34;</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Define movement functions</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>move_right</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>nonlocal</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>            <span class=n>position</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>move_down</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>nonlocal</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>            <span class=n>position</span> <span class=o>+=</span> <span class=n>n</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>move_left</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>nonlocal</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>            <span class=n>position</span> <span class=o>-=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>move_up</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>nonlocal</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>            <span class=n>position</span> <span class=o>-=</span> <span class=n>n</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Process commands</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>command</span> <span class=ow>in</span> <span class=n>commands</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>command</span> <span class=o>==</span> <span class=s2>&#34;RIGHT&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>move_right</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>command</span> <span class=o>==</span> <span class=s2>&#34;DOWN&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>move_down</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>command</span> <span class=o>==</span> <span class=s2>&#34;LEFT&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>move_left</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>command</span> <span class=o>==</span> <span class=s2>&#34;UP&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>move_up</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Return final position</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>position</span>
</span></span></code></pre></div><p>This code defines a class <code>Solution</code> with a method <code>finalPositionOfSnake</code> that takes the size of the matrix <code>n</code> and a list of commands, and returns the final position of the snake after executing all the commands. The movement functions (<code>move_right</code>, <code>move_down</code>, <code>move_left</code>, <code>move_up</code>) are used to update the position of the snake based on the current direction.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Math</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Find the length of AC in the isosceles triangle ABC.
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/triangle.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>Since triangle ABC is isosceles, we know that AC = BC. Therefore, we can set the expressions for AC and BC equal to each other and solve for x:</p><p>4x + 1 = 5x - 0.5</p><p>Subtracting 4x from both sides, we get:</p><p>1 = x - 0.5</p><p>Adding 0.5 to both sides, we get:</p><p>1.5 = x</p><p>Now that we know the value of x, we can substitute it back into the expression for AC to find its length:</p><p>AC = 4x + 1</p><p>AC = 4(1.5) + 1</p><p>AC = 6 + 1</p><p>AC = 7</p><p>Therefore, the length of AC is 7.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction of High Resolution Images</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Extract all page titles in the screenshot of search result.
<img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot1.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>The page titles in the screenshot of the search result are:</p><ol><li>你好，Qwen2</li><li>QwenLM/Qwen2</li><li>Qwen2 - a Qwen Collection</li><li>快速开始</li><li>Qwen2-7B</li><li>Qwen2-Math，新一代数学模型</li><li>Qwen</li><li>Qwen/Qwen2-7B-Instruct</li><li>[2407.10671] Qwen2 Technical Report</li><li>qwen2</li></ol></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Math</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please tell me how many days are between Ruka&rsquo;s birthday and Yuki&rsquo;s birthday? Think carefully step by step.
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/math1.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>To determine the number of days between Ruka&rsquo;s and Yuki&rsquo;s birthdays, we need to calculate the difference between their respective birth dates.</p><ol><li>Ruka&rsquo;s birthday is on June 24.</li><li>Yuki&rsquo;s birthday is on September 17.</li></ol><p>Let&rsquo;s break it down step by step:</p><ol><li><p><strong>From June 24 to June 30:</strong></p><ul><li>June 24 to June 30 is 6 days.</li></ul></li><li><p><strong>From July 1 to August 31:</strong></p><ul><li>July has 31 days.</li><li>August has 31 days.</li><li>Total days in July and August: 31 + 31 = 62 days.</li></ul></li><li><p><strong>From September 1 to September 17:</strong></p><ul><li>September 1 to September 17 is 17 days.</li></ul></li></ol><p>Now, we add all these days together:</p><ul><li>June 24 to June 30: 6 days</li><li>July 1 to August 31: 62 days</li><li>September 1 to September 17: 17 days</li></ul><p>Total days = 6 + 62 + 17 = 85 days</p><p>Therefore, there are 85 days between Ruka&rsquo;s and Yuki&rsquo;s birthdays.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What are the Linux versions and their release dates in the picture? Return results as a JSON list.
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/information_extraction.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;6.11-rc3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-08-11&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;6.10.4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-08-11&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;6.9.12 [EOL]&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-07-27&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;6.6.45&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-08-11&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;6.1.104&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-08-11&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;5.15.164&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-07-27&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;5.10.223&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-07-27&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;5.4.281&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-07-27&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;4.19.319&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-07-27&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;next-20240812&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;release_date&#34;</span><span class=p>:</span> <span class=s2>&#34;2024-08-12&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></div></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction and Organization</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>将图中的每日的最高气温和最低气温整理成表格。
<img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/table_weather.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>以下是图中每日的最高气温和最低气温整理成的表格：</p><table><thead><tr><th>日期</th><th>最高气温</th><th>最低气温</th></tr></thead><tbody><tr><td>08/12</td><td>31°C</td><td>23°C</td></tr><tr><td>08/13</td><td>30°C</td><td>22°C</td></tr><tr><td>08/14</td><td>30°C</td><td>23°C</td></tr><tr><td>08/15</td><td>31°C</td><td>24°C</td></tr><tr><td>08/16</td><td>31°C</td><td>24°C</td></tr><tr><td>08/17</td><td>30°C</td><td>24°C</td></tr><tr><td>08/18</td><td>30°C</td><td>23°C</td></tr><tr><td>08/19</td><td>30°C</td><td>22°C</td></tr></tbody></table><p>希望这对你有帮助！</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction of High Resolution Images with Extreme Aspect Ratio</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>qwen-vl-max-0809对视频内容的理解功能存在哪些限制？
<img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot2.png alt=image></div><div class=role>Qwen2-VL</div><div class=content>qwen-vl-max-0809对视频内容的理解功能存在以下限制：视频文件大小最大为150MB，视频文件格式支持MP4、AVI、MKV、MOV、FLV、WMV等，视频尺寸无限制，但视频文件会被调整到约600K像素数，更大尺寸的视频文件不会有更好的理解效果。</div></div></div></div><h3 id=3-视频理解与实时聊天>3. 视频理解与实时聊天<a hidden class=anchor aria-hidden=true href=#3-视频理解与实时聊天>#</a></h3><p>除了静态图像，Qwen2-VL 的能力已扩展到视频内容分析。它能够总结视频要点、即时回答相关问题，并维持连贯对话，仿佛一位随时待命的个人助手，帮助用户从视频中获取有价值的信息。这一特性无疑拓宽了AI应用的边界，为用户提供更加直观且即时的帮助。</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Video Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please describe the video in detail.<figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>The video begins with a man speaking to the camera, followed by a group of people sitting in a control room. The camera then cuts to two men floating inside a space station, where they are seen speaking to the camera. The men appear to be astronauts, and they are wearing space suits. The space station is filled with various equipment and machinery, and the camera pans around to show the different areas of the station. The men continue to speak to the camera, and they appear to be discussing their mission and the various tasks they are performing. Overall, the video provides a fascinating glimpse into the world of space exploration and the daily lives of astronauts.</div><div class=role>User</div><div class=content>What is the color of the clothes the two astronauts wearing?</div><div class=role>Qwen2-VL</div><div class=content>The two astronauts are wearing blue and black clothes.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Video Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>视频中的人在做什么？<figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/part1_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>视频中的人正在使用吸尘器清理地板。</div><div class=role>User</div><div class=content>如果要进一步清洁地面的话，接下来该怎么做？</div><div class=role>Qwen2-VL</div><div class=content>如果要进一步清洁地面，可以使用拖把或湿布擦拭地面，以去除吸尘器无法清除的污渍和灰尘。</div><div class=role>User</div><div class=content>已经用拖把清洁了地面，评价一下清洁工作？<figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/part2_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>从视频中可以看出，清洁工作已经完成，地面已经变得干净整洁。</div><div class=role>User</div><div class=content>通过你看到的，接下来还能做些什么？</div><div class=role>Qwen2-VL</div><div class=content>接下来可以将拖把清洗干净，晾干备用。</div></div></div></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Live chat</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Live chat</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/live_chat.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Screen sharing chat</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Screen sharing chat</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Screen%20sharing%20chat.mov></video></figure></div></div></div></div><h3 id=4-visual-agent-能力-函数调用和视觉交互>4. Visual Agent 能力: 函数调用和视觉交互.<a hidden class=anchor aria-hidden=true href=#4-visual-agent-能力-函数调用和视觉交互>#</a></h3><p>Qwen2-VL 在作为视觉代理方面展示了强大的潜力，已经初步具备一些能够利用视觉能力完成一些自动化的工具调用和交互。</p><ul><li>Function Calling: 模型支持函数调用，使其能够利用外部工具进行实时数据检索——无论是航班状态、天气预报还是包裹追踪——通过解读视觉线索。视觉解读与功能执行的整合提升了模型的实用性，使其成为信息管理和决策的有力工具。</li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Function Calling</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Ask about the weather</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/funcall_weather.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Write code based on the structure diagram.</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter3.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Visualize Charts</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Implement the formula in code</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter2.mov></video></figure></div></div></div></div><ul><li>Visual Interactions: 视觉交互对多模态大模型是更高的感知能力要求。我们初步做了一些简单的探索，让模型能够更像人一样和环境交互。使得 Qwen2-VL 不仅作为观察者，而是能有代替人做更多的执行者的可能。</li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: UI Interactions</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Operate a Mobile Phone</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Operate%20a%20Mobile%20Phone.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Play a Game</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>21 points</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Play%20a%20Game.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Visual Interactions</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Operate a Robotic Arm</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Operate%20a%20Robotic%20Arm.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Action and Reasoning</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Put a cold mug in the microwave</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/pick_clean_then_place_in_recep_trial.mp4></video></figure></div></div></div></div><p>当然模型并不完美，仍然存在一些局限性，比如无法从视频中提取音频、知识仅更新至 2023 年 6 月等。此外，模型在复杂的指令和复杂场景下不能保证完全正确，在计数、人物识别和3D空间感知方面也相对薄弱。</p><h2 id=模型架构>模型架构<a hidden class=anchor aria-hidden=true href=#模型架构>#</a></h2><p>整体上我们仍然延续了 Qwen-VL 中 ViT 加 Qwen2 的串联结构，在三个不同尺度的模型上，我们都采用 600M 规模大小的 ViT，并且支持图像和视频统一输入。为了让模型更清楚地感知视觉信息和理解视频，我们还进行了以下升级：</p><ul><li>Qwen2-VL 在架构上的一大改进是实现了对<strong>原生动态分辨率</strong>的全面支持。与上一代模型相比，Qwen2-VL 能够处理任意分辨率的图像输入，不同大小图片被转换为动态数量的 tokens，最小只占 4 个 tokens。这种设计不仅确保了模型输入与图像原始信息之间的高度一致性，更是模拟了人类视觉感知的自然方式，赋予模型处理任意尺寸图像的强大能力，使其在图像处理领域展现出更加灵活和高效的表现。</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/qwen2_vl.jpg#center width=90%></figure><ul><li>Qwen2-VL 在架构上的另一重要创新则是<strong>多模态旋转位置嵌入（M-ROPE）</strong>。传统的旋转位置嵌入只能捕捉一维序列的位置信息，而 M-ROPE 通过将原始旋转嵌入分解为代表时间、高度和宽度的三个部分，使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息。这一创新赋予了语言模型强大的多模态处理和推理能力，能够更好地理解和建模复杂的多模态数据。</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/mrope.png#center width=90%></figure><h2 id=玩转-qwen2-vl>玩转 Qwen2-VL<a hidden class=anchor aria-hidden=true href=#玩转-qwen2-vl>#</a></h2><p>如需使用 Qwen2-VL 系列的最强模型 Qwen2-VL-72B，目前您可以通过我们的官方 API （登录 <a href=https://dashscope.aliyun.com/>DashScope</a> 注册并获取API Key）来体验该模型：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>base64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>encode_image</span><span class=p>(</span><span class=n>image_path</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>image_path</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>image_file</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>base64</span><span class=o>.</span><span class=n>b64encode</span><span class=p>(</span><span class=n>image_file</span><span class=o>.</span><span class=n>read</span><span class=p>())</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Path to your image</span>
</span></span><span class=line><span class=cl><span class=n>image_path</span> <span class=o>=</span> <span class=s2>&#34;dog_and_girl.jpeg&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Getting the base64 string</span>
</span></span><span class=line><span class=cl><span class=n>base64_image</span> <span class=o>=</span> <span class=n>encode_image</span><span class=p>(</span><span class=n>image_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_response</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;DASHSCOPE_API_KEY&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-vl-max-0809&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What is this?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image_url&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;image_url&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                            <span class=s2>&#34;url&#34;</span><span class=p>:</span> <span class=s2>&#34;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&#34;</span>
</span></span><span class=line><span class=cl>                        <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image_url&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s2>&#34;image_url&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;url&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;data:image/jpeg;base64,</span><span class=si>{</span><span class=n>base64_image</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                    <span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>top_p</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stream</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stream_options</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;include_usage&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>chunk</span><span class=o>.</span><span class=n>model_dump_json</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>get_response</span><span class=p>()</span>
</span></span></code></pre></div><p>Qwen2-VL 系列的 2B 和 7B 模型及其量化模型已经开源，您可以在Hugging Face和ModelScope上找到它们，通过 Model Card 深入了解使用指南、特色功能及性能指标。下面我们展示一个基于 HF Transformers 的最简用例。</p><p>请先确保您已通过此命令 <code>pip install git+https://github.com/huggingface/transformers</code> 从源安装了 <code>transformers</code>。如果您没有通过源安装，您可能遇到如下报错：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>KeyError: <span class=s1>&#39;qwen2_vl&#39;</span>
</span></span></code></pre></div><p>我们提供了帮助您处理不同类型的视觉输入的工具。它支持 base64、URL、穿插的图文与视频等。您可以通过如下命令安装：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install qwen-vl-utils
</span></span></code></pre></div><p>下面是一段示例代码。我们推荐您使用 flash attention 2 来实现加速和节省显存。</p><details><summary>示例代码</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>Qwen2VLForConditionalGeneration</span><span class=p>,</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoProcessor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>qwen_vl_utils</span> <span class=kn>import</span> <span class=n>process_vision_info</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># default: Load the model on the available device(s)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Qwen2VLForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Qwen/Qwen2-VL-7B-Instruct&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.</span>
</span></span><span class=line><span class=cl><span class=c1># model = Qwen2VLForConditionalGeneration.from_pretrained(</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#34;Qwen/Qwen2-VL-7B-Instruct&#34;,</span>
</span></span><span class=line><span class=cl><span class=c1>#     torch_dtype=torch.bfloat16,</span>
</span></span><span class=line><span class=cl><span class=c1>#     attn_implementation=&#34;flash_attention_2&#34;,</span>
</span></span><span class=line><span class=cl><span class=c1>#     device_map=&#34;auto&#34;,</span>
</span></span><span class=line><span class=cl><span class=c1># )</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># default processer</span>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-VL-7B-Instruct&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.</span>
</span></span><span class=line><span class=cl><span class=c1># min_pixels = 256*28*28</span>
</span></span><span class=line><span class=cl><span class=c1># max_pixels = 1280*28*28</span>
</span></span><span class=line><span class=cl><span class=c1># processor = AutoProcessor.from_pretrained(&#34;Qwen/Qwen2-VL-7B-Instruct&#34;, min_pixels=min_pixels, max_pixels=max_pixels)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=s2>&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Describe this image.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Preparation for inference</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span> <span class=o>=</span> <span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inference: Generation of the output</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>)</span> <span class=p>:]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>output_text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span></code></pre></div></details><blockquote><p>为了方便大家更好地使用我们的模型，我们提供了开源生态的多个三方框架的支持，其中包括量化工具（<a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>）、部署工具（<a href=https://github.com/vllm-project/vllm>vLLM</a>）、以及微调工具（<a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>）等。</p></blockquote><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>Qwen2-VL-2B 和 Qwen2-VL-7B 均使用 Apache 2.0 License。</p><h2 id=下一步>下一步<a hidden class=anchor aria-hidden=true href=#下一步>#</a></h2><p>我们希望得到您的反馈以及了解您如何应用 Qwen2-VL。在接下来的时间里，我们将进一步基于下一代语言模型打造更强的视觉语言模型，同时将研究结合更多模态以实现一个真正意义的统一模型！</p></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>