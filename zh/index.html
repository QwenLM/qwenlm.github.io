<!doctype html><html lang=zh dir=auto><head><meta name=generator content="Hugo 0.106.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen</title><meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/zh/><link crossorigin=anonymous href=/assets/css/stylesheet.3045213cee0439fa94bd732879be5b19072f75f4d28f6e62dd58c34f5243e49f.css integrity="sha256-MEUhPO4EOfqUvXMoeb5bGQcvdfTSj25i3VjDT1JD5J8=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/json href=http://qwenlm.github.io/zh/index.json><link rel=alternate hreflang=en href=http://qwenlm.github.io/><link rel=alternate hreflang=zh href=http://qwenlm.github.io/zh/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.f20a5212619392e989b6d24ad9ce42302014debfad4d3c8c01db030c36d03475.js integrity="sha256-8gpSEmGTkumJttJK2c5CMCAU3r+tTTyMAdsDDDbQNHU="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="http://qwenlm.github.io/zh/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Qwen","url":"http://qwenlm.github.io/","description":"Qwen","thumbnailUrl":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-fade-in"><div class=hero-header><h1>Qwen</h1></div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Qwen2.5: 基础模型大派对！</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 在 Qwen2 发布后的过去三个月里，许多开发者基于 Qwen2 语言模型构建了新的模型，并为我们提供了宝贵的反馈。在这段时间里，我们专注于创建更智能、更博学的语言模型。今天，我们很高兴地向大家介绍 Qwen 家族的最新成员：Qwen2.5。
我们将要宣布的可能是历史上最大的开源发布！让我们开始这场盛会吧！
我们的最新发布包括了语言模型 Qwen2.5，以及专门针对编程的 Qwen2.5-Coder 和数学的 Qwen2.5-Math 模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：
Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B; Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B; Qwen2.5-Math: 1.5B, 7B, 以及72B。 除了3B和72B的版本外，我们所有的开源模型都采用了 Apache 2.0 许可证。您可以在相应的 Hugging Face 仓库中找到许可证文件。除此之外，我们还通过 Model Studio 提供了旗舰语言模型 Qwen-Plus 和 Qwen-Turbo 的 API，诚邀您来体验和使用！此外，我们还开源了相比上个月发布的版本有性能提升的 Qwen2-VL-72B。
如需了解更多关于 Qwen2.5、Qwen2.5-Coder 和 Qwen2.5-Math 的详细信息，请随时访问以下链接：
Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math
准备好迎接我们全面的模型系列所带来的无限可能吧！我们非常高兴能够与您分享这些前沿模型，并期待看到您使用它们所取得的非凡成就！
要点总结 就 Qwen2.5 语言模型而言，所有模型都在我们最新的大规模数据集上进行了预训练，该数据集包含多达 18T tokens。相较于 Qwen2，Qwen2....</p></div><footer class=entry-footer><span title='2024-09-19 00:00:04 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;621 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5: 基础模型大派对！" href=http://qwenlm.github.io/zh/blog/qwen2.5/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-LLM：扩展大型语言模型的边界</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 我们隆重推出最新发布的Qwen2.5系列语言模型！我们共开源了7款decoder-only的稠密模型，参数规模从0.5B到72B不等。我们调研发现产品对10B至30B模型的兴趣明显增加，同时3B规模的模型也越来越适用于移动端场景。为此，Qwen2.5系列开源了Qwen2.5-3B、Qwen2.5-14B 和 Qwen2.5-32B。同时，我们还推出了Qwen-Plus与Qwen-Turbo版本，可以通过阿里云大模型服务平台的API服务进行体验。
相比Qwen2系列，Qwen2.5带来了以下全新升级：
全面开源：考虑到用户对10B至30B范围模型的需求和移动端对3B模型的兴趣，此次除了继续开源Qwen2系列中的0.5B/1.5B/7B/72B四款模型外，Qwen2.5系列还增加了两个高性价比的中等规模模型—— Qwen2.5-14B 和 Qwen2.5-32B，以及一款适合移动端的 Qwen2.5-3B。所有模型在同类开源产品中均具有很强的竞争力，例如Qwen2.5-32B的整体表现超越了Qwen2-72B，Qwen2.5-14B则领先于Qwen2-57B-A14B。
更大规模、更高质量的预数据训练集：我们的预训练数据集规模从 7T tokens 扩展到了 18T tokens。
知识储备升级：Qwen2.5的知识涵盖更广。在MMLU基准中，Qwen2.5-7B 和 72B的得分相较于Qwen2分别从70.3提升到 74.2，和从84.2提升到 86.1。此外，Qwen2.5还在 GPQA、MMLU-Pro、MMLU-redux 和 ARC-c 等多个基准测试中有了明显提升。
代码能力增强：得益于Qwen2.5-Coder的突破，Qwen2.5在代码生成能力上也大幅提升。Qwen2.5-72B-Instruct在LiveCodeBench（2305-2409）、MultiPL-E和MBPP中的分别得分 55.5、75.1 和 88.2，优于Qwen2-72B-Instruct的32.2、69.2和80.2。
数学能力提升：引入了Qwen2-math的技术后，Qwen2.5的数学推理表现也有了快速提升。在MATH基准测试中，Qwen2.5-7B/72B-Instruct得分从Qwen2-7B/72B-Instruct的52.9/69.0上升到了 75.5/83.1。
更符合人类偏好：Qwen2.5生成的内容更加贴近人类的偏好。具体来看，Qwen2.5-72B-Instruct的Arena-Hard得分从 48.1 大幅提升至 81.2，MT-Bench得分也从 9.12 提升到了 9.35，与之前的Qwen2-72B相比提升显著。
其他核心能力提升：Qwen2.5在 指令跟随、生成 长文本（从1K升级到 8K tokens）、理解 结构化数据（如表格），以及生成 结构化输出（尤其是JSON）上都有非常明显的进步。此外，Qwen2.5能够更好响应多样化的 系统提示，用户可以给模型设置 特定角色 或 自定义条件。
模型基础信息 本次发布的 Qwen2.5 语言模型系列包括七个开源模型，规模从 0.5B 到 72B 不等。大多数模型支持 128K（131,072）个 token 的上下文长度，并能生成 8K token 的文本，支持长篇内容创作。除部分特殊版本外，模型主要采用 Apache 2....</p></div><footer class=entry-footer><span title='2024-09-19 00:00:03 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1564 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-LLM：扩展大型语言模型的边界" href=http://qwenlm.github.io/zh/blog/qwen2.5-llm/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Coder: 码无止境，学无止境!</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 四月初，我们发布了 CodeQwen1.5, 得到了社区广泛的关注与喜爱。自那以后，我们一直在继续努力提升代码模型。今天，我们很高兴地宣布新一代的开放代码模型 Qwen2.5-Coder 的发布。并正式将 CodeQwen 的命名改为 Qwen-Coder，我们认为 Coder 更加拟人、灵动，期待其可以在未来真正与人类结对编程。Qwen2.5-Coder 是我们 Qwen2.5 开源家族的一员，共包括三个尺寸的模型：1.5B、 7B 和 32B（在路上）。
本次更新的两大核心包括代码训练数据的进一步 scaling，以及探索在提升代码能力的同时保持数学和通用能力。
码无止境：Qwen2.5-Coder 基于强大的 Qwen2.5 初始化，扩增了更大规模的代码训练数据持续训练，包括源代码、文本代码混合数据、合成数据等共计 5.5T tokens。使得 Qwen2.5-Coder 在代码生成、代码推理、代码修复等任务上都有了显著提升。 学无止境：我们希望 Qwen2.5-Coder 在提升代码能力的同时，也能保持在数学、通用能力等方面的优势。因此，我们在 Qwen2.5-Coder 中加入了更多的数学、通用能力数据，为未来的真实应用提供更为全面的基座。 Qwen2.5-Coder: Base Models Qwen2.5-Coder 最多 128K tokens 上下文，支持 92 种编程语言，并在多个代码相关的评估任务中都取得了显著的提升，包括代码生成、多编程语言代码生成、代码补全、代码修复等。值得注意的是，本次开源的 7B 版本 Qwen2.5-Coder，甚至打败了更大尺寸的 DeepSeek-Coder-V2-Lite 和 Codestral-20B，成为当前最强大的基础代码模型之一。除了代码任务外，Qwen2.5-Coder 也具备极具竞争力的数学能力。面向通用任务，我们评估了 MMLU 和 ARC，结果表明 Qwen2.5-Coder 很好的保持了 Qwen2.5 的通用能力。
Qwen2.5-Coder-Instruct: Instruction-Tuned Models 我们在 Qwen2.5-Coder 的基础上，通过指令微调，得到了 Qwen2.5-Coder-Instruct。Qwen2.5-Coder-Instruct 除了进一步提升了多个任务上的性能外，还在更多的评估中体现出了卓越的泛化性。...</p></div><footer class=entry-footer><span title='2024-09-19 00:00:02 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;233 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Coder: 码无止境，学无止境!" href=http://qwenlm.github.io/zh/blog/qwen2.5-coder/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Math: 世界领先的数学开源大语言模型</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
🚨 Qwen2.5-Math主要被设计用于通过CoT或TIR的方式解中英数学题，我们不推荐在其他任务上使用该系列模型。 简介 一个月前，我们开源了 Qwen 家族的第一款数学专项大语言模型- Qwen2-Math。 今天，我们将它再度升级并开源 Qwen2.5-Math 系列，包括基础模型 Qwen2.5-Math-1.5B/7B/72B，指令微调模型Qwen2.5-Math-1.5B/7B/72B-Instruct和数学奖励模型 Qwen2.5-Math-RM-72B。
相较于 Qwen2-Math 只支持使用思维链（CoT）解答英文数学题目，Qwen2.5 系列扩展为同时支持使用思维链和工具集成推理（TIR） 解决中英双语的数学题。Qwen2.5-Math系列相比上一代Qwen2.5-Math在中文和英文的数学解题能力上均实现了显著提升。
虽然 CoT 在增强 LLM 的推理能力方面发挥着重要作用，但它在实现计算精度和处理复杂的数学或算法推理任务方面依然面临挑战，例如寻找二次方程的根或计算矩阵的特征值等等。而 TIR（如使用python解释器）可以进一步提高模型在精确计算、符号操作和算法操作方面的能力。Qwen2.5-Math-1.5B/7B/72B-Instruct 使用 TIR 在 MATH 基准测试中分别达到 79.7、85.3 和 87.8的高分。
Qwen2.5-Math: 基础模型 Qwen2-Math 和 Qwen2.5-Math 的整体训练流程如上图所示。在训练完 Qwen2-Math 基础模型后，我们通过三个主要途径将其进一步升级为 Qwen2.5-Math 模型：
1）利用 Qwen2-Math-72B-Instruct 模型合成更多高质量的数学预训练数据。
2）通过多轮召回从网络资源、书籍和代码中获取更多高质量的数学数据，尤其是中文数学数据。
3）利用 Qwen2.5 系列基础模型进行参数初始化，它们相比Qwen2有更强大的语言理解、代码生成和文本推理能力。
最终，我们为 Qwen2.5-Math-1.5B/7B/72B 构建了名为 Qwen Math Corpus v2 的预训练数据集，并保持上下文长度为4K。与用于 Qwen2-Math 预训练的 Qwen Math Corpus v1 相比，Qwen Math Corpus v2 的总 token 数量从 700B 增加到超过 1T。...</p></div><footer class=entry-footer><span title='2024-09-19 00:00:01 +0800 +0800'>2024年9月19日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;341 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Math: 世界领先的数学开源大语言模型" href=http://qwenlm.github.io/zh/blog/qwen2.5-math/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-VL: 更清晰地看世界</h2></header><div class=entry-content><p>DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：
读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！
模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。
Example: Multi-object Recognition
Next
User
Output the color and number of each box....</p></div><footer class=entry-footer><span title='2024-08-29 00:24:00 +0800 +0800'>2024年8月29日</span>&nbsp;·&nbsp;12 分钟&nbsp;·&nbsp;2549 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-VL: 更清晰地看世界" href=http://qwenlm.github.io/zh/blog/qwen2-vl/></a></article><footer class=page-footer><nav class=pagination><a class=next href=http://qwenlm.github.io/zh/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>