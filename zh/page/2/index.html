<!doctype html><html lang=zh dir=auto><head><meta name=generator content="Hugo 0.106.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen</title><meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/zh/><link crossorigin=anonymous href=/assets/css/stylesheet.3045213cee0439fa94bd732879be5b19072f75f4d28f6e62dd58c34f5243e49f.css integrity="sha256-MEUhPO4EOfqUvXMoeb5bGQcvdfTSj25i3VjDT1JD5J8=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/json href=http://qwenlm.github.io/zh/index.json><link rel=alternate hreflang=en href=http://qwenlm.github.io/><link rel=alternate hreflang=zh href=http://qwenlm.github.io/zh/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.f20a5212619392e989b6d24ad9ce42302014debfad4d3c8c01db030c36d03475.js integrity="sha256-8gpSEmGTkumJttJK2c5CMCAU3r+tTTyMAdsDDDbQNHU="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="http://qwenlm.github.io/zh/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Qwen","url":"http://qwenlm.github.io/","description":"Qwen","thumbnailUrl":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-fade-in"><div class=hero-header><h1>Qwen</h1></div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Qwen2-VL: 更清晰地看世界</h2></header><div class=entry-content><p>DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
经历了接近一年时间的持续努力，今天我们很高兴地宣布我们最新一代的视觉语言模型：Qwen2-VL ！Qwen2-VL 基于 Qwen2 打造，相比 Qwen-VL，它具有以下特点：
读懂不同分辨率和不同长宽比的图片：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
理解20分钟以上的长视频：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
能够操作手机和机器人的视觉智能体：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
多语言支持：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。 我们以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API！开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。希望能为您提供便捷的开发体验！
模型性能 我们从六个方面来评估我们模型的视觉能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景下的问答、视频理解、Agent 能力。整体来看，我们 72B 规模的模型在大部分的指标上都达到了最优，甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等闭源模型，特别是在文档理解方面优势明显，仅在对综合的大学题目上和 GPT-4o 还有差距。同时 Qwen2-VL 72B 也刷新了开源多模态模型的最好表现。 在 7B 规模上，我们同样支持图像、多图、视频的输入，在更经济的规模上也实现了有竞争力的性能表现，特别是像 DocVQA 之类的文档理解能力和 MTVQA 考察的图片中多语言文字理解能力都处于 SOTA 水平。 除此之外，我们还提供了一个更小的 2B 规模的模型，以此支持移动端的丰富应用。它具备完整图像视频多语言的理解能力，性能强劲，特别在视频文档和通用场景问答相较同规模模型优势明显。 模型能力案例 1. 更细节的识别理解 Qwen2-VL 不仅能识别植物和地标，而且能理解场景中多个对象间的关系。我们还特别增强了对手写文字及图像中多种语言的识别能力，令其在全球范围内更加易用。
Example: Multi-object Recognition
Next
User
Output the color and number of each box....</p></div><footer class=entry-footer><span title='2024-08-29 00:24:00 +0800 +0800'>2024年8月29日</span>&nbsp;·&nbsp;12 分钟&nbsp;·&nbsp;2549 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-VL: 更清晰地看世界" href=http://qwenlm.github.io/zh/blog/qwen2-vl/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-Audio：开启语音对话！</h2></header><div class=entry-content><p>DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
在一个通用的AI系统中，核心模型应该能够理解不同模态的信息。当前的大语言模型现在已经能够理解语言并进行推理，并且已经扩展到了更多的模态，包括视觉和音频。此前我们陆续发布了多个 Qwen 语言模型系列以及 Qwen-VL 和 Qwen-Audio 等多模态模型。今天，我们正式发布 Qwen2-Audio。这是 Qwen-Audio 的下一代版本，它能够接受音频和文本输入，并生成文本输出。Qwen2-Audio 具有以下特点：
语音聊天：用户可以使用语音向音频语言模型发出指令，无需通过自动语音识别（ASR）模块。
音频分析：该模型能够根据文本指令分析音频信息，包括语音、声音、音乐等。
多语言支持：该模型支持超过8种语言和方言，例如中文、英语、粤语、法语、意大利语、西班牙语、德语和日语。
我们在 Hugging Face 和 ModelScope 上开源了 Qwen2-Audio-7B 以及 Qwen2-Audio-7B-Instruct，并且我们搭建了一个 demo 供用户互动。下面是一些具体例子：
语音交互 语音交互：基础交互
Next
User
Your browser does not support the audio element. Qwen2-Audio
这种情况可能会让人感到沮丧和困惑，但是你需要记住的是，你不能强迫任何人去喜欢你。如果你真的喜欢这个女孩，那么最好的办法就是尊重她的感受，不去打扰她，同时也要保持自己的尊严。 以下是一些建议：   1. 尊重她的决定：如果她明确表示不喜欢你，那么你应该尊重她的决定，不要试图改变她的想法。 2. 不要过于干涉：避免过多地干涉她的生活，也不要试图通过各种方式来接近她。 3. 保持距离：尽量避免与她有太多的接触，尤其是在她不希望的情况下。 4. 寻求支持：如果你感到很难过或者困惑，可以向你的朋友或家人寻求支持和建议。   最重要的是，你要保持自己的尊严和自信，不要因为别人的喜好而失去自我。 Your browser does not support the audio element. User
Your browser does not support the audio element....</p></div><footer class=entry-footer><span title='2024-08-09 16:22:39 +0800 +0800'>2024年8月9日</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;1058 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-Audio：开启语音对话！" href=http://qwenlm.github.io/zh/blog/qwen2-audio/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-Math，新一代数学模型</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
🚨 此模型目前主要支持英语。我们将尽快推出中英双语版本。 简介 在过去的一年里，我们非常关注大模型的推理能力的提升，尤其关注其在数学相关的任务上的表现。今天，我们非常高兴地介绍 Qwen2 开源家族的新成员——Qwen2-Math-1.5B/7B/72B 系列。Qwen2-Math 是一系列基于 Qwen2 LLM 构建的专门用于数学解题的语言模型，其数学能力显著超越了开源模型，甚至超过了闭源模型（如 GPT-4o）。我们希望Qwen2-Math能够为科学界解决需要复杂多步逻辑推理的高级数学问题做出贡献。
我们在一系列数学基准评测上评估了我们的数学专用模型 Qwen2-Math。在 Math 上的评测结果表明，我们最大的数学专用模型 Qwen2-Math-72B-Instruct 超越了最先进的模型，包括 GPT-4o、Claude-3.5-Sonnet、Gemini-1.5-Pro 和 Llama-3.1-405B。
Qwen2-Math基础模型 Qwen2-Math 的基础模型使用 Qwen2-1.5B/7B/72B 进行初始化，然后在精心设计的数学专用语料库上进行预训练，该语料库包含大规模高质量的数学网络文本、书籍、代码、考试题目以及由 Qwen2 模型合成的数学预训练数据。
我们在三个广泛使用的英语数学基准 GSM8K、Math 和 MMLU-STEM 上评估了我们的 Qwen2-Math 基模型。此外，我们还评估了三个中国数学基准 CMATH，GaoKao Math Cloze 和 GaoKao Math QA。所有评估均使用 Few-shot CoT 方式。
Qwen2-Math指令微调模型 我们首先基于 Qwen2-Math-72B 训练了一个数学专用的奖励模型。然后，我们将这个密集的奖励信号与一个二元信号结合，该二元信号指示模型是否正确回答了问题。这个组合信号被用作监督来通过拒绝采样构建 SFT 数据，并在此SFT模型的基础上进一步使用 GRPO 来优化模型。
我们对 Qwen2-Math-Instruct 在英语和中文的数学基准评测上进行了评估。除了常用的基准评测，如 GSM8K 和 MATH 之外，我们还加入了更具挑战性的考试以全面检测 Qwen2-Math-Instruct 的能力，例如 OlympiadBench、CollegeMath、高考（GaoKao）、AIME2024 以及 AMC2023。对于中文的数学评测集，我们使用了 CMATH、2024年中国高考数学题以及2024年中国中考数学题。...</p></div><footer class=entry-footer><span title='2024-08-08 00:00:00 +0800 +0800'>2024年8月8日</span>&nbsp;·&nbsp;25 分钟&nbsp;·&nbsp;5159 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-Math，新一代数学模型" href=http://qwenlm.github.io/zh/blog/qwen2-math/></a></article><article class=post-entry><header class=entry-header><h2>你好，Qwen2</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 历经数月努力, 我们很高兴迎来了Qwen系列模型从Qwen1.5到Qwen2的重大升级。这一次，我们为大家带来了：
5个尺寸的预训练和指令微调模型, 包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B； 在中文英语的基础上，训练数据中增加了27种语言相关的高质量数据； 多个评测基准上的领先表现； 代码和数学能力显著提升； 增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。 目前，我们已在Hugging Face和ModelScope上同步开源。期待听到你们的使用反馈！
模型基础信息 Qwen2系列包含5个尺寸的预训练和指令微调模型，其中包括Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。如下表所示:
模型 Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B 参数量 0.49B 1.54B 7.07B 57.41B 72.71B 非Embedding参数量 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False 上下文长度 32K 32K 128K 64K 128K 在Qwen1.5系列中，只有32B和110B的模型使用了GQA。这一次，所有尺寸的模型都使用了GQA，以便让大家体验到GQA带来的推理加速和显存占用降低的优势。针对小模型，由于embedding参数量较大，我们使用了tie embedding的方法让输入和输出层共享参数，增加非embedding参数的占比。
上下文长度方面，所有的预训练模型均在32K tokens的数据上进行训练，并且我们发现其在128K tokens时依然能在PPL评测中取得不错的表现。然而，对指令微调模型而言，除PPL评测之外还需要进行大海捞针等长序列理解实验。在该表中，我们根据大海捞针实测结果，列出了各个指令微调模型所支持的最大上下文长度。而在使用YARN这类方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct均实现了长达128K tokens上下文长度的支持。
我们投入了大量精力研究如何扩展多语言预训练和指令微调数据的规模并提升其质量，从而提升模型的多语言能力。尽管大语言模型本身具有一定的泛化性，我们还是针对性地对除中英文以外的27种语言进行了增强：
地区 语言 西欧 德语、法语、西班牙语、葡萄牙语、 意大利语、荷兰语 东欧及中欧 俄语、捷克语、波兰语 中东 阿拉伯语、波斯语、希伯来语、土耳其语 东亚 日语、韩语 东南亚 越南语、泰语、印尼语、马来语、老挝语、缅甸语、宿务语、高棉语、菲律宾语 南亚 印地语、孟加拉语、乌尔都语 此外，我们针对性地优化了多语言场景中常见的语言转换（code switch）问题，模型当前发生语言转换的概率大幅度降低。我们使用容易触发语言转换现象的提示词进行测试，观察到Qwen2系列模型在此方面能力的显著提升。...</p></div><footer class=entry-footer><span title='2024-06-07 00:00:00 +0800 +0800'>2024年6月7日</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;1599 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 你好，Qwen2" href=http://qwenlm.github.io/zh/blog/qwen2/></a></article><article class=post-entry><header class=entry-header><h2>使用Qwen-Agent将上下文记忆扩展到百万量级</h2></header><div class=entry-content><p>我们开发了一个智能体用于理解包含百万字词的文档，虽然仅使用Qwen2模型的8k上下文，但效果超过RAG和长序列原生模型。我们还利用此智能体合成长上下文数据，用于训练长上下文的Qwen模型。</p></div><footer class=entry-footer><span title='2024-06-06 11:59:59 +0800 +0800'>2024年6月6日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;106 字&nbsp;·&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 使用Qwen-Agent将上下文记忆扩展到百万量级" href=http://qwenlm.github.io/zh/blog/qwen-agent-2405/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=http://qwenlm.github.io/zh/>«&nbsp;上一页&nbsp;</a>
<a class=next href=http://qwenlm.github.io/zh/page/3/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>