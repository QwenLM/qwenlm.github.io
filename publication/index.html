<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publication | Qwen</title><meta name=keywords content><meta name=description content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/publication/><link crossorigin=anonymous href=/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://qwenlm.github.io/publication/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Publication"><meta property="og:description" content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><meta property="og:type" content="article"><meta property="og:url" content="http://qwenlm.github.io/publication/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Publication"><meta name=twitter:description content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publication","item":"http://qwenlm.github.io/publication/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Publication","name":"Publication","description":"Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.\nQwen Team. (2023). Qwen Technical Report. arXiv.\nBai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. \u0026amp; Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026amp; Yang, H. (2022).","keywords":[],"articleBody":"Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.\nQwen Team. (2023). Qwen Technical Report. arXiv.\nBai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. \u0026 Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026 Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. ICML.\nBai, J., Men, R., Yang, H., Ren, X., Dang, K.E., Zhang, Y., Zhou, X., Wang, P., Tan, S., Yang, A., Cui, Z., Han, Y., Bai, S., Ge, W., Ma, J., Lin, J., Zhou, J., \u0026 Zhou, C. (2022). OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models. arXiv, abs/2212.04408.\nLin, J., Men, R., Yang, A., Zhou, C., Zhang, Y., Wang, P., Zhou, J., Tang, J., \u0026 Yang, H. (2021). M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining. KDD.\nYang, A., Pan, J., Lin, J., Men, R., Zhang, Y., Zhou, J., \u0026 Zhou, C. (2022). Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. arXiv, abs/2211.01335.\nMa, J., Bai, S., \u0026 Zhou, C. (2022). Pretrained Diffusion Models for Unified Human Motion Synthesis. arXiv, abs/2212.02837.\nYang, H., Lin, J., Yang, A., Wang, P., Zhou, C., \u0026 Yang, H. (2022). Prompt Tuning for Generative Multimodal Pretrained Models. arXiv, abs/2208.02532.\nZhou, X., Wang, J., Cui, Z., Zhang, S., Yan, Z., Zhou, J., \u0026 Zhou, C. (2022). MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition. arXiv, abs/2212.00500.\nHuang, Y., Lin, J., Zhou, C., Yang, H., \u0026 Huang, L. (2022). Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably). ICML.\nBai, S., Zhou, H., Li, Z., Zhou, C., \u0026 Yang, H. (2022). Single Stage Virtual Try-on via Deformable Attention Flows. ECCV.\nCui, Z., Ma, J., Zhou, C., Zhou, J., Yang, H. (2022). M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv, abs/2205.08084.\nZhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., \u0026 Yang, H. (2021). UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis. NeurIPS.\nLin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., Zhou, J., \u0026 Yang, H. (2021). M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. arXiv, abs/2110.03888.\nYang, A., Lin, J., Men, R., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Wang, J., Li, Y., Zhang, D., Lin, W., Lin, Q., Zhou, J., \u0026 Yang, H. (2021). M6-T: Exploring sparse expert models and beyond. arXiv, abs/2105.15082.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., \u0026 Tang, J. (2021). CogView: Mastering Text-to-Image Generation via Transformers. NeurIPS.\nRen, S., Lin, J., Zhao, G., Men, R., Yang, A., Zhou, J., Sun, X., \u0026 Yang, H. (2021). Learning Relation Alignment for Calibrated Cross-modal Retrieval. ACL-IJCNLP.\nWang, P., Lin, J., Yang, A., Zhou, C., Zhang, Y., Zhou, J., \u0026 Yang, H. (2021). Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation. Findings of ACL-IJCNLP.\nLin, J., Yang, A., Zhang, Y., Liu, J., Zhou, J., \u0026 Yang, H. (2020). InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining. arXiv, abs/2003.13198.\nZhang, Z., Zhou, C., Ma, J., Lin, Z., Zhou, J., Yang, H., \u0026 Zhao, Z. (2021). Learning to Rehearse in Long Sequence Memorization. ICML.\nZhou, C., Ma, J., Zhang, J., Zhou, J., \u0026 Yang, H. (2021). Contrastive learning for debiased candidate generation in large-scale recommender systems. KDD.\nMa, J., Zhou, C., Cui, P., Yang, H., \u0026 Zhu, W. (2019). Learning Disentangled Representations for Recommendation. NeurIPS.\nChen, Q., Lin, J., Zhang, Y., Ding, M., Cen, Y., Yang, H., \u0026 Tang, J. (2019). Towards Knowledge-Based Recommender Dialog System. EMNLP-IJCNLP.\nChen, Q., Lin, J., Zhang, Y., Yang, H., Zhou, J., \u0026 Tang, J. (2019). Towards Knowledge-Based Personalized Product Description Generation in E-commerce. KDD.\n","wordCount":"647","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://qwenlm.github.io/publication/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_ç¼ç»-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_å½¢ç¶"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span class=active>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#7085B6 0%,#87A7D9 50%,#DEF3F8 100%)"></div><div class="hero text-light"><h1 class=post-title>Publication</h1></div></div><main class=main><article class=post-single><div class=post-content><h1 id=selected-publications>Selected Publications<a hidden class=anchor aria-hidden=true href=#selected-publications>#</a></h1><ul><li><p>Qwen Team. (2024). <em>Qwen2 Techinical Report.</em> <strong>arXiv</strong>.</p></li><li><p>Qwen Team. (2023). <em>Qwen Technical Report.</em> <strong>arXiv</strong>.</p></li><li><p>Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). <em>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.</em> <strong>arXiv</strong>.</p></li><li><p>Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). <em>Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.</em> <strong>ICML</strong>.</p></li><li><p>Bai, J., Men, R., Yang, H., Ren, X., Dang, K.E., Zhang, Y., Zhou, X., Wang, P., Tan, S., Yang, A., Cui, Z., Han, Y., Bai, S., Ge, W., Ma, J., Lin, J., Zhou, J., & Zhou, C. (2022). <em>OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models.</em> arXiv, abs/2212.04408.</p></li><li><p>Lin, J., Men, R., Yang, A., Zhou, C., Zhang, Y., Wang, P., Zhou, J., Tang, J., & Yang, H. (2021). <em>M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining.</em> <strong>KDD</strong>.</p></li><li><p>Yang, A., Pan, J., Lin, J., Men, R., Zhang, Y., Zhou, J., & Zhou, C. (2022). <em>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese.</em> arXiv, abs/2211.01335.</p></li><li><p>Ma, J., Bai, S., & Zhou, C. (2022). <em>Pretrained Diffusion Models for Unified Human Motion Synthesis.</em> arXiv, abs/2212.02837.</p></li><li><p>Yang, H., Lin, J., Yang, A., Wang, P., Zhou, C., & Yang, H. (2022). <em>Prompt Tuning for Generative Multimodal Pretrained Models.</em> arXiv, abs/2208.02532.</p></li><li><p>Zhou, X., Wang, J., Cui, Z., Zhang, S., Yan, Z., Zhou, J., & Zhou, C. (2022). <em>MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition.</em> arXiv, abs/2212.00500.</p></li><li><p>Huang, Y., Lin, J., Zhou, C., Yang, H., & Huang, L. (2022). <em>Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably).</em> <strong>ICML</strong>.</p></li><li><p>Bai, S., Zhou, H., Li, Z., Zhou, C., & Yang, H. (2022). <em>Single Stage Virtual Try-on via Deformable Attention Flows</em>. <strong>ECCV</strong>.</p></li><li><p>Cui, Z., Ma, J., Zhou, C., Zhou, J., Yang, H. (2022). <em>M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</em>. arXiv, abs/2205.08084.</p></li><li><p>Zhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., & Yang, H. (2021). <em>UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis.</em> <strong>NeurIPS</strong>.</p></li><li><p>Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., Zhou, J., & Yang, H. (2021). <em>M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining.</em> arXiv, abs/2110.03888.</p></li><li><p>Yang, A., Lin, J., Men, R., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Wang, J., Li, Y., Zhang, D., Lin, W., Lin, Q., Zhou, J., & Yang, H. (2021). <em>M6-T: Exploring sparse expert models and beyond.</em> arXiv, abs/2105.15082.</p></li><li><p>Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). <em>CogView: Mastering Text-to-Image Generation via Transformers</em>. <strong>NeurIPS</strong>.</p></li><li><p>Ren, S., Lin, J., Zhao, G., Men, R., Yang, A., Zhou, J., Sun, X., & Yang, H. (2021). <em>Learning Relation Alignment for Calibrated Cross-modal Retrieval.</em> <strong>ACL-IJCNLP</strong>.</p></li><li><p>Wang, P., Lin, J., Yang, A., Zhou, C., Zhang, Y., Zhou, J., & Yang, H. (2021). <em>Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation.</em> <strong>Findings of ACL-IJCNLP</strong>.</p></li><li><p>Lin, J., Yang, A., Zhang, Y., Liu, J., Zhou, J., & Yang, H. (2020). <em>InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining.</em> arXiv, abs/2003.13198.</p></li><li><p>Zhang, Z., Zhou, C., Ma, J., Lin, Z., Zhou, J., Yang, H., & Zhao, Z. (2021). <em>Learning to Rehearse in Long Sequence Memorization.</em> <strong>ICML</strong>.</p></li><li><p>Zhou, C., Ma, J., Zhang, J., Zhou, J., & Yang, H. (2021). <em>Contrastive learning for debiased candidate generation in large-scale recommender systems.</em> <strong>KDD</strong>.</p></li><li><p>Ma, J., Zhou, C., Cui, P., Yang, H., & Zhu, W. (2019). <em>Learning Disentangled Representations for Recommendation.</em> <strong>NeurIPS</strong>.</p></li><li><p>Chen, Q., Lin, J., Zhang, Y., Ding, M., Cen, Y., Yang, H., & Tang, J. (2019). <em>Towards Knowledge-Based Recommender Dialog System.</em> <strong>EMNLP-IJCNLP</strong>.</p></li><li><p>Chen, Q., Lin, J., Zhang, Y., Yang, H., Zhou, J., & Tang, J. (2019). <em>Towards Knowledge-Based Personalized Product Description Generation in E-commerce.</em> <strong>KDD</strong>.</p></li></ul></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>