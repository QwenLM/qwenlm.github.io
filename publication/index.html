<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publication | Qwen</title><meta name=keywords content><meta name=description content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/publication/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/publication/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Publication"><meta property="og:description" content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/publication/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Publication"><meta name=twitter:description content="Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.
Qwen Team. (2023). Qwen Technical Report. arXiv.
Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publication","item":"https://qwenlm.github.io/publication/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Publication","name":"Publication","description":"Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.\nQwen Team. (2023). Qwen Technical Report. arXiv.\nBai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. \u0026amp; Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026amp; Yang, H. (2022).","keywords":[],"articleBody":"Selected Publications Qwen Team. (2024). Qwen2 Techinical Report. arXiv.\nQwen Team. (2023). Qwen Technical Report. arXiv.\nBai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. \u0026 Zhou., J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., \u0026 Yang, H. (2022). Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. ICML.\nBai, J., Men, R., Yang, H., Ren, X., Dang, K.E., Zhang, Y., Zhou, X., Wang, P., Tan, S., Yang, A., Cui, Z., Han, Y., Bai, S., Ge, W., Ma, J., Lin, J., Zhou, J., \u0026 Zhou, C. (2022). OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models. arXiv, abs/2212.04408.\nLin, J., Men, R., Yang, A., Zhou, C., Zhang, Y., Wang, P., Zhou, J., Tang, J., \u0026 Yang, H. (2021). M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining. KDD.\nYang, A., Pan, J., Lin, J., Men, R., Zhang, Y., Zhou, J., \u0026 Zhou, C. (2022). Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. arXiv, abs/2211.01335.\nMa, J., Bai, S., \u0026 Zhou, C. (2022). Pretrained Diffusion Models for Unified Human Motion Synthesis. arXiv, abs/2212.02837.\nYang, H., Lin, J., Yang, A., Wang, P., Zhou, C., \u0026 Yang, H. (2022). Prompt Tuning for Generative Multimodal Pretrained Models. arXiv, abs/2208.02532.\nZhou, X., Wang, J., Cui, Z., Zhang, S., Yan, Z., Zhou, J., \u0026 Zhou, C. (2022). MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition. arXiv, abs/2212.00500.\nHuang, Y., Lin, J., Zhou, C., Yang, H., \u0026 Huang, L. (2022). Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably). ICML.\nBai, S., Zhou, H., Li, Z., Zhou, C., \u0026 Yang, H. (2022). Single Stage Virtual Try-on via Deformable Attention Flows. ECCV.\nCui, Z., Ma, J., Zhou, C., Zhou, J., Yang, H. (2022). M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv, abs/2205.08084.\nZhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., \u0026 Yang, H. (2021). UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis. NeurIPS.\nLin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., Zhou, J., \u0026 Yang, H. (2021). M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. arXiv, abs/2110.03888.\nYang, A., Lin, J., Men, R., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Wang, J., Li, Y., Zhang, D., Lin, W., Lin, Q., Zhou, J., \u0026 Yang, H. (2021). M6-T: Exploring sparse expert models and beyond. arXiv, abs/2105.15082.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., \u0026 Tang, J. (2021). CogView: Mastering Text-to-Image Generation via Transformers. NeurIPS.\nRen, S., Lin, J., Zhao, G., Men, R., Yang, A., Zhou, J., Sun, X., \u0026 Yang, H. (2021). Learning Relation Alignment for Calibrated Cross-modal Retrieval. ACL-IJCNLP.\nWang, P., Lin, J., Yang, A., Zhou, C., Zhang, Y., Zhou, J., \u0026 Yang, H. (2021). Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation. Findings of ACL-IJCNLP.\nLin, J., Yang, A., Zhang, Y., Liu, J., Zhou, J., \u0026 Yang, H. (2020). InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining. arXiv, abs/2003.13198.\nZhang, Z., Zhou, C., Ma, J., Lin, Z., Zhou, J., Yang, H., \u0026 Zhao, Z. (2021). Learning to Rehearse in Long Sequence Memorization. ICML.\nZhou, C., Ma, J., Zhang, J., Zhou, J., \u0026 Yang, H. (2021). Contrastive learning for debiased candidate generation in large-scale recommender systems. KDD.\nMa, J., Zhou, C., Cui, P., Yang, H., \u0026 Zhu, W. (2019). Learning Disentangled Representations for Recommendation. NeurIPS.\nChen, Q., Lin, J., Zhang, Y., Ding, M., Cen, Y., Yang, H., \u0026 Tang, J. (2019). Towards Knowledge-Based Recommender Dialog System. EMNLP-IJCNLP.\nChen, Q., Lin, J., Zhang, Y., Yang, H., Zhou, J., \u0026 Tang, J. (2019). Towards Knowledge-Based Personalized Product Description Generation in E-commerce. KDD.\n","wordCount":"647","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/publication/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span class=active>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#7085B6 0%,#87A7D9 50%,#DEF3F8 100%)"></div><div class="hero text-light"><h1 class=post-title>Publication</h1></div></div><main class=main><article class=post-single><div class=post-content><h1 id=selected-publications>Selected Publications<a hidden class=anchor aria-hidden=true href=#selected-publications>#</a></h1><ul><li><p>Qwen Team. (2024). <em>Qwen2 Techinical Report.</em> <strong>arXiv</strong>.</p></li><li><p>Qwen Team. (2023). <em>Qwen Technical Report.</em> <strong>arXiv</strong>.</p></li><li><p>Bai, J., Bai S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C. & Zhou., J. (2023). <em>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.</em> <strong>arXiv</strong>.</p></li><li><p>Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). <em>Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.</em> <strong>ICML</strong>.</p></li><li><p>Bai, J., Men, R., Yang, H., Ren, X., Dang, K.E., Zhang, Y., Zhou, X., Wang, P., Tan, S., Yang, A., Cui, Z., Han, Y., Bai, S., Ge, W., Ma, J., Lin, J., Zhou, J., & Zhou, C. (2022). <em>OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models.</em> arXiv, abs/2212.04408.</p></li><li><p>Lin, J., Men, R., Yang, A., Zhou, C., Zhang, Y., Wang, P., Zhou, J., Tang, J., & Yang, H. (2021). <em>M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining.</em> <strong>KDD</strong>.</p></li><li><p>Yang, A., Pan, J., Lin, J., Men, R., Zhang, Y., Zhou, J., & Zhou, C. (2022). <em>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese.</em> arXiv, abs/2211.01335.</p></li><li><p>Ma, J., Bai, S., & Zhou, C. (2022). <em>Pretrained Diffusion Models for Unified Human Motion Synthesis.</em> arXiv, abs/2212.02837.</p></li><li><p>Yang, H., Lin, J., Yang, A., Wang, P., Zhou, C., & Yang, H. (2022). <em>Prompt Tuning for Generative Multimodal Pretrained Models.</em> arXiv, abs/2208.02532.</p></li><li><p>Zhou, X., Wang, J., Cui, Z., Zhang, S., Yan, Z., Zhou, J., & Zhou, C. (2022). <em>MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition.</em> arXiv, abs/2212.00500.</p></li><li><p>Huang, Y., Lin, J., Zhou, C., Yang, H., & Huang, L. (2022). <em>Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably).</em> <strong>ICML</strong>.</p></li><li><p>Bai, S., Zhou, H., Li, Z., Zhou, C., & Yang, H. (2022). <em>Single Stage Virtual Try-on via Deformable Attention Flows</em>. <strong>ECCV</strong>.</p></li><li><p>Cui, Z., Ma, J., Zhou, C., Zhou, J., Yang, H. (2022). <em>M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</em>. arXiv, abs/2205.08084.</p></li><li><p>Zhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., & Yang, H. (2021). <em>UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis.</em> <strong>NeurIPS</strong>.</p></li><li><p>Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., Zhou, J., & Yang, H. (2021). <em>M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining.</em> arXiv, abs/2110.03888.</p></li><li><p>Yang, A., Lin, J., Men, R., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Wang, J., Li, Y., Zhang, D., Lin, W., Lin, Q., Zhou, J., & Yang, H. (2021). <em>M6-T: Exploring sparse expert models and beyond.</em> arXiv, abs/2105.15082.</p></li><li><p>Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). <em>CogView: Mastering Text-to-Image Generation via Transformers</em>. <strong>NeurIPS</strong>.</p></li><li><p>Ren, S., Lin, J., Zhao, G., Men, R., Yang, A., Zhou, J., Sun, X., & Yang, H. (2021). <em>Learning Relation Alignment for Calibrated Cross-modal Retrieval.</em> <strong>ACL-IJCNLP</strong>.</p></li><li><p>Wang, P., Lin, J., Yang, A., Zhou, C., Zhang, Y., Zhou, J., & Yang, H. (2021). <em>Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation.</em> <strong>Findings of ACL-IJCNLP</strong>.</p></li><li><p>Lin, J., Yang, A., Zhang, Y., Liu, J., Zhou, J., & Yang, H. (2020). <em>InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining.</em> arXiv, abs/2003.13198.</p></li><li><p>Zhang, Z., Zhou, C., Ma, J., Lin, Z., Zhou, J., Yang, H., & Zhao, Z. (2021). <em>Learning to Rehearse in Long Sequence Memorization.</em> <strong>ICML</strong>.</p></li><li><p>Zhou, C., Ma, J., Zhang, J., Zhou, J., & Yang, H. (2021). <em>Contrastive learning for debiased candidate generation in large-scale recommender systems.</em> <strong>KDD</strong>.</p></li><li><p>Ma, J., Zhou, C., Cui, P., Yang, H., & Zhu, W. (2019). <em>Learning Disentangled Representations for Recommendation.</em> <strong>NeurIPS</strong>.</p></li><li><p>Chen, Q., Lin, J., Zhang, Y., Ding, M., Cen, Y., Yang, H., & Tang, J. (2019). <em>Towards Knowledge-Based Recommender Dialog System.</em> <strong>EMNLP-IJCNLP</strong>.</p></li><li><p>Chen, Q., Lin, J., Zhang, Y., Yang, H., Zhou, J., & Tang, J. (2019). <em>Towards Knowledge-Based Personalized Product Description Generation in E-commerce.</em> <strong>KDD</strong>.</p></li></ul></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>