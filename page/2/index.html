<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.106.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen</title><meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Qwickly forging AGI, enhancing intelligence."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/json href=https://qwenlm.github.io/index.json><link rel=alternate hreflang=en href=https://qwenlm.github.io/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen"><meta property="og:description" content="Qwickly forging AGI, enhancing intelligence."><meta property="og:type" content="website"><meta property="og:url" content="https://qwenlm.github.io/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen"><meta name=twitter:description content="Qwickly forging AGI, enhancing intelligence."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Qwen","url":"https://qwenlm.github.io/","description":"Qwen","thumbnailUrl":"https://qwenlm.github.io/favicon.png","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><img class=hero-background style=opacity:0 onload="this.style.opacity=1" src=https://cdn.jsdelivr.net/gh/qwenlm/qwenlm.github.io@master/static/img/background.webp width=100%><div class=hero-gradient></div><div class=hero-blur></div><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-light text-fade-in"><div class=hero-header><svg width="280" height="100" style="filter:drop-shadow(0 0 32px #4f2dda)" viewBox="0 0 66.067 20.526" xmlns="http://www.w3.org/2000/svg"><path d="m8.7933 13.184h3.3982l1.4263 1.6743q1.3146-1.0914 2.0216-2.6417.71934-1.5503.71934-3.3238.0-2.8401-1.7487-4.5765-1.7363-1.7487-4.6013-1.7487-1.4387.0-2.6913.5209-1.2526.5085-2.2324 1.5131-1.0914 1.1286-1.6743 2.5549-.58291 1.4263-.58291 2.9766.0 2.8029 1.7115 4.5889 1.7115 1.7859 4.3532 1.7859.5457.0 1.1286-.08682.59531-.08682 1.2402-.27285zm6.2012 7.3422-1.9844-2.2944q-1.0542.40928-2.034.60772-.97978.21084-1.9348.21084-4.1548.0-6.598-2.3812t-2.4433-6.4368q0-2.2076.78135-4.13.78135-1.9348 2.2448-3.3362 1.4139-1.3519 3.237-2.0588 1.8355-.70693 3.9315-.70693 4.0804.0 6.536 2.4061 2.4557 2.4061 2.4557 6.412.0 2.4681-.99219 4.5765t-2.8277 3.5223l3.0262 3.6091zm5.3578-13.692h2.6913l1.91 6.0027q.0124.03721.04961.14883.34726 1.1038.37207 1.6371.13643-.42168.34727-.89297.22324-.48369.5457-1.0294l3.9439-6.7345 1.9224 6.9329q.11162.38447.18604.79375.07441.40928.12402.94258.18604-.5209.37207-.94258.19844-.42168.38447-.73174l3.5595-6.1268h3.0262l-7.9747 12.489-1.9844-6.5732q-.11162-.34727-.19844-.74414-.07441-.40928-.13643-.90537-.27285.57051-.48369.99219t-.32246.59531l-3.9936 6.6353zm22.758 4.4648h6.0647q-.03721-1.1906-.74414-1.8728-.69453-.69453-1.8728-.69453-1.3395.0-2.2696.69453t-1.1782 1.8728zm5.9159 3.6835 1.9224 1.5007q-1.0294 1.3519-2.2324 1.972-1.203.62012-2.7657.62012-2.6169.0-4.2292-1.5751-1.6123-1.5875-1.6123-4.1672.0-3.0262 1.8479-4.9609 1.8604-1.9472 4.7253-1.9472 2.3937.0 3.8075 1.4883 1.4263 1.4759 1.4263 3.9688.0.21084-.0248.5457-.0124.32246-.04961.78135h-8.9669q0 1.5999.80615 2.5549.80615.94258 2.1456.94258.93018.0 1.7611-.44648.84336-.45889 1.4387-1.2774zm13.357 3.6091.89297-6.7593q.0248-.18604.03721-.38447.0124-.21084.0124-.60772.0-1.1534-.5581-1.7611-.55811-.62012-1.6247-.62012-1.6619.0-2.6045 1.079-.94258 1.0666-1.2278 3.2866l-.74414 5.7671h-2.6665l1.5503-11.757h2.5673l-.19844 1.4139q1.017-.93018 2.096-1.3767 1.0914-.44648 2.3192-.44648 1.8107.0 2.8153.95498 1.017.94258 1.017 2.6541.0.43408-.04961 1.017-.04961.57051-.14883 1.3395l-.81855 6.2012z" fill="#fff" stroke-linejoin="round" stroke-opacity=".7352" stroke-width="1.6"/></svg></div><div class=hero-content>Qwickly forging AGI, enhancing intelligence.</div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Qwen2.5-LLM: Extending the boundary of LLMs</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2....</p></div><footer class=entry-footer><span title='2024-09-19 00:00:03 +0800 +0800'>September 19, 2024</span>&nbsp;¬∑&nbsp;13 min&nbsp;¬∑&nbsp;2680 words&nbsp;¬∑&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-LLM: Extending the boundary of LLMs" href=https://qwenlm.github.io/blog/qwen2.5-llm/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Coder: Code More, Learn More!</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In early April, we introduced CodeQwen1.5, which garnered significant attention from the community. Since then, we have been working to enhance the coding model. Today, we are excited to announce the release of the next generation of open-source coding models, Qwen2.5-Coder, and officially rename CodeQwen to Qwen-Coder. We think ‚ÄúCoder‚Äù is more human-like and agile, reflecting our vision of it becoming a true coding partner in the future....</p></div><footer class=entry-footer><span title='2024-09-19 00:00:02 +0800 +0800'>September 19, 2024</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;666 words&nbsp;¬∑&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Coder: Code More, Learn More!" href=https://qwenlm.github.io/blog/qwen2.5-coder/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Math: The world's leading open-sourced mathematical LLMs</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DISCORD
üö® Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks. Introduction A month ago, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. Today, we have upgraded it and open-sourced Qwen2.5-Math series, including base models Qwen2.5-Math-1.5B/7B/72B, instruction-tuned models Qwen2.5-Math-1.5B/7B/72B-Instruct, and mathematical reward model Qwen2....</p></div><footer class=entry-footer><span title='2024-09-19 00:00:01 +0800 +0800'>September 19, 2024</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1258 words&nbsp;¬∑&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Math: The world's leading open-sourced mathematical LLMs" href=https://qwenlm.github.io/blog/qwen2.5-math/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-VL: To See the World More Clearly</h2></header><div class=entry-content><p>DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD
After a year‚Äôs relentless efforts, today we are thrilled to release Qwen2-VL! Qwen2-VL is the latest version of the vision language models based on Qwen2 in the Qwen model familities. Compared with Qwen-VL, Qwen2-VL has the capabilities of:
SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.
Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc....</p></div><footer class=entry-footer><span title='2024-08-29 00:24:00 +0800 +0800'>August 29, 2024</span>&nbsp;¬∑&nbsp;17 min&nbsp;¬∑&nbsp;3569 words&nbsp;¬∑&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-VL: To See the World More Clearly" href=https://qwenlm.github.io/blog/qwen2-vl/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2-Audio: Chat with Your Voice!</h2></header><div class=entry-content><p>DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs....</p></div><footer class=entry-footer><span title='2024-08-09 16:18:19 +0800 +0800'>August 9, 2024</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;1999 words&nbsp;¬∑&nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2-Audio: Chat with Your Voice!" href=https://qwenlm.github.io/blog/qwen2-audio/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://qwenlm.github.io/>¬´&nbsp;Prev&nbsp;</a>
<a class=next href=https://qwenlm.github.io/page/3/>Next&nbsp;&nbsp;¬ª</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>