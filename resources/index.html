<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>API | Qwen</title><meta name=keywords content><meta name=description content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
>>> from transformers import AutoModelForCausalLM, AutoTokenizer >>> device = &#34;cuda&#34; # the device to load the model onto >>> model = AutoModelForCausalLM.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;, device_map=&#34;auto&#34;) >>> tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;) >>> prompt = &#34;Give me a short introduction to large language model."><meta name=author content="Qwen"><link rel=canonical href=http://qwenlm.github.io/resources/><link crossorigin=anonymous href=/assets/css/stylesheet.f5d76626f153544f6e9bd68b39eb314ecb1f44bf7b35a3005e09280aa212498c.css integrity="sha256-9ddmJvFTVE9um9aLOesxTssfRL97NaMAXgkoCqISSYw=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://qwenlm.github.io/resources/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.53ab4bb7e1c22dec792a170c5772f789233ba13d530a9bf47d9d2424b4f9a861.js integrity="sha256-U6tLt+HCLex5KhcMV3L3iSM7oT1TCpv0fZ0kJLT5qGE="></script><meta property="og:title" content="API"><meta property="og:description" content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
>>> from transformers import AutoModelForCausalLM, AutoTokenizer >>> device = &#34;cuda&#34; # the device to load the model onto >>> model = AutoModelForCausalLM.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;, device_map=&#34;auto&#34;) >>> tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;) >>> prompt = &#34;Give me a short introduction to large language model."><meta property="og:type" content="article"><meta property="og:url" content="http://qwenlm.github.io/resources/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="API"><meta name=twitter:description content="Links In the following, we provide important links for you to refer to our opensource resources.
GITHUB HUGGING FACE MODELSCOPE API
Quick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:
>>> from transformers import AutoModelForCausalLM, AutoTokenizer >>> device = &#34;cuda&#34; # the device to load the model onto >>> model = AutoModelForCausalLM.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;, device_map=&#34;auto&#34;) >>> tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen2/Qwen2-7B-Chat-beta&#34;) >>> prompt = &#34;Give me a short introduction to large language model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"API","item":"http://qwenlm.github.io/resources/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"API","name":"API","description":"Links In the following, we provide important links for you to refer to our opensource resources.\nGITHUB HUGGING FACE MODELSCOPE API\nQuick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:\n\u0026gt;\u0026gt;\u0026gt; from transformers import AutoModelForCausalLM, AutoTokenizer \u0026gt;\u0026gt;\u0026gt; device = \u0026#34;cuda\u0026#34; # the device to load the model onto \u0026gt;\u0026gt;\u0026gt; model = AutoModelForCausalLM.from_pretrained(\u0026#34;Qwen2/Qwen2-7B-Chat-beta\u0026#34;, device_map=\u0026#34;auto\u0026#34;) \u0026gt;\u0026gt;\u0026gt; tokenizer = AutoTokenizer.from_pretrained(\u0026#34;Qwen2/Qwen2-7B-Chat-beta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; prompt = \u0026#34;Give me a short introduction to large language model.","keywords":[],"articleBody":"Links In the following, we provide important links for you to refer to our opensource resources.\nGITHUB HUGGING FACE MODELSCOPE API\nQuick Start It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:\n\u003e\u003e\u003e from transformers import AutoModelForCausalLM, AutoTokenizer \u003e\u003e\u003e device = \"cuda\" # the device to load the model onto \u003e\u003e\u003e model = AutoModelForCausalLM.from_pretrained(\"Qwen2/Qwen2-7B-Chat-beta\", device_map=\"auto\") \u003e\u003e\u003e tokenizer = AutoTokenizer.from_pretrained(\"Qwen2/Qwen2-7B-Chat-beta\") \u003e\u003e\u003e prompt = \"Give me a short introduction to large language model.\" \u003e\u003e\u003e messages = [{\"role\": \"user\", \"content\": prompt}] \u003e\u003e\u003e text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) \u003e\u003e\u003e model_inputs = tokenizer([text], return_tensors=\"pt\").to(device) \u003e\u003e\u003e generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True) \u003e\u003e\u003e generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)] \u003e\u003e\u003e response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] ","wordCount":"120","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Qwen"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://qwenlm.github.io/resources/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="OFA-Sys (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span class=active>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(to top,#a18cd1 0%,#fbc2eb 100%)"></div><div class="hero text-light"><h1 class=post-title>API</h1></div></div><main class=main><article class=post-single><div class=post-content><h2 id=links>Links<a hidden class=anchor aria-hidden=true href=#links>#</a></h2><p>In the following, we provide important links for you to refer to our opensource resources.</p><p><a href=https://github.com/QwenLM/Qwen class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://dashscope.aliyun.com/ class="btn external" target=_blank>API</a></p><h2 id=quick-start>Quick Start<a hidden class=anchor aria-hidden=true href=#quick-start>#</a></h2><p>It is simple to use Qwen through Hugging Face Transformers. Below is a demo usage for a quick start:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=c1># the device to load the model onto</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen2/Qwen2-7B-Chat-beta&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen2/Qwen2-7B-Chat-beta&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language model.&#34;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>messages</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span><span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>