<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=&#34;https://qwen.ai/research&#34;"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen3Guard: Real-time Safety for Your Token Stream | Qwen</title><meta name=keywords content><meta name=description content="Tech Report GitHub Hugging Face ModelScope DISCORD
Introduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.
Qwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen3guard/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen3guard/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen3guard/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen3Guard: Real-time Safety for Your Token Stream"><meta property="og:description" content="Tech Report GitHub Hugging Face ModelScope DISCORD
Introduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.
Qwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen3guard/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-09-23T04:00:00+08:00"><meta property="article:modified_time" content="2025-09-23T04:00:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen3Guard: Real-time Safety for Your Token Stream"><meta name=twitter:description content="Tech Report GitHub Hugging Face ModelScope DISCORD
Introduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.
Qwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen3Guard: Real-time Safety for Your Token Stream","item":"https://qwenlm.github.io/blog/qwen3guard/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen3Guard: Real-time Safety for Your Token Stream","name":"Qwen3Guard: Real-time Safety for Your Token Stream","description":"Tech Report GitHub Hugging Face ModelScope DISCORD\nIntroduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.\nQwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments.","keywords":[],"articleBody":" Tech Report GitHub Hugging Face ModelScope DISCORD\nIntroduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.\nQwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments.\nQwen3Guard is available in two specialized variants:\nQwen3Guard-Gen, a generative model that accepts full user prompts and model responses to perform safety classification. Ideal for offline safety annotation and filtering of datasets, or for supplying safety-based rewards in reinforcement learning. Qwen3Guard-Stream, which marks a significant departure from previously open-sourced guard models by enabling efficient, real-time streaming safety detection during response generation. Both variants come in three sizes, 0.6B, 4B, and 8B parameters, to suit a wide range of deployment scenarios and resource constraints.\nYou can download the open-source models from Hugging Face or ModelScope. You can also access the Alibaba Cloud AI Guardrails service, powered by Qwen3Guard technology.\nKey Features Real-Time Streaming Detection Qwen3Guard-Stream is engineered for low latency, on the fly moderation during token generation, ensuring safety without sacrificing responsiveness. This is accomplished by attaching two lightweight classification heads to the transformer’s final layer, allowing the model to receive the response in a streaming fashion — token by token, as it is being generated — and output safety classifications instantly at each step.\nThree-Tier Severity Classification Beyond the conventional Safe and Unsafe labels, we introduce an additional Controversial label to enable flexible safety policies tailored to diverse use cases. Specifically, depending on the application scenario, Controversial instances can be dynamically reclassified as either Safe or Unsafe, allowing users to adjust classification strictness on demand.\nAs demonstrated in the evaluation below, existing guardrail models, constrained by binary labeling, struggle to adapt simultaneously to differing dataset standards. In contrast, Qwen3Guard achieves robust and consistent performance across both datasets by flexibly switching between strict and loose classification modes, thanks to the three-tier severity design.\nMultilingual Support Qwen3Guard supports 119 languages and dialects, making it suitable for global deployments and cross-linguistic applications with consistent, high quality safety performance.\nLanguage Family Languages \u0026 Dialects Indo-European English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian Sino-Tibetan Chinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese Afro-Asiatic Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian), Hebrew, Maltese Austronesian Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines) Dravidian Tamil, Telugu, Kannada, Malayalam Turkic Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar Tai-Kadai Thai, Lao Uralic Finnish, Estonian, Hungarian Austroasiatic Vietnamese, Khmer Other Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili More Applications We also demonstrate (1) using Qwen3Guard-Gen for safety RL to enhance model safety while preserving overall output helpfulness, and (2) using Qwen3Guard-Stream to enable real-time, on-the-fly intervention that ensures safe outputs without requiring model re-training. See details in our Technical Report.\nDevelop with Qwen3Guard Qwen3Guard-Gen Qwen3Guard-Gen operates similarly to a large language model, with its chat template specifically optimized for safety classification tasks. Outputs are generated in a predefined, structured format.\nYou can use the following codes to moderate user prompt or model responses.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer import re model_name = \"Qwen/Qwen3Guard-Gen-4B\" # load the tokenizer and the model tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) def extract_label_and_categories(content): safe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\" category_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide \u0026 Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)\" safe_label_match = re.search(safe_pattern, content) label = safe_label_match.group(1) if safe_label_match else None categories = re.findall(category_pattern, content) return label, categories def extract_label_categories_refusal(content): safe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\" category_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide \u0026 Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)\" refusal_pattern = r\"Refusal: (Yes|No)\" safe_label_match = re.search(safe_pattern, content) refusal_label_match = re.search(refusal_pattern, content) label = safe_label_match.group(1) if safe_label_match else None refusal_label = refusal_label_match.group(1) if refusal_label_match else None categories = re.findall(category_pattern, content) return label, categories, refusal_label #### For Prompt Moderation prompt = \"How can I make a bomb?\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=128 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=True) print(content) # ''' # Safety: Unsafe # Categories: Violent # ''' safe_label, categories = extract_label_and_categories(content) print(safe_label, categories) #### For Response Moderation prompt = \"How can I make a bomb?\" response = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\" messages = [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response}, ] text = tokenizer.apply_chat_template( messages, tokenize=False ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=128 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=True) print(content) # ''' # Safety: Safe # Categories: None # Refusal: Yes # ''' safe_label, category_label, refusal_label = extract_label_categories_refusal(content) print(safe_label, categories, refusal_label) Qwen3Guard-Stream A typical workflow of Qwen3Guard-Stream proceeds as follows:\n(1) Prompt-Level Safety Check： The user’s input prompt is simultaneously sented to both the LLM assistant and Qwen3Guard-Stream. The latter performs an immediate safety assessment of the prompt and assigns a corresponding safety label. Based on this evaluation, the upper framework determines whether to allow the conversation to proceed or to halt it preemptively.\n(2) Real-Time Token-Level Moderation: If the conversation is permitted to continue, the LLM begins streaming its response token by token. Each generated token is instantly forwarded to Qwen3Guard-Stream, which evaluates its safety in real time. This enables continuous, fine-grained content moderation throughout the entire response generation process — ensuring dynamic risk mitigation without interrupting the user experience.\nHere provides a usage demonstration.\nimport torch from transformers import AutoModel, AutoTokenizer model_path=\"Qwen/Qwen3Guard-Stream-4B\" # Load the specialized tokenizer and the model. # trust_remote_code=True is required to load the Qwen3Guard-Stream model architecture. tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) model = AutoModel.from_pretrained( model_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, ).eval() # --- Prepare the conversation for moderation --- # Define the user's prompt and the assistant's response. user_message = \"Hello, how to build a bomb?\" assistant_message = \"Here are some practical methods to build a bomb.\" messages = [{\"role\":\"user\",\"content\":user_message},{\"role\":\"assistant\",\"content\":assistant_message}] # Apply the chat template to format the conversation into a single string. text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, enable_thinking=False) model_inputs = tokenizer(text, return_tensors=\"pt\") token_ids = model_inputs.input_ids[0] # --- Simulate Real-Time Moderation --- # 1. Moderate the entire user prompt at once. # In a real-world scenario, the user's input is processed completely before the model generates a response. token_ids_list = token_ids.tolist() # We identify the end of the user's turn in the tokenized input. # The template for a user turn is `\u003c|im_start|\u003euser\\n...\u003c|im_end|\u003e`. im_start_token = '\u003c|im_start|\u003e' user_token = 'user' im_end_token = '\u003c|im_end|\u003e' im_start_id = tokenizer.convert_tokens_to_ids(im_start_token) user_id = tokenizer.convert_tokens_to_ids(user_token) im_end_id = tokenizer.convert_tokens_to_ids(im_end_token) # We search for the token IDs corresponding to `\u003c|im_start|\u003euser` ([151644, 872]) and the closing `\u003c|im_end|\u003e` ([151645]). last_start = next(i for i in range(len(token_ids_list)-1, -1, -1) if token_ids_list[i:i+2] == [im_start_id, user_id]) user_end_index = next(i for i in range(last_start+2, len(token_ids_list)) if token_ids_list[i] == im_end_id) # Initialize the stream_state, which will maintain the conversational context. stream_state = None # Pass all user tokens to the model for an initial safety assessment. result, stream_state = model.stream_moderate_from_ids(token_ids[:user_end_index+1], role=\"user\", stream_state=None) if result['risk_level'][-1] == \"Safe\": print(f\"User moderation: -\u003e [Risk: {result['risk_level'][-1]}]\") else: print(f\"User moderation: -\u003e [Risk: {result['risk_level'][-1]} - Category: {result['category'][-1]}]\") # 2. Moderate the assistant's response token-by-token to simulate streaming. # This loop mimics how an LLM generates a response one token at a time. print(\"Assistant streaming moderation:\") for i in range(user_end_index + 1, len(token_ids)): # Get the current token ID for the assistant's response. current_token = token_ids[i] # Call the moderation function for the single new token. # The stream_state is passed and updated in each call to maintain context. result, stream_state = model.stream_moderate_from_ids(current_token, role=\"assistant\", stream_state=stream_state) token_str = tokenizer.decode([current_token]) # Print the generated token and its real-time safety assessment. if result['risk_level'][-1] == \"Safe\": print(f\"Token: {repr(token_str)} -\u003e [Risk: {result['risk_level'][-1]}]\") else: print(f\"Token: {repr(token_str)} -\u003e [Risk: {result['risk_level'][-1]} - Category: {result['category'][-1]}]\") model.close_stream(stream_state) For more usage examples, please visit our GitHub repository.\nFuture Work AI safety remains an ongoing challenge. With Qwen3Guard, we take our one step forward. We will continue advancing more flexible, efficient, and robust safety methods, including improving intrinsic model safety through architectural and training innovations, and developing dynamic, inference-time interventions. Our goal is to build AI systems that are not only technically capable but also aligned with human values and societal norms, ensuring responsible global deployment.\n","wordCount":"1470","inLanguage":"en","datePublished":"2025-09-23T04:00:00+08:00","dateModified":"2025-09-23T04:00:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen3guard/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog at <a href=https://qwen.ai/research>qwen.ai</a>!</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/research"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen3Guard: Real-time Safety for Your Token Stream</h1><div class=post-meta><span title='2025-09-23 04:00:00 +0800 +0800'>September 23, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1470 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen3guard/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/banner.png alt="Qwen3 Main Image" width=100%></figure><p><a href=https://github.com/QwenLM/Qwen3Guard/blob/main/Qwen3Guard_Technical_Report.pdf class="btn external" target=_blank>Tech Report</a>
<a href=https://github.com/QwenLM/Qwen3Guard class="btn external" target=_blank>GitHub</a>
<a href=https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1 class="btn external" target=_blank>Hugging Face</a>
<a href=https://modelscope.cn/collections/Qwen3Guard-308c39ef5ffb4b class="btn external" target=_blank>ModelScope</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.</p><p>Qwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/performance.png width=100%></figure><p>Qwen3Guard is available in two specialized variants:</p><ul><li><strong>Qwen3Guard-Gen</strong>, a generative model that accepts full user prompts and model responses to perform safety classification. Ideal for offline safety annotation and filtering of datasets, or for supplying safety-based rewards in reinforcement learning.</li><li><strong>Qwen3Guard-Stream</strong>, which marks a significant departure from previously open-sourced guard models by enabling efficient, real-time streaming safety detection during response generation.</li></ul><p>Both variants come in three sizes, 0.6B, 4B, and 8B parameters, to suit a wide range of deployment scenarios and resource constraints.</p><p>You can download the open-source models from <a href=https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1>Hugging Face</a> or <a href=https://modelscope.cn/collections/Qwen3Guard-308c39ef5ffb4b>ModelScope</a>. You can also access the <a href=https://www.alibabacloud.com/en/product/ai_guardrails>Alibaba Cloud AI Guardrails service</a>, powered by Qwen3Guard technology.</p><p><br><br></p><h2 id=key-features>Key Features<a hidden class=anchor aria-hidden=true href=#key-features>#</a></h2><h3 id=real-time-streaming-detection>Real-Time Streaming Detection<a hidden class=anchor aria-hidden=true href=#real-time-streaming-detection>#</a></h3><p>Qwen3Guard-Stream is engineered for low latency, on the fly moderation during token generation, ensuring safety without sacrificing responsiveness. This is accomplished by attaching two lightweight classification heads to the transformer&rsquo;s final layer, allowing the model to receive the response in a streaming fashion — token by token, as it is being generated — and output safety classifications instantly at each step.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/stream.png width=100%></figure><h3 id=three-tier-severity-classification>Three-Tier Severity Classification<a hidden class=anchor aria-hidden=true href=#three-tier-severity-classification>#</a></h3><p>Beyond the conventional Safe and Unsafe labels, we introduce an additional <strong>Controversial</strong> label to enable flexible safety policies tailored to diverse use cases. Specifically, depending on the application scenario, Controversial instances can be dynamically reclassified as either Safe or Unsafe, allowing users to adjust classification strictness on demand.</p><p>As demonstrated in the evaluation below, existing guardrail models, constrained by binary labeling, struggle to adapt simultaneously to differing dataset standards. In contrast, Qwen3Guard achieves robust and consistent performance across both datasets by flexibly switching between strict and loose classification modes, thanks to the three-tier severity design.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3Guard/Precision_Recall.png width=100%></figure><h3 id=multilingual-support>Multilingual Support<a hidden class=anchor aria-hidden=true href=#multilingual-support>#</a></h3><p>Qwen3Guard supports <strong>119 languages and dialects</strong>, making it suitable for global deployments and cross-linguistic applications with consistent, high quality safety performance.</p><table><thead><tr><th>Language Family</th><th>Languages & Dialects</th></tr></thead><tbody><tr><td>Indo-European</td><td>English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian</td></tr><tr><td>Sino-Tibetan</td><td>Chinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese</td></tr><tr><td>Afro-Asiatic</td><td>Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta&rsquo;izzi-Adeni, Tunisian), Hebrew, Maltese</td></tr><tr><td>Austronesian</td><td>Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines)</td></tr><tr><td>Dravidian</td><td>Tamil, Telugu, Kannada, Malayalam</td></tr><tr><td>Turkic</td><td>Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar</td></tr><tr><td>Tai-Kadai</td><td>Thai, Lao</td></tr><tr><td>Uralic</td><td>Finnish, Estonian, Hungarian</td></tr><tr><td>Austroasiatic</td><td>Vietnamese, Khmer</td></tr><tr><td>Other</td><td>Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili</td></tr></tbody></table><h3 id=more-applications>More Applications<a hidden class=anchor aria-hidden=true href=#more-applications>#</a></h3><p>We also demonstrate (1) using Qwen3Guard-Gen for safety RL to enhance model safety while preserving overall output helpfulness, and (2) using Qwen3Guard-Stream to enable real-time, on-the-fly intervention that ensures safe outputs without requiring model re-training. See details in our <a href=https://github.com/QwenLM/Qwen3Guard/blob/main/Qwen3Guard_Technical_Report.pdf>Technical Report</a>.</p><h2 id=develop-with-qwen3guard>Develop with Qwen3Guard<a hidden class=anchor aria-hidden=true href=#develop-with-qwen3guard>#</a></h2><h3 id=qwen3guard-gen>Qwen3Guard-Gen<a hidden class=anchor aria-hidden=true href=#qwen3guard-gen>#</a></h3><p>Qwen3Guard-Gen operates similarly to a large language model, with its chat template specifically optimized for safety classification tasks. Outputs are generated in a predefined, structured format.</p><p>You can use the following codes to moderate user prompt or model responses.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;Qwen/Qwen3Guard-Gen-4B&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># load the tokenizer and the model</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>extract_label_and_categories</span><span class=p>(</span><span class=n>content</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Safety: (Safe|Unsafe|Controversial)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>category_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide &amp; Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>safe_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span> <span class=o>=</span> <span class=n>safe_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>safe_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>categories</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>findall</span><span class=p>(</span><span class=n>category_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>label</span><span class=p>,</span> <span class=n>categories</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>extract_label_categories_refusal</span><span class=p>(</span><span class=n>content</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Safety: (Safe|Unsafe|Controversial)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>category_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide &amp; Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_pattern</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;Refusal: (Yes|No)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>safe_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>safe_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_label_match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>refusal_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span> <span class=o>=</span> <span class=n>safe_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>safe_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>refusal_label</span> <span class=o>=</span> <span class=n>refusal_label_match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>refusal_label_match</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>categories</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>findall</span><span class=p>(</span><span class=n>category_pattern</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>label</span><span class=p>,</span> <span class=n>categories</span><span class=p>,</span> <span class=n>refusal_label</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#### For Prompt Moderation</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;How can I make a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># conduct text completion</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_ids</span> <span class=o>=</span> <span class=n>generated_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=nb>len</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]):]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>content</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Safety: Unsafe</span>
</span></span><span class=line><span class=cl><span class=c1># Categories: Violent</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span> <span class=o>=</span> <span class=n>extract_label_and_categories</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#### For Response Moderation</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;How can I make a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=s2>&#34;As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>response</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># conduct text completion</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_ids</span> <span class=o>=</span> <span class=n>generated_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=nb>len</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]):]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>content</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Safety: Safe</span>
</span></span><span class=line><span class=cl><span class=c1># Categories: None</span>
</span></span><span class=line><span class=cl><span class=c1># Refusal: Yes</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>safe_label</span><span class=p>,</span> <span class=n>category_label</span><span class=p>,</span> <span class=n>refusal_label</span> <span class=o>=</span> <span class=n>extract_label_categories_refusal</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>safe_label</span><span class=p>,</span> <span class=n>categories</span><span class=p>,</span> <span class=n>refusal_label</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=qwen3guard-stream>Qwen3Guard-Stream<a hidden class=anchor aria-hidden=true href=#qwen3guard-stream>#</a></h3><p>A typical workflow of Qwen3Guard-Stream proceeds as follows:</p><p><strong>(1) Prompt-Level Safety Check</strong>： The user’s input prompt is simultaneously sented to both the LLM assistant and Qwen3Guard-Stream. The latter performs an immediate safety assessment of the prompt and assigns a corresponding safety label. Based on this evaluation, the upper framework determines whether to allow the conversation to proceed or to halt it preemptively.</p><p><strong>(2) Real-Time Token-Level Moderation</strong>: If the conversation is permitted to continue, the LLM begins streaming its response token by token. Each generated token is instantly forwarded to Qwen3Guard-Stream, which evaluates its safety in real time. This enables continuous, fine-grained content moderation throughout the entire response generation process — ensuring dynamic risk mitigation without interrupting the user experience.</p><p>Here provides a usage demonstration.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModel</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen3Guard-Stream-4B&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Load the specialized tokenizer and the model.</span>
</span></span><span class=line><span class=cl><span class=c1># trust_remote_code=True is required to load the Qwen3Guard-Stream model architecture.</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_path</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># --- Prepare the conversation for moderation ---</span>
</span></span><span class=line><span class=cl><span class=c1># Define the user&#39;s prompt and the assistant&#39;s response.</span>
</span></span><span class=line><span class=cl><span class=n>user_message</span> <span class=o>=</span> <span class=s2>&#34;Hello, how to build a bomb?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>assistant_message</span> <span class=o>=</span> <span class=s2>&#34;Here are some practical methods to build a bomb.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span><span class=s2>&#34;user&#34;</span><span class=p>,</span><span class=s2>&#34;content&#34;</span><span class=p>:</span><span class=n>user_message</span><span class=p>},{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span><span class=s2>&#34;assistant&#34;</span><span class=p>,</span><span class=s2>&#34;content&#34;</span><span class=p>:</span><span class=n>assistant_message</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply the chat template to format the conversation into a single string.</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>enable_thinking</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>token_ids</span> <span class=o>=</span> <span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- Simulate Real-Time Moderation ---</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. Moderate the entire user prompt at once.</span>
</span></span><span class=line><span class=cl><span class=c1># In a real-world scenario, the user&#39;s input is processed completely before the model generates a response.</span>
</span></span><span class=line><span class=cl><span class=n>token_ids_list</span> <span class=o>=</span> <span class=n>token_ids</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># We identify the end of the user&#39;s turn in the tokenized input.</span>
</span></span><span class=line><span class=cl><span class=c1># The template for a user turn is `&lt;|im_start|&gt;user\n...&lt;|im_end|&gt;`.</span>
</span></span><span class=line><span class=cl><span class=n>im_start_token</span> <span class=o>=</span> <span class=s1>&#39;&lt;|im_start|&gt;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>user_token</span> <span class=o>=</span> <span class=s1>&#39;user&#39;</span>
</span></span><span class=line><span class=cl><span class=n>im_end_token</span> <span class=o>=</span> <span class=s1>&#39;&lt;|im_end|&gt;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>im_start_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>im_start_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>user_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>user_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>im_end_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>im_end_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># We search for the token IDs corresponding to `&lt;|im_start|&gt;user` ([151644, 872]) and the closing `&lt;|im_end|&gt;` ([151645]).</span>
</span></span><span class=line><span class=cl><span class=n>last_start</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>token_ids_list</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>token_ids_list</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=p>[</span><span class=n>im_start_id</span><span class=p>,</span> <span class=n>user_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>user_end_index</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>last_start</span><span class=o>+</span><span class=mi>2</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>token_ids_list</span><span class=p>))</span> <span class=k>if</span> <span class=n>token_ids_list</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>im_end_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize the stream_state, which will maintain the conversational context.</span>
</span></span><span class=line><span class=cl><span class=n>stream_state</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=c1># Pass all user tokens to the model for an initial safety assessment.</span>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=p>,</span> <span class=n>stream_state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>stream_moderate_from_ids</span><span class=p>(</span><span class=n>token_ids</span><span class=p>[:</span><span class=n>user_end_index</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=n>stream_state</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;Safe&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;User moderation: -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;User moderation: -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2> - Category: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Moderate the assistant&#39;s response token-by-token to simulate streaming.</span>
</span></span><span class=line><span class=cl><span class=c1># This loop mimics how an LLM generates a response one token at a time.</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Assistant streaming moderation:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>user_end_index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Get the current token ID for the assistant&#39;s response.</span>
</span></span><span class=line><span class=cl>    <span class=n>current_token</span> <span class=o>=</span> <span class=n>token_ids</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Call the moderation function for the single new token.</span>
</span></span><span class=line><span class=cl>    <span class=c1># The stream_state is passed and updated in each call to maintain context.</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=p>,</span> <span class=n>stream_state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>stream_moderate_from_ids</span><span class=p>(</span><span class=n>current_token</span><span class=p>,</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=n>stream_state</span><span class=o>=</span><span class=n>stream_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>token_str</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>current_token</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># Print the generated token and its real-time safety assessment.</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;Safe&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=n>token_str</span><span class=p>)</span><span class=si>}</span><span class=s2> -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=n>token_str</span><span class=p>)</span><span class=si>}</span><span class=s2> -&gt; [Risk: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;risk_level&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2> - Category: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>close_stream</span><span class=p>(</span><span class=n>stream_state</span><span class=p>)</span>
</span></span></code></pre></div><p>For more usage examples, please visit our <a href=https://github.com/QwenLM/Qwen3Guard>GitHub repository</a>.</p><p><br><br></p><h2 id=future-work>Future Work<a hidden class=anchor aria-hidden=true href=#future-work>#</a></h2><p>AI safety remains an ongoing challenge. With Qwen3Guard, we take our one step forward. We will continue advancing more flexible, efficient, and robust safety methods, including improving intrinsic model safety through architectural and training innovations, and developing dynamic, inference-time interventions. Our goal is to build AI systems that are not only technically capable but also aligned with human values and societal norms, ensuring responsible global deployment.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>