<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generalizing an LLM from 8k to 1M Context using Qwen-Agent | Qwen</title><meta name=keywords content><meta name=description content="We&rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models."><meta name=author content="Qwen Team"><link rel=canonical href=http://qwenlm.github.io/blog/qwen-agent-2405/><link crossorigin=anonymous href=/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://qwenlm.github.io/blog/qwen-agent-2405/><link rel=alternate hreflang=zh href=http://qwenlm.github.io/zh/blog/qwen-agent-2405/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Generalizing an LLM from 8k to 1M Context using Qwen-Agent"><meta property="og:description" content="We&rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models."><meta property="og:type" content="article"><meta property="og:url" content="http://qwenlm.github.io/blog/qwen-agent-2405/"><meta property="og:image" content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-06-06T11:59:59+08:00"><meta property="article:modified_time" content="2024-06-06T11:59:59+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Generalizing an LLM from 8k to 1M Context using Qwen-Agent"><meta name=twitter:description content="We&rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Generalizing an LLM from 8k to 1M Context using Qwen-Agent","item":"http://qwenlm.github.io/blog/qwen-agent-2405/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generalizing an LLM from 8k to 1M Context using Qwen-Agent","name":"Generalizing an LLM from 8k to 1M Context using Qwen-Agent","description":"We\u0026rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.","keywords":[],"articleBody":"Qwen-Agent TLDR: We’ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.\nIntroduction Recently, there has been a hype trend in LLMs that can natively process sequences of millions of tokens. Most work has been focusing on sophisticated mathematical tweaks like RoPE-based extrapolation or architectural overhauls such as non-transformer LLMs. However, preparing fine-tuning data that is sufficiently long is a less discussed but equally important topic.\nWe adopt the following approach:\nWe use a weak 8k-context chat model to build a relatively strong agent capable of handling 1M-contexts. Subsequently, we synthesize fine-tuning data using the agent and apply automated filtering to ensure quality. Finally, we use the synthetic data to fine-tune a pretrained model, resulting in a strong 1M-context chat model. This blog primarily focuses on Step 1, with details of the subsequent steps to be revealed in the coming weeks or months.\nBuilding the Agent The agent we are building consists of three levels of complexity, each building upon the previous one.\nLevel 1: Retrieval-Augmented Generation A naive approach to processing a 1M-token context is to simply use retrieval-augmented generation (RAG) . RAG divides the context into shorter chunks, each not exceeding 512 tokens, for example, and then retains only the most relevant chunks within an 8k-token context.\nThe challenge lies in how to pinpoint the chunks that are the most relevant. After several trials, we have come up with a keyword-based solution:\nStep 1: Instruct the chat model to separate the instruction and the non-instruction information in the user’s query. For instance, transform the user query \"You should reply in 2000 words and be as detailed as possible. My question is, when were bicycles invented? Reply in English.\" into {\"information\": [\"when were bicycles invented\"], \"instruction\": [\"reply in 2000 words\", \"be as detailed as possible\", \"reply in English\"]}. Step 2: Ask the chat model to deduce multilingual keywords from the informational part of the query. For example, the phrase \"when were bicycles invented\" would be converted to {\"keywords_en\": [\"bicycles\", \"invented\", \"when\"], \"keywords_zh\": [\"自行车\", \"发明\", \"时间\"]}. Step 3: Employ the BM25 algorithm, a traditional keyword-based retrieval method, to locate the chunks that most relevant to the extracted keywords. Dataflows of retrieval-augmented generation We have also experimented with vector-based retrieval. However, in most cases, it does not offer a significant enough improvement to outweigh the additional complexity that arises from the necessity of deploying a separate embedding model.\nRAG Code Level 2: Chunk-by-Chunk Reading The aforementioned RAG approach is fast but often fails when the relevant chunks do not have sufficient keyword overlap with the user query, resulting in these chunks not being retrieved and thus not provided to the model. Although vector retrieval theoretically can mitigate this issue, in practice, it frequently does not.\nTo address this limitation, we employ a brute-force strategy to reduce the chance of missing relevant context:\nStep 1: For each 512-token chunk, we ask the model to assess its relevance to the user query, outputting \"None\" if it is deemed irrelevant, or outputting the relevant sentences if it is deemed relevant. The chunks are processed in parallel to avoid long waiting times. Step 2: We then take the outputs that are not \"None\" (the relevant sentences) and use them as the search query to retrieve the most relevant chunks (within an 8k-context limit) using BM25. Step 3: Finally, we generate the final answer based on the retrieved context in the same manner as RAG. Dataflows of chunk-by-chunk reading Agent Code Level 3: Step-by-Step Reasoning A classic challenge in document-based question-answering is multi-hop reasoning. For example, consider answering the question “What vehicle was invented in the same century as the Fifth Symphony was composed?” when given a long document containing relevant facts. The model needs to first determine the answer to the sub-question “In which century was the Fifth Symphony composed?” which is the 19th century. Then, it can realize that a chunk containing “Bicycles were invented in the 19th century” is actually relevant to the original question.\nTool-calling (also known as function-calling) agents or ReAct agents are classic solutions that have built-in capabilities for question decomposition and step-by-step reasoning. We therefore wrap the aforementioned Level-2 agent as a tool to be called by a tool-calling agent. The tool-calling agent conducts multi-hop reasoning as follows:\nAsk the Lv3-Agent a question. while (the Lv3-Agent cannot answer the question based on its memory) { The Lv3-Agent proposes a new sub-question to be answered. The Lv3-Agent asks the Lv2-Agent the sub-question. Add the Lv2-Agent's response to the Lv3-Agent's memory. } The Lv3-Agent provides the final answer to the original question. Dataflows of step-by-step reasioning For example, the Lv3-Agent initially poses a sub-question to the Lv2-Agent: “In which century was Beethoven’s Fifth Symphony composed?” Upon receiving the response, “the 19th century,” the Lv3-Agent formulates a subsequent sub-question: “What vehicle was invented during the 19th century?” By consolidating all the feedback from the Lv2-Agent, the Lv3-Agent can then answer the original question: “What vehicle was invented in the same century that the Fifth Symphony was composed?”\nExperiments We conducted experiments on two benchmarks designed for 256k-context:\nNeedleBench is a benchmark designed to test whether a model can identify the most relevant sentences within a context filled with numerous irrelevant ones, akin to finding needles in a haystack. Answering a question may require the simultaneous discovery of several “needles” and the execution of multi-hop reasoning. LV-Eval is a challenging benchmark that demands the comprehension of multiple pieces of evidence at once. We modified the evaluation metric from LV-Eval’s original version because it was excessively stringent, resulting in a high number of false negatives. We compared the following methods:\nThe 32k-Model, a 7B chat model fine-tuned mainly on 8k-context samples, with a few 32k-context samples, extended to a 256k context using a training-free method such as RoPE-based extrapolation. The 4k-RAG, which uses the same model as the 32k-Model but applies the Lv1-Agent RAG strategy. It only retrieves and processes the most relevant 4k context. The 4k-Agent, using the same model as the 32k-Model, follows the more advanced agent strategy described above. The agent strategy utilizes only a 4k-context with the model each time. The empirical data reveals:\nIn scenarios with short contexts, the 4k-RAG may perform less effectively than the 32k-Model. This could be due to difficulties in retrieving the right information or understanding multiple parts. Conversely, as document length increases, the 4k-RAG becomes more likely to outperform the 32k-Model. This trend suggests the 32k-Model isn’t optimally trained for handling long contexts. Significantly, the 4K-Agent consistently surpasses the 32k-Model and the 4k-RAG. Its ability to read all the context in chunks allows it to avoid the limitations posed by under-trained context lengths. Overall, the 32k-Model should ideally outshine all if it receives proper training. However, due to its under-training in practice, the 32k-Model under-performs compared to the 4k-Agent.\nFinally, we have also tested the agent on a 1-million-token pressure test (finding a single needle in a haystack of 1 million tokens) and found that it functioned properly. However, we still lack a more reliable quantitative benchmark for evaluating its performance in handling contexts of 1 million tokens in real-world applications.\nConclusion In this blog, we have introduced how to build the agent that is capable of handling 1M-context with a 8k-context model. It then becomes obvious how to synthesize the data once the agent is prepared. For instance, we could enlist volunteers to interact with the agents and record the outcomes to construct the fine-tuning dataset. Additionally, we can employ the agent to cross-validate the data generated by other methods to ensure the quality of the data. Moreover, the general idea of distilling an agent into a model is applicable to other fields as well, such as enhancing a model’s ability to solve long-horizon tasks.\nWhat’s More Qwen-Agent, our open-source RAG and agent framework, which began as internal utility code to facilitate model development, has recently undergone rapid development. We have released an implementation of the aforementioned long-context agent in the framework.\nWe hope to provide you with models that have improved capabilities for handling long contexts, as well as a more user-friendly infrastructure framework in the near future.\nCitation @misc{qwen-agent-2405, title = {Generalizing an LLM from 8k to 1M Context using Qwen-Agent}, url = {https://qwenlm.github.io/blog/qwen-agent-2405/}, author = {Qwen Team}, month = {May}, year = {2024} } ","wordCount":"1412","inLanguage":"en","datePublished":"2024-06-06T11:59:59+08:00","dateModified":"2024-06-06T11:59:59+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://qwenlm.github.io/blog/qwen-agent-2405/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Generalizing an LLM from 8k to 1M Context using Qwen-Agent</h1><div class=post-meta><span title='2024-06-06 11:59:59 +0800 +0800'>June 6, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1412 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=http://qwenlm.github.io/zh/blog/qwen-agent-2405/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><a href=https://github.com/QwenLM/Qwen-Agent class="btn external" target=_blank>Qwen-Agent</a><p><strong>TLDR:</strong> We&rsquo;ve created an agent using Qwen2 models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.</p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Recently, there has been a <strike>hype</strike> trend in LLMs that can natively process sequences of millions of tokens. Most work has been focusing on sophisticated mathematical tweaks like RoPE-based extrapolation or architectural overhauls such as non-transformer LLMs. However, preparing fine-tuning data that is sufficiently long is a less discussed but equally important topic.</p><p>We adopt the following approach:</p><ol><li>We use a <em>weak</em> 8k-context chat model to build a relatively <em>strong</em> agent capable of handling 1M-contexts.</li><li>Subsequently, we synthesize fine-tuning data using the agent and apply automated filtering to ensure quality.</li><li>Finally, we use the synthetic data to fine-tune a pretrained model, resulting in a <em>strong</em> 1M-context chat model.</li></ol><p>This blog primarily focuses on Step 1, with details of the subsequent steps to be revealed in the coming weeks or months.</p><h1 id=building-the-agent>Building the Agent<a hidden class=anchor aria-hidden=true href=#building-the-agent>#</a></h1><p>The agent we are building consists of three levels of complexity, each building upon the previous one.</p><h2 id=level-1-retrieval-augmented-generation>Level 1: Retrieval-Augmented Generation<a hidden class=anchor aria-hidden=true href=#level-1-retrieval-augmented-generation>#</a></h2><p>A naive approach to processing a 1M-token context is to simply use retrieval-augmented generation (RAG) .
RAG divides the context into shorter chunks, each not exceeding 512 tokens, for example, and then retains only the most relevant chunks within an 8k-token context.</p><p>The challenge lies in how to pinpoint the chunks that are the most relevant. After several trials, we have come up with a keyword-based solution:</p><ul><li>Step 1: Instruct the chat model to separate the instruction and the non-instruction information in the user&rsquo;s query. For instance, transform the user query <code>"You should reply in 2000 words and be as detailed as possible. My question is, when were bicycles invented? Reply in English."</code> into <code>{"information": ["when were bicycles invented"], "instruction": ["reply in 2000 words", "be as detailed as possible", "reply in English"]}</code>.</li><li>Step 2: Ask the chat model to deduce multilingual keywords from the informational part of the query. For example, the phrase <code>"when were bicycles invented"</code> would be converted to <code>{"keywords_en": ["bicycles", "invented", "when"], "keywords_zh": ["自行车", "发明", "时间"]}</code>.</li><li>Step 3: Employ the BM25 algorithm, a traditional keyword-based retrieval method, to locate the chunks that most relevant to the extracted keywords.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-lv1-agent.png width=100%><figcaption><h4>Dataflows of retrieval-augmented generation</h4></figcaption></figure><p>We have also experimented with vector-based retrieval. However, in most cases, it does not offer a significant enough improvement to outweigh the additional complexity that arises from the necessity of deploying a separate embedding model.</p><a href=https://github.com/QwenLM/Qwen-Agent/blob/main/examples/assistant_rag.py class="btn external" target=_blank>RAG Code</a><h2 id=level-2-chunk-by-chunk-reading>Level 2: Chunk-by-Chunk Reading<a hidden class=anchor aria-hidden=true href=#level-2-chunk-by-chunk-reading>#</a></h2><p>The aforementioned RAG approach is fast but often fails when the relevant chunks do not have sufficient keyword overlap with the user query, resulting in these chunks not being retrieved and thus not provided to the model. Although vector retrieval theoretically can mitigate this issue, in practice, it frequently does not.</p><p>To address this limitation, we employ a brute-force strategy to reduce the chance of missing relevant context:</p><ul><li>Step 1: For each 512-token chunk, we ask the model to assess its relevance to the user query, outputting <code>"None"</code> if it is deemed irrelevant, or outputting the relevant sentences if it is deemed relevant. The chunks are processed in parallel to avoid long waiting times.</li><li>Step 2: We then take the outputs that are not <code>"None"</code> (the relevant sentences) and use them as the search query to retrieve the most relevant chunks (within an 8k-context limit) using BM25.</li><li>Step 3: Finally, we generate the final answer based on the retrieved context in the same manner as RAG.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-lv2-agent.png width=100%><figcaption><h4>Dataflows of chunk-by-chunk reading</h4></figcaption></figure><a href=https://github.com/QwenLM/Qwen-Agent/blob/main/examples/parallel_doc_qa.py class="btn external" target=_blank>Agent Code</a><h2 id=level-3-step-by-step-reasoning>Level 3: Step-by-Step Reasoning<a hidden class=anchor aria-hidden=true href=#level-3-step-by-step-reasoning>#</a></h2><p>A classic challenge in document-based question-answering is multi-hop reasoning. For example, consider answering the question &ldquo;What vehicle was invented in the same century as the Fifth Symphony was composed?&rdquo; when given a long document containing relevant facts. The model needs to first determine the answer to the sub-question &ldquo;In which century was the Fifth Symphony composed?&rdquo; which is the 19th century. Then, it can realize that a chunk containing &ldquo;Bicycles were invented in the 19th century&rdquo; is actually relevant to the original question.</p><p>Tool-calling (also known as function-calling) agents or ReAct agents are classic solutions that have built-in capabilities for question decomposition and step-by-step reasoning. We therefore wrap the aforementioned Level-2 agent as a tool to be called by a tool-calling agent. The tool-calling agent conducts multi-hop reasoning as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Ask the Lv3-Agent a question.
</span></span><span class=line><span class=cl>while (the Lv3-Agent cannot answer the question based on its memory) {
</span></span><span class=line><span class=cl>    The Lv3-Agent proposes a new sub-question to be answered.
</span></span><span class=line><span class=cl>    The Lv3-Agent asks the Lv2-Agent the sub-question.
</span></span><span class=line><span class=cl>    Add the Lv2-Agent&#39;s response to the Lv3-Agent&#39;s memory.
</span></span><span class=line><span class=cl>}
</span></span><span class=line><span class=cl>The Lv3-Agent provides the final answer to the original question.
</span></span></code></pre></div><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-lv3-agent.png width=100%><figcaption><h4>Dataflows of step-by-step reasioning</h4></figcaption></figure><p>For example, the Lv3-Agent initially poses a sub-question to the Lv2-Agent: &ldquo;In which century was Beethoven&rsquo;s Fifth Symphony composed?&rdquo; Upon receiving the response, &ldquo;the 19th century,&rdquo; the Lv3-Agent formulates a subsequent sub-question: &ldquo;What vehicle was invented during the 19th century?&rdquo; By consolidating all the feedback from the Lv2-Agent, the Lv3-Agent can then answer the original question: &ldquo;What vehicle was invented in the same century that the Fifth Symphony was composed?&rdquo;</p><h1 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h1><p>We conducted experiments on two benchmarks designed for 256k-context:</p><ul><li>NeedleBench is a benchmark designed to test whether a model can identify the most relevant sentences within a context filled with numerous irrelevant ones, akin to finding needles in a haystack. Answering a question may require the simultaneous discovery of several &ldquo;needles&rdquo; and the execution of multi-hop reasoning.</li><li>LV-Eval is a challenging benchmark that demands the comprehension of multiple pieces of evidence at once. We modified the evaluation metric from LV-Eval&rsquo;s original version because it was excessively stringent, resulting in a high number of false negatives.</li></ul><p>We compared the following methods:</p><ul><li>The 32k-Model, a 7B chat model fine-tuned mainly on 8k-context samples, with a few 32k-context samples, extended to a 256k context using a training-free method such as RoPE-based extrapolation.</li><li>The 4k-RAG, which uses the same model as the 32k-Model but applies the Lv1-Agent RAG strategy. It only retrieves and processes the most relevant 4k context.</li><li>The 4k-Agent, using the same model as the 32k-Model, follows the more advanced agent strategy described above. The agent strategy utilizes only a 4k-context with the model each time.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-blog-long-context-results.png width=100%></figure><p>The empirical data reveals:</p><ul><li>In scenarios with short contexts, the 4k-RAG may perform less effectively than the 32k-Model. This could be due to difficulties in retrieving the right information or understanding multiple parts.</li><li>Conversely, as document length increases, the 4k-RAG becomes more likely to outperform the 32k-Model. This trend suggests the 32k-Model isn&rsquo;t optimally trained for handling long contexts.</li><li>Significantly, the 4K-Agent consistently surpasses the 32k-Model and the 4k-RAG. Its ability to read all the context in chunks allows it to avoid the limitations posed by under-trained context lengths.</li></ul><p>Overall, the 32k-Model should ideally outshine all if it receives proper training. However, due to its under-training in practice, the 32k-Model under-performs compared to the 4k-Agent.</p><p>Finally, we have also tested the agent on a 1-million-token pressure test (finding a single needle in a haystack of 1 million tokens) and found that it functioned properly. However, we still lack a more reliable quantitative benchmark for evaluating its performance in handling contexts of 1 million tokens in real-world applications.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>In this blog, we have introduced how to build the agent that is capable of handling 1M-context with a 8k-context model. It then becomes obvious how to synthesize the data once the agent is prepared. For instance, we could enlist volunteers to interact with the agents and record the outcomes to construct the fine-tuning dataset. Additionally, we can employ the agent to cross-validate the data generated by other methods to ensure the quality of the data. Moreover, the general idea of distilling an agent into a model is applicable to other fields as well, such as enhancing a model&rsquo;s ability to solve long-horizon tasks.</p><h1 id=whats-more>What&rsquo;s More<a hidden class=anchor aria-hidden=true href=#whats-more>#</a></h1><p><a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a>, our open-source RAG and agent framework, which began as internal utility code to facilitate model development, has recently undergone rapid development. We have released an implementation of the aforementioned long-context agent in the framework.</p><p>We hope to provide you with models that have improved capabilities for handling long contexts, as well as a more user-friendly infrastructure framework in the near future.</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><pre tabindex=0><code>@misc{qwen-agent-2405,
    title = {Generalizing an LLM from 8k to 1M Context using Qwen-Agent},
    url = {https://qwenlm.github.io/blog/qwen-agent-2405/},
    author = {Qwen Team},
    month = {May},
    year = {2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>