<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-LLM: Extending the boundary of LLMs | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5-llm/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-llm/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-llm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-LLM: Extending the boundary of LLMs"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5-llm/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-09-19T00:00:03+08:00"><meta property="article:modified_time" content="2024-09-19T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-LLM: Extending the boundary of LLMs"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5-LLM: Extending the boundary of LLMs","item":"https://qwenlm.github.io/blog/qwen2.5-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-LLM: Extending the boundary of LLMs","name":"Qwen2.5-LLM: Extending the boundary of LLMs","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2.","keywords":[],"articleBody":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2.5-3B, Qwen2.5-14B, and Qwen2.5-32B. Furthermore, we are excited to offer additional models, including Qwen-Plus and Qwen-Turbo, available through API services via Alibaba Cloud Model Studio.\nCompared with the Qwen2 series, the Qwen2.5 series has the following upgrades:\nFull-scale Open-source: Considering that users have a strong interest in models in the 10-30B range for production and 3B models for mobile applications, Qwen2.5, in addition to continuing to open source the four models of 0.5/1.5/7/72B of the same size as Qwen2, also added two medium-sized cost-effective models of Qwen2.5-14B and Qwen2.5-32B and a mobile-side model called Qwen2.5-3B. All models are highly competitive compared to open-source models of the same level. For example, Qwen2.5-32B beats Qwen2-72B and Qwen2.5-14B outperforms Qwen2-57B-A14B in our comprehensive evaluations.\nLarger and Higher Quality Pre-training Dataset: The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of 18 trillion tokens.\nKnowledge Enhancement: Qwen2.5 has acquired significantly more knowledge. On MMLU benchmarks, Qwen2.5-7/72B are improved from 70.3 to 74.2 and 84.2 to 86.1 compared to Qwen2-7/72B. We observe that Qwen2.5 also has significant improvements on the GPQA/MMLU-Pro/MMLU-redux/ARC-c benchmarks.\nCoding Enhancement: Thanks to the technical breakthrough of Qwen2.5-Coder, Qwen2.5 has greatly improved capabilities in coding. Qwen2.5-72B-Instruct achieves 55.5, 75.1, and 88.2 scores on LiveCodeBench (2305-2409), MultiPL-E and MBPP, respectively, outperforming Qwen2-72B-Instruct with 32.2, 69.2, and 80.2.\nMath Enhancement: After integrating Qwen2-math’s technology, the mathematical ability of Qwen2.5 has also been rapidly improved. On the MATH benchmark, the scores of Qwen2.5-7B/72B-Instruct have been increased from 52.9/69.0 of Qwen2-7B/72B-Instruct to 75.5/83.1.\nBetter Human Preference: Qwen2.5 is capable of generating responses that align more closely with human preferences. Specifically, the Arena-Hard score for Qwen2.5-72B-Instruct has increased significantly from 48.1 to 81.2, and the MT-Bench score has improved from 9.12 to 9.35, compared to Qwen2-72B-Instruct.\nOther Core Capabilities Enhancement: Qwen2.5 achieves significant improvements in instruction following, generating long texts (increased from 1k to over 8K tokens), understanding structured data (e.g., tables), and generating structured outputs, especially JSON. Furthermore, Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nModel Card Here is a model card detailing the key parameters of the Qwen2.5 LLM models. This release includes seven open-sourced models with sizes ranging from 0.5B to 72B. Most models support a context length of 128K (131,072) tokens and can generate up to 8K tokens, enabling the production of extensive text outputs. The majority of these models are licensed under Apache 2.0, while Qwen2.5-3B and Qwen2.5-72B are governed by the Qwen Research License and Qwen License, respectively.\nModels Params Non-Emb Params Layers Heads (KV) Tie Embedding Context Length Generation Length License Qwen2.5-0.5B 0.49B 0.36B 24 14 / 2 Yes 32K 8K Apache 2.0 Qwen2.5-1.5B 1.54B 1.31B 28 12 / 2 Yes 32K 8K Apache 2.0 Qwen2.5-3B 3.09B 2.77B 36 16 / 2 Yes 32K 8K Qwen Research Qwen2.5-7B 7.61B 6.53B 28 28 / 4 No 128K 8K Apache 2.0 Qwen2.5-14B 14.7B 13.1B 48 40 / 8 No 128K 8K Apache 2.0 Qwen2.5-32B 32.5B 31.0B 64 40 / 8 No 128K 8K Apache 2.0 Qwen2.5-72B 72.7B 70.0B 80 64 / 8 No 128K 8K Qwen Performance This section presents the performance metrics for both base language models and instruction-tuned models across various benchmark evaluations, encompassing a diverse array of domains and tasks.\nQwen2.5 Base Language Model Evaluation The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.\nThe evaluation datasets include:\nGeneral Tasks: MMLU (5-shot), MMLU-Pro (5-shot), MMLU-redux (5-shot), BBH (3-shot), ARC-C (25-shot), TruthfulQA (0-shot), Winogrande (5-shot), HellaSwag (10-shot)\nMath \u0026 Science Tasks: GPQA (5-shot), Theorem QA (5-shot), GSM8K (4-shot), MATH (4-shot)\nCoding Tasks: HumanEval (0-shot), HumanEval+ (0-shot), MBPP (0-shot), MBPP+ (0-shot), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)\nMultilingual Tasks: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)\nQwen2.5-72B Performance Datasets Llama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B General Tasks MMLU 79.5 77.8 85.2 84.2 86.1 MMLU-Pro 52.8 51.6 61.6 55.7 58.1 MMLU-redux 75.0 72.9 - 80.5 83.9 BBH 81.0 78.9 85.9 82.4 86.3 ARC-C 68.8 70.7 - 68.9 72.4 TruthfulQA 45.6 51.0 - 54.8 60.4 WindoGrande 85.3 85.0 86.7 85.1 83.9 HellaSwag 88.0 88.7 - 87.3 87.6 Mathematics \u0026 Science Tasks GPQA 36.3 34.3 - 37.4 45.9 Theoremqa 32.3 35.9 - 42.8 42.4 MATH 42.5 41.7 53.8 50.9 62.1 MMLU-stem 73.7 71.7 - 79.6 82.7 GSM8K 77.6 83.7 89.0 89.0 91.5 Coding Tasks HumanEval 48.2 46.3 61.0 64.6 59.1 HumanEval+ 42.1 40.2 - 56.1 51.2 MBPP 70.4 71.7 73.0 76.9 84.7 MBPP+ 58.4 58.1 - 63.9 69.2 MultiPL-E 46.3 46.7 - 59.6 60.5 Multilingual Tasks Multi-Exam 70.0 63.5 - 76.6 78.7 Multi-Understanding 79.9 77.7 - 80.7 89.6 Multi-Mathematics 67.1 62.9 - 76.0 76.7 Multi-Translation 38.0 23.3 - 37.8 39.0 The Qwen2.5-72B base model significantly outperforms its peers in the same category across a wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges.\nQwen2.5-14B/32B Performance Datasets Qwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2-57B-A14B Qwen2.5-14B Qwen2.5-32B General Tasks MMLU 74.3 75.2 77.2 76.5 79.7 83.3 MMLU-pro 44.1 49.1 48.3 43.0 51.2 55.1 MMLU-redux 69.0 - 74.1 72.4 76.6 82.0 BBH 66.8 74.9 76.4 67.0 78.2 84.5 ARC-C 63.6 71.4 65.6 64.1 67.3 70.4 Truthfulqa 57.4 40.1 53.9 57.7 58.4 57.8 Winogrande 81.5 59.7 84.9 79.5 - 82.0 Hellaswag 85.0 86.4 85.9 85.2 - 85.2 Mathematics \u0026 Science Tasks GPQA 30.8 34.9 37.4 34.3 32.8 48.0 Theoremqa 28.8 35.8 40.0 33.5 43.0 44.1 MATH 36.1 42.7 41.7 43.0 55.6 57.7 MMLU-stem 66.5 71.0 72.6 69.8 76.4 80.9 GSM8K 78.5 81.1 81.7 80.7 90.2 92.9 Coding Tasks HumanEval 43.3 54.9 46.3 53.0 56.7 58.5 HumanEval+ 40.2 46.3 40.2 46.3 51.2 52.4 MBPP 64.2 75.7 65.5 71.9 76.7 84.5 MBPP+ 53.9 60.2 55.4 57.4 63.2 67.2 MultiPL-E 38.5 48.0 39.5 49.8 53.5 59.4 Multilingual Tasks Multi-Exam 61.6 65.8 58.3 65.5 70.6 75.4 Multi-Understanding 76.5 82.2 73.9 77.0 85.9 88.4 Multi-Mathematics 56.1 61.6 49.3 62.3 68.5 73.7 Multi-Translation 33.5 38.7 30.0 34.5 36.2 37.3 The Qwen2.5-14B model demonstrates a solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP.\nQwen2.5-7B Performance Datasets Mistral-7B Llama3-8B Gemma2-9B Qwen2-7B Qwen2.5-7B #Non-emb Params 7.0B 7.0B 8.2B 6.5B 6.5B General Tasks MMLU 64.2 66.6 71.3 70.3 74.2 MMLU-pro 30.9 35.4 44.7 40.1 45.0 MMLU-redux 58.1 61.6 67.9 68.1 71.1 BBH 56.1 57.7 68.2 62.3 70.4 ARC-C 60.0 59.3 68.2 60.6 63.7 Trurhfulqa 42.2 44.0 45.3 54.2 56.4 Winogrande 78.4 77.4 79.5 77.0 75.9 Hellaswag 83.3 82.1 81.9 80.7 80.2 Mathematics \u0026 Science Tasks GPQA 24.7 25.8 32.8 30.8 36.4 Theoremqa 19.2 22.1 28.9 29.6 36.0 MATH 10.2 20.5 37.7 43.5 49.8 MMLU-stem 50.1 55.3 65.1 64.2 72.3 GSM8K 36.2 55.3 70.7 80.2 85.4 Coding Tasks HumanEval 29.3 33.5 37.8 51.2 57.9 HumanEval+ 24.4 29.3 30.5 43.3 50.6 MBPP 51.1 53.9 62.2 64.2 74.9 MBPP+ 40.9 44.4 50.6 51.9 62.9 MultiPL-E 29.4 22.6 34.9 41.0 50.3 Multilingual Tasks Multi-Exam 47.1 52.3 61.2 59.2 59.4 Multi-Understanding 63.3 68.6 78.3 72.0 79.3 Multi-Mathematics 26.3 36.3 53.0 57.5 57.8 Multi-Translation 23.3 31.9 36.5 31.5 32.4 The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.\nQwen2.5-0.5B/1.5B/3B Performance Datasets Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B General Tasks MMLU 44.3 47.5 55.9 60.9 52.2 65.6 MMLU-pro 14.7 15.7 21.6 28.5 23.0 34.6 MMLU-redux 40.7 45.1 51.8 58.5 50.9 63.7 BBH 18.2 20.3 36.5 45.1 41.9 56.3 ARC-C 31.0 35.6 43.7 54.7 55.7 56.5 Trurhfulqa 39.7 40.2 45.9 46.6 36.2 48.9 Winogrande 56.9 56.3 65.0 65.0 71.5 71.1 Hellaswag 49.1 52.1 67.0 67.9 74.6 74.6 Mathematics \u0026 Science Tasks GPQA 29.8 24.8 20.7 24.2 25.3 26.3 Theoremqa 9.6 16.0 14.8 22.1 15.9 27.4 MATH 11.2 19.5 21.6 35.0 18.3 42.6 MMLU-stem 27.5 39.8 42.7 54.8 45.8 62.5 GSM8K 36.4 41.6 46.9 68.5 30.3 79.1 Coding Tasks HumanEval 22.6 30.5 34.8 37.2 19.5 42.1 HumanEval+ 18.9 26.8 29.9 32.9 15.9 36.0 MBPP 33.1 39.3 46.9 60.2 42.1 57.1 MBPP+ 27.6 33.8 37.6 49.6 33.6 49.4 MultiPL-E 16.3 18.9 27.9 33.1 17.6 41.2 Multilingual Tasks Multi-Exam 29.4 30.8 43.1 47.9 38.1 54.6 Multi-Understanding 40.4 41.0 50.7 65.1 46.8 76.6 Multi-Mathematics 7.8 13.5 21.3 37.5 18.2 48.9 Multi-Translation 14.1 15.3 23.8 25.0 26.9 29.3 For edge-side models, Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.\nInstruction-tuned Model Evaluation The evaluation of instruction-tuned models mainly focuses on the model performance of natural language understanding, general question answering, reasoning, coding, mathematics, instruction following, human alignment, etc.\nThe datasets for evaluation include:\nGeneral Tasks: MMLU-Pro, MMLU-redux\nMath \u0026 Science Tasks: GPQA, GSM8K, MATH\nCoding Tasks: HumanEval, MBPP, MultiPL-E, LiveCodeBench 2305-2409, LiveBench 0831\nInstruction \u0026 Alignment Tasks: IFeval strict-prompt, Arena-Hard, AlignBench v1.1, MTbench\nQwen2.5-72B-Instruct Performance Datasets Mistral-Large2 Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2-72B-Instruct Qwen2.5-72B-Instruct MMLU-Pro 69.4 66.4 73.3 64.4 71.1 MMLU-redux 83.0 83.0 86.2 81.6 86.8 GPQA 52.0 46.7 51.1 42.4 49.0 MATH 69.9 68.0 73.8 69.0 83.1 GSM8K 92.7 95.1 96.8 93.2 95.8 HumanEval 92.1 80.5 89.0 86.0 86.6 MBPP 80.0 84.2 84.5 80.2 88.2 MultiPL-E 76.9 68.2 73.5 69.2 75.1 LiveCodeBench 2305-2409 42.2 32.1 41.6 32.2 55.5 LiveBench 0831 48.5 46.6 53.2 41.5 52.3 IFeval strict-prompt 64.1 83.6 86.0 77.6 84.1 Arena-Hard 73.1 55.7 69.3 48.1 81.2 AlignBench v1.1 7.69 5.94 5.95 8.15 8.16 MTbench 8.61 8.79 9.08 9.12 9.35 The Qwen2.5-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B in several critical tasks. Qwen2.5-72B-Instruct excels in mathematics (MATH: 83.1), coding (LiveCodeBench: 55.5), and chatting (Arena-Hard: 81.2). Compared to its base model Qwen2.5-72B and its predecessor Qwen2-72B-Instruct, the Qwen2.5-72B-Instruct showcases comprehensive improvements across all tasks.\nQwen-Turbo \u0026 Qwen2.5-14B-Instruct \u0026 Qwen2.5-32B-Instruct Performance Datasets Qwen2-57B-A14B-Instruct Gemma2-27B-IT GPT4o-mini Qwen-Turbo Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct MMLU-Pro 52.8 55.5 63.1 64.8 63.7 69.0 MMLU-redux 72.6 75.7 81.5 80.4 80.0 83.9 GPQA 34.3 38.4 40.2 44.4 45.5 49.5 MATH 49.1 54.4 70.2 81.0 80.0 83.1 GSM8K 85.3 90.4 93.2 93.6 94.8 95.9 HumanEval 79.9 78.7 88.4 86.6 83.5 88.4 MBPP 70.9 81.0 85.7 80.2 82.0 84.0 MultiPL-E 66.4 67.4 75.0 73.0 72.8 75.4 LiveCodeBench 2305-2409 22.5 - 40.7 43.1 42.6 51.2 LiveBench 0831 31.1 39.6 43.3 41.6 44.4 50.7 IFeval strict-prompt 59.9 77.1 80.4 74.9 81.0 79.5 Arena-Hard 17.8 57.5 74.9 68.4 68.3 74.5 AlignBench v1.1 7.02 7.22 7.81 7.99 7.94 7.93 MTbench 8.55 9.10 - 8.86 8.88 9.20 The Qwen2.5-32B-Instruct model demonstrates superior performance across most tasks when compared to other models of similar size. In comparison to GPT-4o-mini, our open-source model, Qwen2.5-14B-Instruct, along with our API model, Qwen-Turbo, also deliver competitive results across all benchmarks.\nQwen2.5-7B-Instruct Performance Datasets Gemma2-9b-IT Llama3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct MMLU-Pro 52.1 48.3 44.1 56.3 MMLU-redux 72.8 67.2 67.3 75.4 GPQA 32.8 32.8 34.3 36.4 MATH 44.3 51.9 52.9 75.5 GSM8K 76.7 84.5 85.7 91.6 HumanEval 68.9 72.6 79.9 84.8 MBPP 74.9 69.6 67.2 79.2 MultiPL-E 53.4 50.7 59.1 70.4 LiveCodeBench 2305-2409 18.9 8.3 23.9 28.7 LiveBench 0831 30.6 26.7 29.2 35.9 IFeval strict-prompt 70.1 75.9 54.7 71.2 Arena-Hard 41.6 27.8 25.0 52.0 AlignBench v1.1 7.05 4.75 7.13 7.33 MTbench 8.49 8.23 8.26 8.75 The Qwen2.5-7B-Instruct model significantly outperforms its competitors, Gemma2-9b-IT and Llama3.1-8B-Instruct, across all tasks except IFeval. Notably, Qwen2.5-7B-Instruct demonstrates clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8).\nQwen2.5-3B-Instruct Performance Datasets Gemma2-2B-IT Phi3.5-mini-Instruct MiniCPM3-4B Qwen2.5-3B-Instruct Non-Emb Params 2.0B 3.6B 4.0B 2.8B MMLU-Pro 26.7 47.5 43.0 43.7 MMLU-redux 51.9 67.7 59.9 64.4 GPQA 29.3 27.2 31.3 30.3 MATH 26.6 48.5 46.6 65.9 GSM8K 63.2 86.2 81.1 86.7 HumanEval 68.9 72.6 74.4 74.4 MBPP 74.9 63.2 72.5 72.7 MultiPL-E 30.5 47.2 49.1 60.2 LiveCodeBench 2305-2409 5.8 15.8 23.8 19.9 LiveBench 0831 20.1 27.4 27.6 26.8 IFeval strict-prompt 51.0 52.1 68.4 58.2 As for the edge-side instruction model, the Qwen2.5-3B-Instruct model has fewer parameters than both the Phi3.5-mini-Instruct and MiniCPM3-4B models. Despite this, it outperforms them in mathematics and coding tasks while delivering competitive results in language understanding.\nQwen2.5-0.5B/1.5B-Instruct Performance Datasets Qwen2-0.5B-Instruct Qwen2.5-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2.5-1.5B-Instruct MMLU-Pro 14.4 15.0 22.9 32.4 MMLU-redux 12.9 24.1 41.2 50.7 GPQA 23.7 29.8 21.2 29.8 MATH 13.9 34.4 25.3 55.2 GSM8K 40.1 49.6 61.6 73.2 HumanEval 31.1 35.4 42.1 61.6 MBPP 39.7 49.6 44.2 63.2 MultiPL-E 20.8 28.5 38.5 50.4 LiveCodeBench 2305-2409 1.6 5.1 4.5 14.8 LiveBench 0831 7.4 12.6 12.4 18.8 IFeval strict-prompt 14.6 27.9 29.0 42.5 Qwen2.5-1.5B-Instruct and Qwen2.5-0.5B-Instruct have seen large performance improvements over their previous versions, making them well-suited for edge-side applications in highly resource-constrained environments.\nPerformances on Multilingualism To evaluate the multilingual performance of instruction-tuned models, we collect and extend benchmarks as follows:\nIFEval (multilingual): We translate the examples from IFEval (English version) to construct multilingual IFEval examples after removing examples with language-specific contents (e.g., “start with letter A”). We collect 100 examples for each language among Arabic (ar), Spanish (es), French (fr), Indonesian (in), Japanese (ja), Korean (ko), Portuguese (pt), and Vietnamese (vi) languages. All examples are checked and post-edited (if neccessary) by paid volunteers. Knowledge: We use 5 MMLU-like benchmarks (multi-choice) to testify the knowledge utilization ability of Qwen2.5 series models on multilingualism, including AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Also, we present the performances on translated MMLU (i.e., okapi_MMLU, from English to multiple languages). MGSM8K (extended): Aside from the examples in the original MGSM8K benchmark, we extend the language support with Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). We translate 250 examples (same as the other languages engaged in MGSM8K) into those 4 languages. All examples are also checked and post-edited (if necessary) by paid volunteers. Cultural Nuances: We also use BLEnD, a benchmark aiming at testifying cultural nuances of LLMs, to testify LLMs from the Qwen2.5 series. Datasets Qwen2-72B-Instruct Llama3.1-70B-Instruct Qwen2.5-32B-Instruct Mistral-Large-Instruct-2407 (123B) GPT4o-mini Qwen2.5-72B-Instruct Instruction Following IFEval (multilingual) 79.69 80.47 82.68 82.69 85.03 86.98 Knowledge AMMLU (Arabic) 68.85 70.08 70.44 69.24 69.73 72.44 JMMLU (Japanese) 77.37 73.89 76.55 75.77 73.74 80.56 KMMLU (Korean) 57.04 53.23 60.75 56.42 56.77 61.96 IndoMMLU (Indonesian) 66.31 67.50 66.42 63.21 67.75 69.25 TurkishMMLU (Turkish) 69.22 66.89 72.41 64.78 71.19 76.12 okapi MMLU (translated) 77.84 76.49 77.16 78.37 73.44 79.97 Math Reasoning MGSM8K (extended) 82.72 73.31 87.15 89.01 87.36 88.16 Cultural Nuances BLEnD 25.90 30.49 27.88 33.47 35.91 32.48 Datasets Qwen2-7B-Instruct Llama3.1-8B-Instruct Qwen2.5-7B-Instruct Gemma-2-9B-Instruct Mistral-Nemo-Instruct-2407 (12B) Qwen2.5-14B-Instruct Instruction Following IFEval (multilingual) 51.43 60.68 74.87 77.47 64.59 77.08 Knowledge AMMLU (Arabic) 54.87 54.28 59.78 60.26 53.92 66.81 JMMLU (Japanese) 57.71 53.26 61.88 64.59 55.17 72.78 KMMLU (Korean) 43.96 42.28 46.59 46.24 42.22 59.71 IndoMMLU (Indonesian) 54.05 53.92 56.42 61.73 50.76 65.09 TurkishMMLU (Turkish) 49.27 45.61 54.28 55.44 34.44 66.85 okapi MMLU (translated) 60.47 55.18 66.98 46.72 59.65 72.12 Math Reasoning MGSM8K (extended) 56.13 66.05 66.11 78.37 54.75 82.27 Cultural Nuances BLEnD 22.49 19.47 23.66 28.31 26.61 26.99 Demo Cases Here we provide several cases to demonstrate the new or enhanced capabilities of Qwen2.5, including generating JSON output, generating long texts, and understanding structured data.\nExample: Generating JSON Output\rNext\rJSON Output\rExample: Structured Data Understanding\rNext\rTable Understanding\rExample: Long Text Generation\rNext\rText Generation\r","wordCount":"2680","inLanguage":"en","datePublished":"2024-09-19T00:00:03+08:00","dateModified":"2024-09-19T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5-llm/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-LLM: Extending the boundary of LLMs</h1><div class=post-meta><span title='2024-09-19 00:00:03 +0800 +0800'>September 19, 2024</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2680 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5-llm/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2.5-3B, Qwen2.5-14B, and Qwen2.5-32B. Furthermore, we are excited to offer additional models, including Qwen-Plus and Qwen-Turbo, available through API services via <a href=https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm>Alibaba Cloud Model Studio</a>.</p><p>Compared with the Qwen2 series, the Qwen2.5 series has the following upgrades:</p><ol><li><p><strong>Full-scale Open-source</strong>: Considering that users have a strong interest in models in the 10-30B range for production and 3B models for mobile applications, Qwen2.5, in addition to continuing to open source the four models of 0.5/1.5/7/72B of the same size as Qwen2, also added two medium-sized cost-effective models of <strong>Qwen2.5-14B</strong> and <strong>Qwen2.5-32B</strong> and a mobile-side model called <strong>Qwen2.5-3B</strong>.
All models are highly competitive compared to open-source models of the same level. For example, Qwen2.5-32B beats Qwen2-72B and Qwen2.5-14B outperforms Qwen2-57B-A14B in our comprehensive evaluations.</p></li><li><p><strong>Larger and Higher Quality Pre-training Dataset</strong>: The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of <strong>18 trillion</strong> tokens.</p></li><li><p><strong>Knowledge Enhancement</strong>: Qwen2.5 has acquired significantly more knowledge. On MMLU benchmarks, Qwen2.5-7/72B are improved from 70.3 to <strong>74.2</strong> and 84.2 to <strong>86.1</strong> compared to Qwen2-7/72B. We observe that Qwen2.5 also has significant improvements on the GPQA/MMLU-Pro/MMLU-redux/ARC-c benchmarks.</p></li><li><p><strong>Coding Enhancement</strong>: Thanks to the technical breakthrough of Qwen2.5-Coder, Qwen2.5 has greatly improved capabilities in coding. Qwen2.5-72B-Instruct achieves <strong>55.5</strong>, <strong>75.1</strong>, and <strong>88.2</strong> scores on LiveCodeBench (2305-2409), MultiPL-E and MBPP, respectively, outperforming Qwen2-72B-Instruct with 32.2, 69.2, and 80.2.</p></li><li><p><strong>Math Enhancement</strong>: After integrating Qwen2-math&rsquo;s technology, the mathematical ability of Qwen2.5 has also been rapidly improved. On the MATH benchmark, the scores of Qwen2.5-7B/72B-Instruct have been increased from 52.9/69.0 of Qwen2-7B/72B-Instruct to <strong>75.5/83.1</strong>.</p></li><li><p><strong>Better Human Preference</strong>: Qwen2.5 is capable of generating responses that align more closely with human preferences. Specifically, the Arena-Hard score for Qwen2.5-72B-Instruct has increased significantly from <strong>48.1</strong> to <strong>81.2</strong>, and the MT-Bench score has improved from <strong>9.12</strong> to <strong>9.35</strong>, compared to Qwen2-72B-Instruct.</p></li><li><p><strong>Other Core Capabilities Enhancement</strong>: Qwen2.5 achieves significant improvements in <strong>instruction following</strong>, <strong>generating long texts</strong> (increased from 1k to over <strong>8K tokens</strong>), <strong>understanding structured data</strong> (e.g., tables), and <strong>generating structured outputs</strong>, especially JSON. Furthermore, Qwen2.5 models are generally more resilient to the diversity of <strong>system prompts</strong>, enhancing <strong>role-play</strong> implementation and <strong>condition-setting</strong> for chatbots.</p></li></ol><h1 id=model-card>Model Card<a hidden class=anchor aria-hidden=true href=#model-card>#</a></h1><p>Here is a model card detailing the key parameters of the Qwen2.5 LLM models. This release includes seven open-sourced models with sizes ranging from 0.5B to 72B. Most models support a context length of 128K (131,072) tokens and can generate up to 8K tokens, enabling the production of extensive text outputs. The majority of these models are licensed under Apache 2.0, while Qwen2.5-3B and Qwen2.5-72B are governed by the Qwen Research License and Qwen License, respectively.</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Params</th><th style=text-align:center>Non-Emb Params</th><th style=text-align:center>Layers</th><th style=text-align:center>Heads (KV)</th><th style=text-align:center>Tie Embedding</th><th style=text-align:center>Context Length</th><th style=text-align:center>Generation Length</th><th style=text-align:center>License</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2.5-0.5B</td><td style=text-align:center>0.49B</td><td style=text-align:center>0.36B</td><td style=text-align:center>24</td><td style=text-align:center>14 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-1.5B</td><td style=text-align:center>1.54B</td><td style=text-align:center>1.31B</td><td style=text-align:center>28</td><td style=text-align:center>12 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-3B</td><td style=text-align:center>3.09B</td><td style=text-align:center>2.77B</td><td style=text-align:center>36</td><td style=text-align:center>16 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen Research</td></tr><tr><td style=text-align:left>Qwen2.5-7B</td><td style=text-align:center>7.61B</td><td style=text-align:center>6.53B</td><td style=text-align:center>28</td><td style=text-align:center>28 / 4</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-14B</td><td style=text-align:center>14.7B</td><td style=text-align:center>13.1B</td><td style=text-align:center>48</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-32B</td><td style=text-align:center>32.5B</td><td style=text-align:center>31.0B</td><td style=text-align:center>64</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-72B</td><td style=text-align:center>72.7B</td><td style=text-align:center>70.0B</td><td style=text-align:center>80</td><td style=text-align:center>64 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen</td></tr></tbody></table><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><p>This section presents the performance metrics for both base language models and instruction-tuned models across various benchmark evaluations, encompassing a diverse array of domains and tasks.</p><h2 id=qwen25-base-language-model-evaluation>Qwen2.5 Base Language Model Evaluation<a hidden class=anchor aria-hidden=true href=#qwen25-base-language-model-evaluation>#</a></h2><p>The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.</p><p>The evaluation datasets include:</p><p><strong>General Tasks</strong>: MMLU (5-shot), MMLU-Pro (5-shot), MMLU-redux (5-shot), BBH (3-shot), ARC-C (25-shot), TruthfulQA (0-shot), Winogrande (5-shot), HellaSwag (10-shot)</p><p><strong>Math & Science Tasks</strong>: GPQA (5-shot), Theorem QA (5-shot), GSM8K (4-shot), MATH (4-shot)</p><p><strong>Coding Tasks</strong>: HumanEval (0-shot), HumanEval+ (0-shot), MBPP (0-shot), MBPP+ (0-shot), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p><p><strong>Multilingual Tasks</strong>: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p><h3 id=qwen25-72b-performance>Qwen2.5-72B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-72b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-70B</th><th style=text-align:center>Mixtral-8x22B</th><th style=text-align:center>Llama-3-405B</th><th style=text-align:center>Qwen2-72B</th><th style=text-align:center><strong>Qwen2.5-72B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>79.5</td><td style=text-align:center>77.8</td><td style=text-align:center>85.2</td><td style=text-align:center>84.2</td><td style=text-align:center><strong>86.1</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>52.8</td><td style=text-align:center>51.6</td><td style=text-align:center><strong>61.6</strong></td><td style=text-align:center>55.7</td><td style=text-align:center>58.1</td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>75.0</td><td style=text-align:center>72.9</td><td style=text-align:center>-</td><td style=text-align:center>80.5</td><td style=text-align:center><strong>83.9</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>81.0</td><td style=text-align:center>78.9</td><td style=text-align:center>85.9</td><td style=text-align:center>82.4</td><td style=text-align:center><strong>86.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>68.8</td><td style=text-align:center>70.7</td><td style=text-align:center>-</td><td style=text-align:center>68.9</td><td style=text-align:center><strong>72.4</strong></td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>45.6</td><td style=text-align:center>51.0</td><td style=text-align:center>-</td><td style=text-align:center>54.8</td><td style=text-align:center><strong>60.4</strong></td></tr><tr><td style=text-align:left>WindoGrande</td><td style=text-align:center>85.3</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.7</strong></td><td style=text-align:center>85.1</td><td style=text-align:center>83.9</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center>88.0</td><td style=text-align:center><strong>88.7</strong></td><td style=text-align:center>-</td><td style=text-align:center>87.3</td><td style=text-align:center>87.6</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>36.3</td><td style=text-align:center>34.3</td><td style=text-align:center>-</td><td style=text-align:center>37.4</td><td style=text-align:center><strong>45.9</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>32.3</td><td style=text-align:center>35.9</td><td style=text-align:center>-</td><td style=text-align:center><strong>42.8</strong></td><td style=text-align:center>42.4</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>42.5</td><td style=text-align:center>41.7</td><td style=text-align:center>53.8</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>62.1</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>73.7</td><td style=text-align:center>71.7</td><td style=text-align:center>-</td><td style=text-align:center>79.6</td><td style=text-align:center><strong>82.7</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>77.6</td><td style=text-align:center>83.7</td><td style=text-align:center>89.0</td><td style=text-align:center>89.0</td><td style=text-align:center><strong>91.5</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>48.2</td><td style=text-align:center>46.3</td><td style=text-align:center><strong>61.0</strong></td><td style=text-align:center>64.6</td><td style=text-align:center>59.1</td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>42.1</td><td style=text-align:center>40.2</td><td style=text-align:center>-</td><td style=text-align:center><strong>56.1</strong></td><td style=text-align:center>51.2</td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>70.4</td><td style=text-align:center>71.7</td><td style=text-align:center>73.0</td><td style=text-align:center>76.9</td><td style=text-align:center><strong>84.7</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>58.4</td><td style=text-align:center>58.1</td><td style=text-align:center>-</td><td style=text-align:center>63.9</td><td style=text-align:center><strong>69.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>46.3</td><td style=text-align:center>46.7</td><td style=text-align:center>-</td><td style=text-align:center>59.6</td><td style=text-align:center><strong>60.5</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>70.0</td><td style=text-align:center>63.5</td><td style=text-align:center>-</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>78.7</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>79.9</td><td style=text-align:center>77.7</td><td style=text-align:center>-</td><td style=text-align:center>80.7</td><td style=text-align:center><strong>89.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>67.1</td><td style=text-align:center>62.9</td><td style=text-align:center>-</td><td style=text-align:center>76.0</td><td style=text-align:center><strong>76.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>38.0</td><td style=text-align:center>23.3</td><td style=text-align:center>-</td><td style=text-align:center>37.8</td><td style=text-align:center><strong>39.0</strong></td></tr></tbody></table><p>The Qwen2.5-72B base model significantly outperforms its peers in the same category across a wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges.</p><h3 id=qwen25-14b32b-performance>Qwen2.5-14B/32B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-14b32b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen1.5-32B</th><th style=text-align:center>Gemma2-27B</th><th style=text-align:center>Yi-1.5-34B</th><th style=text-align:center>Qwen2-57B-A14B</th><th style=text-align:center><strong>Qwen2.5-14B</strong></th><th style=text-align:center><strong>Qwen2.5-32B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>74.3</td><td style=text-align:center>75.2</td><td style=text-align:center>77.2</td><td style=text-align:center>76.5</td><td style=text-align:center>79.7</td><td style=text-align:center><strong>83.3</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>44.1</td><td style=text-align:center>49.1</td><td style=text-align:center>48.3</td><td style=text-align:center>43.0</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>55.1</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>69.0</td><td style=text-align:center>-</td><td style=text-align:center>74.1</td><td style=text-align:center>72.4</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>82.0</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>66.8</td><td style=text-align:center>74.9</td><td style=text-align:center>76.4</td><td style=text-align:center>67.0</td><td style=text-align:center>78.2</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>63.6</td><td style=text-align:center><strong>71.4</strong></td><td style=text-align:center>65.6</td><td style=text-align:center>64.1</td><td style=text-align:center>67.3</td><td style=text-align:center>70.4</td></tr><tr><td style=text-align:left>Truthfulqa</td><td style=text-align:center>57.4</td><td style=text-align:center>40.1</td><td style=text-align:center>53.9</td><td style=text-align:center>57.7</td><td style=text-align:center><strong>58.4</strong></td><td style=text-align:center>57.8</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>81.5</td><td style=text-align:center>59.7</td><td style=text-align:center><strong>84.9</strong></td><td style=text-align:center>79.5</td><td style=text-align:center>-</td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.4</strong></td><td style=text-align:center>85.9</td><td style=text-align:center>85.2</td><td style=text-align:center>-</td><td style=text-align:center>85.2</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>30.8</td><td style=text-align:center>34.9</td><td style=text-align:center>37.4</td><td style=text-align:center>34.3</td><td style=text-align:center>32.8</td><td style=text-align:center><strong>48.0</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>28.8</td><td style=text-align:center>35.8</td><td style=text-align:center>40.0</td><td style=text-align:center>33.5</td><td style=text-align:center>43.0</td><td style=text-align:center><strong>44.1</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>36.1</td><td style=text-align:center>42.7</td><td style=text-align:center>41.7</td><td style=text-align:center>43.0</td><td style=text-align:center>55.6</td><td style=text-align:center><strong>57.7</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>66.5</td><td style=text-align:center>71.0</td><td style=text-align:center>72.6</td><td style=text-align:center>69.8</td><td style=text-align:center>76.4</td><td style=text-align:center><strong>80.9</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>78.5</td><td style=text-align:center>81.1</td><td style=text-align:center>81.7</td><td style=text-align:center>80.7</td><td style=text-align:center>90.2</td><td style=text-align:center><strong>92.9</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>43.3</td><td style=text-align:center>54.9</td><td style=text-align:center>46.3</td><td style=text-align:center>53.0</td><td style=text-align:center>56.7</td><td style=text-align:center><strong>58.5</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>52.4</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>64.2</td><td style=text-align:center>75.7</td><td style=text-align:center>65.5</td><td style=text-align:center>71.9</td><td style=text-align:center>76.7</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>53.9</td><td style=text-align:center>60.2</td><td style=text-align:center>55.4</td><td style=text-align:center>57.4</td><td style=text-align:center>63.2</td><td style=text-align:center><strong>67.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>38.5</td><td style=text-align:center>48.0</td><td style=text-align:center>39.5</td><td style=text-align:center>49.8</td><td style=text-align:center>53.5</td><td style=text-align:center><strong>59.4</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>61.6</td><td style=text-align:center>65.8</td><td style=text-align:center>58.3</td><td style=text-align:center>65.5</td><td style=text-align:center>70.6</td><td style=text-align:center><strong>75.4</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>76.5</td><td style=text-align:center>82.2</td><td style=text-align:center>73.9</td><td style=text-align:center>77.0</td><td style=text-align:center>85.9</td><td style=text-align:center><strong>88.4</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>56.1</td><td style=text-align:center>61.6</td><td style=text-align:center>49.3</td><td style=text-align:center>62.3</td><td style=text-align:center>68.5</td><td style=text-align:center><strong>73.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>33.5</td><td style=text-align:center>38.7</td><td style=text-align:center>30.0</td><td style=text-align:center>34.5</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>37.3</strong></td></tr></tbody></table><p>The Qwen2.5-14B model demonstrates a solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP.</p><h3 id=qwen25-7b-performance>Qwen2.5-7B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-7b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mistral-7B</th><th style=text-align:center>Llama3-8B</th><th style=text-align:center>Gemma2-9B</th><th style=text-align:center>Qwen2-7B</th><th style=text-align:center><strong>Qwen2.5-7B</strong></th></tr></thead><tbody><tr><td style=text-align:left>#Non-emb Params</td><td style=text-align:center>7.0B</td><td style=text-align:center>7.0B</td><td style=text-align:center>8.2B</td><td style=text-align:center>6.5B</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>64.2</td><td style=text-align:center>66.6</td><td style=text-align:center>71.3</td><td style=text-align:center>70.3</td><td style=text-align:center><strong>74.2</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>30.9</td><td style=text-align:center>35.4</td><td style=text-align:center>44.7</td><td style=text-align:center>40.1</td><td style=text-align:center><strong>45.0</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>58.1</td><td style=text-align:center>61.6</td><td style=text-align:center>67.9</td><td style=text-align:center>68.1</td><td style=text-align:center><strong>71.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>56.1</td><td style=text-align:center>57.7</td><td style=text-align:center>68.2</td><td style=text-align:center>62.3</td><td style=text-align:center><strong>70.4</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>60.0</td><td style=text-align:center>59.3</td><td style=text-align:center><strong>68.2</strong></td><td style=text-align:center>60.6</td><td style=text-align:center>63.7</td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>42.2</td><td style=text-align:center>44.0</td><td style=text-align:center>45.3</td><td style=text-align:center>54.2</td><td style=text-align:center><strong>56.4</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>78.4</td><td style=text-align:center>77.4</td><td style=text-align:center><strong>79.5</strong></td><td style=text-align:center>77.0</td><td style=text-align:center>75.9</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center><strong>83.3</strong></td><td style=text-align:center>82.1</td><td style=text-align:center>81.9</td><td style=text-align:center>80.7</td><td style=text-align:center>80.2</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>24.7</td><td style=text-align:center>25.8</td><td style=text-align:center>32.8</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>36.4</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>19.2</td><td style=text-align:center>22.1</td><td style=text-align:center>28.9</td><td style=text-align:center>29.6</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>10.2</td><td style=text-align:center>20.5</td><td style=text-align:center>37.7</td><td style=text-align:center>43.5</td><td style=text-align:center><strong>49.8</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>50.1</td><td style=text-align:center>55.3</td><td style=text-align:center>65.1</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>72.3</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.2</td><td style=text-align:center>55.3</td><td style=text-align:center>70.7</td><td style=text-align:center>80.2</td><td style=text-align:center><strong>85.4</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>33.5</td><td style=text-align:center>37.8</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>57.9</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>24.4</td><td style=text-align:center>29.3</td><td style=text-align:center>30.5</td><td style=text-align:center>43.3</td><td style=text-align:center><strong>50.6</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>51.1</td><td style=text-align:center>53.9</td><td style=text-align:center>62.2</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>74.9</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>40.9</td><td style=text-align:center>44.4</td><td style=text-align:center>50.6</td><td style=text-align:center>51.9</td><td style=text-align:center><strong>62.9</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>29.4</td><td style=text-align:center>22.6</td><td style=text-align:center>34.9</td><td style=text-align:center>41.0</td><td style=text-align:center><strong>50.3</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>47.1</td><td style=text-align:center>52.3</td><td style=text-align:center><strong>61.2</strong></td><td style=text-align:center>59.2</td><td style=text-align:center>59.4</td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>63.3</td><td style=text-align:center>68.6</td><td style=text-align:center>78.3</td><td style=text-align:center>72.0</td><td style=text-align:center><strong>79.3</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>26.3</td><td style=text-align:center>36.3</td><td style=text-align:center>53.0</td><td style=text-align:center>57.5</td><td style=text-align:center><strong>57.8</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>23.3</td><td style=text-align:center>31.9</td><td style=text-align:center><strong>36.5</strong></td><td style=text-align:center>31.5</td><td style=text-align:center>32.4</td></tr></tbody></table><p>The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.</p><h3 id=qwen25-05b15b3b-performance>Qwen2.5-0.5B/1.5B/3B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b3b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center><strong>Qwen2.5-0.5B</strong></th><th style=text-align:center>Qwen2-1.5B</th><th style=text-align:center><strong>Qwen2.5-1.5B</strong></th><th style=text-align:center>Gemma2-2.6B</th><th style=text-align:center><strong>Qwen2.5-3B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>44.3</td><td style=text-align:center>47.5</td><td style=text-align:center>55.9</td><td style=text-align:center>60.9</td><td style=text-align:center>52.2</td><td style=text-align:center><strong>65.6</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>14.7</td><td style=text-align:center>15.7</td><td style=text-align:center>21.6</td><td style=text-align:center>28.5</td><td style=text-align:center>23.0</td><td style=text-align:center><strong>34.6</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>40.7</td><td style=text-align:center>45.1</td><td style=text-align:center>51.8</td><td style=text-align:center>58.5</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>63.7</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>18.2</td><td style=text-align:center>20.3</td><td style=text-align:center>36.5</td><td style=text-align:center>45.1</td><td style=text-align:center>41.9</td><td style=text-align:center><strong>56.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>31.0</td><td style=text-align:center>35.6</td><td style=text-align:center>43.7</td><td style=text-align:center>54.7</td><td style=text-align:center>55.7</td><td style=text-align:center><strong>56.5</strong></td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>39.7</td><td style=text-align:center>40.2</td><td style=text-align:center>45.9</td><td style=text-align:center>46.6</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>56.9</td><td style=text-align:center>56.3</td><td style=text-align:center>65.0</td><td style=text-align:center>65.0</td><td style=text-align:center><strong>71.5</strong></td><td style=text-align:center>71.1</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>49.1</td><td style=text-align:center>52.1</td><td style=text-align:center>67.0</td><td style=text-align:center>67.9</td><td style=text-align:center>74.6</td><td style=text-align:center><strong>74.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>29.8</td><td style=text-align:center>24.8</td><td style=text-align:center>20.7</td><td style=text-align:center>24.2</td><td style=text-align:center>25.3</td><td style=text-align:center><strong>26.3</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>9.6</td><td style=text-align:center>16.0</td><td style=text-align:center>14.8</td><td style=text-align:center>22.1</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>27.4</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>11.2</td><td style=text-align:center>19.5</td><td style=text-align:center>21.6</td><td style=text-align:center>35.0</td><td style=text-align:center>18.3</td><td style=text-align:center><strong>42.6</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>27.5</td><td style=text-align:center>39.8</td><td style=text-align:center>42.7</td><td style=text-align:center>54.8</td><td style=text-align:center>45.8</td><td style=text-align:center><strong>62.5</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.4</td><td style=text-align:center>41.6</td><td style=text-align:center>46.9</td><td style=text-align:center>68.5</td><td style=text-align:center>30.3</td><td style=text-align:center><strong>79.1</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>22.6</td><td style=text-align:center>30.5</td><td style=text-align:center>34.8</td><td style=text-align:center>37.2</td><td style=text-align:center>19.5</td><td style=text-align:center><strong>42.1</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>18.9</td><td style=text-align:center>26.8</td><td style=text-align:center>29.9</td><td style=text-align:center>32.9</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>33.1</td><td style=text-align:center>39.3</td><td style=text-align:center>46.9</td><td style=text-align:center><strong>60.2</strong></td><td style=text-align:center>42.1</td><td style=text-align:center>57.1</td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>27.6</td><td style=text-align:center>33.8</td><td style=text-align:center>37.6</td><td style=text-align:center><strong>49.6</strong></td><td style=text-align:center>33.6</td><td style=text-align:center>49.4</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>16.3</td><td style=text-align:center>18.9</td><td style=text-align:center>27.9</td><td style=text-align:center>33.1</td><td style=text-align:center>17.6</td><td style=text-align:center><strong>41.2</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>29.4</td><td style=text-align:center>30.8</td><td style=text-align:center>43.1</td><td style=text-align:center>47.9</td><td style=text-align:center>38.1</td><td style=text-align:center><strong>54.6</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>40.4</td><td style=text-align:center>41.0</td><td style=text-align:center>50.7</td><td style=text-align:center>65.1</td><td style=text-align:center>46.8</td><td style=text-align:center><strong>76.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>7.8</td><td style=text-align:center>13.5</td><td style=text-align:center>21.3</td><td style=text-align:center>37.5</td><td style=text-align:center>18.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>14.1</td><td style=text-align:center>15.3</td><td style=text-align:center>23.8</td><td style=text-align:center>25.0</td><td style=text-align:center>26.9</td><td style=text-align:center><strong>29.3</strong></td></tr></tbody></table><p>For edge-side models, Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.</p><h2 id=instruction-tuned-model-evaluation>Instruction-tuned Model Evaluation<a hidden class=anchor aria-hidden=true href=#instruction-tuned-model-evaluation>#</a></h2><p>The evaluation of instruction-tuned models mainly focuses on the model performance of natural language understanding, general question answering, reasoning, coding, mathematics, instruction following, human alignment, etc.</p><p>The datasets for evaluation include:</p><p><strong>General Tasks</strong>: MMLU-Pro, MMLU-redux</p><p><strong>Math & Science Tasks</strong>: GPQA, GSM8K, MATH</p><p><strong>Coding Tasks</strong>: HumanEval, MBPP, MultiPL-E, LiveCodeBench 2305-2409, LiveBench 0831</p><p><strong>Instruction & Alignment Tasks</strong>: IFeval strict-prompt, Arena-Hard, AlignBench v1.1, MTbench</p><h3 id=qwen25-72b-instruct-performance>Qwen2.5-72B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-72b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Mistral-Large2 Instruct</th><th>Llama-3.1-70B-Instruct</th><th>Llama-3.1-405B-Instruct</th><th>Qwen2-72B-Instruct</th><th><strong>Qwen2.5-72B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>69.4</td><td>66.4</td><td><strong>73.3</strong></td><td>64.4</td><td>71.1</td></tr><tr><td>MMLU-redux</td><td>83.0</td><td>83.0</td><td>86.2</td><td>81.6</td><td><strong>86.8</strong></td></tr><tr><td>GPQA</td><td><strong>52.0</strong></td><td>46.7</td><td>51.1</td><td>42.4</td><td>49.0</td></tr><tr><td>MATH</td><td>69.9</td><td>68.0</td><td>73.8</td><td>69.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>92.7</td><td>95.1</td><td><strong>96.8</strong></td><td>93.2</td><td>95.8</td></tr><tr><td>HumanEval</td><td><strong>92.1</strong></td><td>80.5</td><td>89.0</td><td>86.0</td><td>86.6</td></tr><tr><td>MBPP</td><td>80.0</td><td>84.2</td><td>84.5</td><td>80.2</td><td><strong>88.2</strong></td></tr><tr><td>MultiPL-E</td><td><strong>76.9</strong></td><td>68.2</td><td>73.5</td><td>69.2</td><td>75.1</td></tr><tr><td>LiveCodeBench 2305-2409</td><td>42.2</td><td>32.1</td><td>41.6</td><td>32.2</td><td><strong>55.5</strong></td></tr><tr><td>LiveBench 0831</td><td>48.5</td><td>46.6</td><td><strong>53.2</strong></td><td>41.5</td><td>52.3</td></tr><tr><td>IFeval strict-prompt</td><td>64.1</td><td>83.6</td><td><strong>86.0</strong></td><td>77.6</td><td>84.1</td></tr><tr><td>Arena-Hard</td><td>73.1</td><td>55.7</td><td>69.3</td><td>48.1</td><td><strong>81.2</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.69</td><td>5.94</td><td>5.95</td><td>8.15</td><td><strong>8.16</strong></td></tr><tr><td>MTbench</td><td>8.61</td><td>8.79</td><td>9.08</td><td>9.12</td><td><strong>9.35</strong></td></tr></tbody></table><p>The Qwen2.5-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B in several critical tasks. Qwen2.5-72B-Instruct excels in mathematics (MATH: 83.1), coding (LiveCodeBench: 55.5), and chatting (Arena-Hard: 81.2). Compared to its base model Qwen2.5-72B and its predecessor Qwen2-72B-Instruct, the Qwen2.5-72B-Instruct showcases comprehensive improvements across all tasks.</p><h3 id=qwen-turbo--qwen25-14b-instruct--qwen25-32b-instruct-performance>Qwen-Turbo & Qwen2.5-14B-Instruct & Qwen2.5-32B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen-turbo--qwen25-14b-instruct--qwen25-32b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Qwen2-57B-A14B-Instruct</th><th>Gemma2-27B-IT</th><th>GPT4o-mini</th><th><strong>Qwen-Turbo</strong></th><th><strong>Qwen2.5-14B-Instruct</strong></th><th><strong>Qwen2.5-32B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.8</td><td>55.5</td><td>63.1</td><td>64.8</td><td>63.7</td><td><strong>69.0</strong></td></tr><tr><td>MMLU-redux</td><td>72.6</td><td>75.7</td><td>81.5</td><td>80.4</td><td>80.0</td><td><strong>83.9</strong></td></tr><tr><td>GPQA</td><td>34.3</td><td>38.4</td><td>40.2</td><td>44.4</td><td>45.5</td><td><strong>49.5</strong></td></tr><tr><td>MATH</td><td>49.1</td><td>54.4</td><td>70.2</td><td>81.0</td><td>80.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>85.3</td><td>90.4</td><td>93.2</td><td>93.6</td><td>94.8</td><td><strong>95.9</strong></td></tr><tr><td>HumanEval</td><td>79.9</td><td>78.7</td><td><strong>88.4</strong></td><td>86.6</td><td>83.5</td><td><strong>88.4</strong></td></tr><tr><td>MBPP</td><td>70.9</td><td>81.0</td><td><strong>85.7</strong></td><td>80.2</td><td>82.0</td><td>84.0</td></tr><tr><td>MultiPL-E</td><td>66.4</td><td>67.4</td><td>75.0</td><td>73.0</td><td>72.8</td><td><strong>75.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>22.5</td><td>-</td><td>40.7</td><td>43.1</td><td>42.6</td><td><strong>51.2</strong></td></tr><tr><td>LiveBench 0831</td><td>31.1</td><td>39.6</td><td>43.3</td><td>41.6</td><td>44.4</td><td><strong>50.7</strong></td></tr><tr><td>IFeval strict-prompt</td><td>59.9</td><td>77.1</td><td>80.4</td><td>74.9</td><td><strong>81.0</strong></td><td>79.5</td></tr><tr><td>Arena-Hard</td><td>17.8</td><td>57.5</td><td><strong>74.9</strong></td><td>68.4</td><td>68.3</td><td>74.5</td></tr><tr><td>AlignBench v1.1</td><td>7.02</td><td>7.22</td><td>7.81</td><td><strong>7.99</strong></td><td>7.94</td><td>7.93</td></tr><tr><td>MTbench</td><td>8.55</td><td>9.10</td><td>-</td><td>8.86</td><td>8.88</td><td><strong>9.20</strong></td></tr></tbody></table><p>The Qwen2.5-32B-Instruct model demonstrates superior performance across most tasks when compared to other models of similar size. In comparison to GPT-4o-mini, our open-source model, Qwen2.5-14B-Instruct, along with our API model, Qwen-Turbo, also deliver competitive results across all benchmarks.</p><h3 id=qwen25-7b-instruct-performance>Qwen2.5-7B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-7b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Gemma2-9b-IT</th><th>Llama3.1-8B-Instruct</th><th>Qwen2-7B-Instruct</th><th><strong>Qwen2.5-7B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.1</td><td>48.3</td><td>44.1</td><td><strong>56.3</strong></td></tr><tr><td>MMLU-redux</td><td>72.8</td><td>67.2</td><td>67.3</td><td><strong>75.4</strong></td></tr><tr><td>GPQA</td><td>32.8</td><td>32.8</td><td>34.3</td><td><strong>36.4</strong></td></tr><tr><td>MATH</td><td>44.3</td><td>51.9</td><td>52.9</td><td><strong>75.5</strong></td></tr><tr><td>GSM8K</td><td>76.7</td><td>84.5</td><td>85.7</td><td><strong>91.6</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td>79.9</td><td><strong>84.8</strong></td></tr><tr><td>MBPP</td><td>74.9</td><td>69.6</td><td>67.2</td><td><strong>79.2</strong></td></tr><tr><td>MultiPL-E</td><td>53.4</td><td>50.7</td><td>59.1</td><td><strong>70.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>18.9</td><td>8.3</td><td>23.9</td><td><strong>28.7</strong></td></tr><tr><td>LiveBench 0831</td><td>30.6</td><td>26.7</td><td>29.2</td><td><strong>35.9</strong></td></tr><tr><td>IFeval strict-prompt</td><td>70.1</td><td><strong>75.9</strong></td><td>54.7</td><td>71.2</td></tr><tr><td>Arena-Hard</td><td>41.6</td><td>27.8</td><td>25.0</td><td><strong>52.0</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.05</td><td>4.75</td><td>7.13</td><td><strong>7.33</strong></td></tr><tr><td>MTbench</td><td>8.49</td><td>8.23</td><td>8.26</td><td><strong>8.75</strong></td></tr></tbody></table><p>The Qwen2.5-7B-Instruct model significantly outperforms its competitors, Gemma2-9b-IT and Llama3.1-8B-Instruct, across all tasks except IFeval. Notably, Qwen2.5-7B-Instruct demonstrates clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8).</p><h3 id=qwen25-3b-instruct-performance>Qwen2.5-3B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-3b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Gemma2-2B-IT</th><th>Phi3.5-mini-Instruct</th><th>MiniCPM3-4B</th><th><strong>Qwen2.5-3B-Instruct</strong></th></tr></thead><tbody><tr><td>Non-Emb Params</td><td>2.0B</td><td>3.6B</td><td>4.0B</td><td>2.8B</td></tr><tr><td>MMLU-Pro</td><td>26.7</td><td><strong>47.5</strong></td><td>43.0</td><td>43.7</td></tr><tr><td>MMLU-redux</td><td>51.9</td><td><strong>67.7</strong></td><td>59.9</td><td>64.4</td></tr><tr><td>GPQA</td><td>29.3</td><td>27.2</td><td><strong>31.3</strong></td><td>30.3</td></tr><tr><td>MATH</td><td>26.6</td><td>48.5</td><td>46.6</td><td><strong>65.9</strong></td></tr><tr><td>GSM8K</td><td>63.2</td><td>86.2</td><td>81.1</td><td><strong>86.7</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td><strong>74.4</strong></td><td><strong>74.4</strong></td></tr><tr><td>MBPP</td><td><strong>74.9</strong></td><td>63.2</td><td>72.5</td><td>72.7</td></tr><tr><td>MultiPL-E</td><td>30.5</td><td>47.2</td><td>49.1</td><td><strong>60.2</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>5.8</td><td>15.8</td><td><strong>23.8</strong></td><td>19.9</td></tr><tr><td>LiveBench 0831</td><td>20.1</td><td>27.4</td><td><strong>27.6</strong></td><td>26.8</td></tr><tr><td>IFeval strict-prompt</td><td>51.0</td><td>52.1</td><td><strong>68.4</strong></td><td>58.2</td></tr></tbody></table><p>As for the edge-side instruction model, the Qwen2.5-3B-Instruct model has fewer parameters than both the Phi3.5-mini-Instruct and MiniCPM3-4B models. Despite this, it outperforms them in mathematics and coding tasks while delivering competitive results in language understanding.</p><h3 id=qwen25-05b15b-instruct-performance>Qwen2.5-0.5B/1.5B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Qwen2-0.5B-Instruct</th><th><strong>Qwen2.5-0.5B-Instruct</strong></th><th>Qwen2-1.5B-Instruct</th><th><strong>Qwen2.5-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>14.4</td><td><strong>15.0</strong></td><td>22.9</td><td><strong>32.4</strong></td></tr><tr><td>MMLU-redux</td><td>12.9</td><td><strong>24.1</strong></td><td>41.2</td><td><strong>50.7</strong></td></tr><tr><td>GPQA</td><td>23.7</td><td><strong>29.8</strong></td><td>21.2</td><td><strong>29.8</strong></td></tr><tr><td>MATH</td><td>13.9</td><td><strong>34.4</strong></td><td>25.3</td><td><strong>55.2</strong></td></tr><tr><td>GSM8K</td><td>40.1</td><td><strong>49.6</strong></td><td>61.6</td><td><strong>73.2</strong></td></tr><tr><td>HumanEval</td><td>31.1</td><td><strong>35.4</strong></td><td>42.1</td><td><strong>61.6</strong></td></tr><tr><td>MBPP</td><td>39.7</td><td><strong>49.6</strong></td><td>44.2</td><td><strong>63.2</strong></td></tr><tr><td>MultiPL-E</td><td>20.8</td><td><strong>28.5</strong></td><td>38.5</td><td><strong>50.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>1.6</td><td><strong>5.1</strong></td><td>4.5</td><td><strong>14.8</strong></td></tr><tr><td>LiveBench 0831</td><td>7.4</td><td><strong>12.6</strong></td><td>12.4</td><td><strong>18.8</strong></td></tr><tr><td>IFeval strict-prompt</td><td>14.6</td><td><strong>27.9</strong></td><td>29.0</td><td><strong>42.5</strong></td></tr></tbody></table><p>Qwen2.5-1.5B-Instruct and Qwen2.5-0.5B-Instruct have seen large performance improvements over their previous versions, making them well-suited for edge-side applications in highly resource-constrained environments.</p><h3 id=performances-on-multilingualism>Performances on Multilingualism<a hidden class=anchor aria-hidden=true href=#performances-on-multilingualism>#</a></h3><p>To evaluate the multilingual performance of instruction-tuned models, we collect and extend benchmarks as follows:</p><ul><li><strong>IFEval (multilingual)</strong>: We translate the examples from IFEval (English version) to construct multilingual IFEval examples after removing examples with language-specific contents (e.g., &ldquo;start with letter A&rdquo;). We collect 100 examples for each language among Arabic (ar), Spanish (es), French (fr), Indonesian (in), Japanese (ja), Korean (ko), Portuguese (pt), and Vietnamese (vi) languages. All examples are checked and post-edited (if neccessary) by paid volunteers.</li><li><strong>Knowledge</strong>: We use 5 MMLU-like benchmarks (multi-choice) to testify the knowledge utilization ability of Qwen2.5 series models on multilingualism, including AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Also, we present the performances on translated MMLU (i.e., okapi_MMLU, from English to multiple languages).</li><li><strong>MGSM8K (extended)</strong>: Aside from the examples in the original MGSM8K benchmark, we extend the language support with Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). We translate 250 examples (same as the other languages engaged in MGSM8K) into those 4 languages. All examples are also checked and post-edited (if necessary) by paid volunteers.</li><li><strong>Cultural Nuances</strong>: We also use BLEnD, a benchmark aiming at testifying cultural nuances of LLMs, to testify LLMs from the Qwen2.5 series.</li></ul><table><thead><tr><th style=text-align:center>Datasets</th><th style=text-align:center>Qwen2-72B-Instruct</th><th style=text-align:center>Llama3.1-70B-Instruct</th><th style=text-align:center>Qwen2.5-32B-Instruct</th><th style=text-align:center>Mistral-Large-Instruct-2407 (123B)</th><th style=text-align:center>GPT4o-mini</th><th style=text-align:center>Qwen2.5-72B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>Instruction Following</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval (multilingual)</td><td style=text-align:center>79.69</td><td style=text-align:center>80.47</td><td style=text-align:center>82.68</td><td style=text-align:center>82.69</td><td style=text-align:center>85.03</td><td style=text-align:center><strong>86.98</strong></td></tr><tr><td style=text-align:center><em><strong>Knowledge</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU (Arabic)</td><td style=text-align:center>68.85</td><td style=text-align:center>70.08</td><td style=text-align:center>70.44</td><td style=text-align:center>69.24</td><td style=text-align:center>69.73</td><td style=text-align:center><strong>72.44</strong></td></tr><tr><td style=text-align:center>JMMLU (Japanese)</td><td style=text-align:center>77.37</td><td style=text-align:center>73.89</td><td style=text-align:center>76.55</td><td style=text-align:center>75.77</td><td style=text-align:center>73.74</td><td style=text-align:center><strong>80.56</strong></td></tr><tr><td style=text-align:center>KMMLU (Korean)</td><td style=text-align:center>57.04</td><td style=text-align:center>53.23</td><td style=text-align:center>60.75</td><td style=text-align:center>56.42</td><td style=text-align:center>56.77</td><td style=text-align:center><strong>61.96</strong></td></tr><tr><td style=text-align:center>IndoMMLU (Indonesian)</td><td style=text-align:center>66.31</td><td style=text-align:center>67.50</td><td style=text-align:center>66.42</td><td style=text-align:center>63.21</td><td style=text-align:center>67.75</td><td style=text-align:center><strong>69.25</strong></td></tr><tr><td style=text-align:center>TurkishMMLU (Turkish)</td><td style=text-align:center>69.22</td><td style=text-align:center>66.89</td><td style=text-align:center>72.41</td><td style=text-align:center>64.78</td><td style=text-align:center>71.19</td><td style=text-align:center><strong>76.12</strong></td></tr><tr><td style=text-align:center>okapi MMLU (translated)</td><td style=text-align:center>77.84</td><td style=text-align:center>76.49</td><td style=text-align:center>77.16</td><td style=text-align:center>78.37</td><td style=text-align:center>73.44</td><td style=text-align:center><strong>79.97</strong></td></tr><tr><td style=text-align:center><em><strong>Math Reasoning</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K (extended)</td><td style=text-align:center>82.72</td><td style=text-align:center>73.31</td><td style=text-align:center>87.15</td><td style=text-align:center><strong>89.01</strong></td><td style=text-align:center>87.36</td><td style=text-align:center>88.16</td></tr><tr><td style=text-align:center><em><strong>Cultural Nuances</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>25.90</td><td style=text-align:center>30.49</td><td style=text-align:center>27.88</td><td style=text-align:center>33.47</td><td style=text-align:center><strong>35.91</strong></td><td style=text-align:center>32.48</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Datasets</th><th style=text-align:center>Qwen2-7B-Instruct</th><th style=text-align:center>Llama3.1-8B-Instruct</th><th style=text-align:center>Qwen2.5-7B-Instruct</th><th style=text-align:center>Gemma-2-9B-Instruct</th><th style=text-align:center>Mistral-Nemo-Instruct-2407 (12B)</th><th style=text-align:center>Qwen2.5-14B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>Instruction Following</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval (multilingual)</td><td style=text-align:center>51.43</td><td style=text-align:center>60.68</td><td style=text-align:center>74.87</td><td style=text-align:center><strong>77.47</strong></td><td style=text-align:center>64.59</td><td style=text-align:center>77.08</td></tr><tr><td style=text-align:center><em><strong>Knowledge</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU (Arabic)</td><td style=text-align:center>54.87</td><td style=text-align:center>54.28</td><td style=text-align:center>59.78</td><td style=text-align:center>60.26</td><td style=text-align:center>53.92</td><td style=text-align:center><strong>66.81</strong></td></tr><tr><td style=text-align:center>JMMLU (Japanese)</td><td style=text-align:center>57.71</td><td style=text-align:center>53.26</td><td style=text-align:center>61.88</td><td style=text-align:center>64.59</td><td style=text-align:center>55.17</td><td style=text-align:center><strong>72.78</strong></td></tr><tr><td style=text-align:center>KMMLU (Korean)</td><td style=text-align:center>43.96</td><td style=text-align:center>42.28</td><td style=text-align:center>46.59</td><td style=text-align:center>46.24</td><td style=text-align:center>42.22</td><td style=text-align:center><strong>59.71</strong></td></tr><tr><td style=text-align:center>IndoMMLU (Indonesian)</td><td style=text-align:center>54.05</td><td style=text-align:center>53.92</td><td style=text-align:center>56.42</td><td style=text-align:center>61.73</td><td style=text-align:center>50.76</td><td style=text-align:center><strong>65.09</strong></td></tr><tr><td style=text-align:center>TurkishMMLU (Turkish)</td><td style=text-align:center>49.27</td><td style=text-align:center>45.61</td><td style=text-align:center>54.28</td><td style=text-align:center>55.44</td><td style=text-align:center>34.44</td><td style=text-align:center><strong>66.85</strong></td></tr><tr><td style=text-align:center>okapi MMLU (translated)</td><td style=text-align:center>60.47</td><td style=text-align:center>55.18</td><td style=text-align:center>66.98</td><td style=text-align:center>46.72</td><td style=text-align:center>59.65</td><td style=text-align:center><strong>72.12</strong></td></tr><tr><td style=text-align:center><em><strong>Math Reasoning</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K (extended)</td><td style=text-align:center>56.13</td><td style=text-align:center>66.05</td><td style=text-align:center>66.11</td><td style=text-align:center>78.37</td><td style=text-align:center>54.75</td><td style=text-align:center><strong>82.27</strong></td></tr><tr><td style=text-align:center><em><strong>Cultural Nuances</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>22.49</td><td style=text-align:center>19.47</td><td style=text-align:center>23.66</td><td style=text-align:center><strong>28.31</strong></td><td style=text-align:center>26.61</td><td style=text-align:center>26.99</td></tr></tbody></table><h1 id=demo-cases>Demo Cases<a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h1><p>Here we provide several cases to demonstrate the new or enhanced capabilities of Qwen2.5, including generating JSON output, generating long texts, and understanding structured data.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Generating JSON Output</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>JSON Output</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/json_output.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Structured Data Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Table Understanding</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/table_understanding.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Long Text Generation</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Text Generation</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/long_text.mp4 autoplay></video></figure></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>