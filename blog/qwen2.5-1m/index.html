<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens | Qwen</title><meta name=keywords content><meta name=description content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
Introduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here&rsquo;s what you can expect from this release:
Opensource Models: We&rsquo;re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we&rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5-1m/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-1m/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens"><meta property="og:description" content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
Introduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here&rsquo;s what you can expect from this release:
Opensource Models: We&rsquo;re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we&rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5-1m/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-27T00:00:03+08:00"><meta property="article:modified_time" content="2025-01-27T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens"><meta name=twitter:description content="Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD
Introduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here&rsquo;s what you can expect from this release:
Opensource Models: We&rsquo;re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we&rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens","item":"https://qwenlm.github.io/blog/qwen2.5-1m/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens","name":"Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens","description":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\nIntroduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here\u0026rsquo;s what you can expect from this release:\nOpensource Models: We\u0026rsquo;re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we\u0026rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts.","keywords":[],"articleBody":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\nIntroduction Two months after upgrading Qwen2.5-Turbo to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here’s what you can expect from this release:\nOpensource Models: We’re releasing two new checkpoints, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, marking the first time we’ve upgraded our opensource Qwen models to handle 1M-token contexts.\nInference Framework: To help developers deploy the Qwen2.5-1M series models more efficiently, we’ve fully open-sourced our inference framework based on vLLM. With integration with sparse attention methods, our framework can process 1M-token inputs 3x to 7x faster.\nTechnical Report: We’re also sharing the technical details behind the Qwen2.5-1M series, including design insights for training and inference frameworks, as well as ablation experiments.\nYou can experience Qwen2.5-1M models online by visiting our demo on Huggingface and Modelscope.\nAdditionally, we recently introduced Qwen Chat, an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the Qwen2.5-Turbo model, which supports long-context processing with a context length of up to 1M tokens.\nModel Performance Let’s start by diving into the performance of the Qwen2.5-1M series models, covering both long-context and short text tasks.\nLong-Context Tasks First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.\nFor more complex long-context understanding tasks, we select RULER, LV-Eval, LongbenchChat used in this blog.\nFrom these results, we can draw a few key conclusions:\nSignificantly Superior to the 128k Version: The Qwen2.5-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length. Notable Performance Advantage: The Qwen2.5-14B-Instruct-1M model not only beats Qwen2.5-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks. Short-Context Tasks Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the Qwen2.5-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison.\nHere’s what we find:\nBoth Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven’t been compromised by the addition of long-sequence processing abilities. Compared to GPT-4o-mini, both Qwen2.5-14B-Instruct-1M and Qwen2.5-Turbo achieve similar performance on short text tasks while supporting a context length that’s eight times longer. Key Techniques Here, we’ll briefly introduce the key techniques behind building Qwen2.5-1M. For more details, please check out our technical report.\nLong-Context Training Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for Qwen2.5-1M through multiple stages:\nWe begin with an intermediate checkpoint of pre-trained Qwen2.5, which had a 4K token context length. In Pretraining, we gradually increase the context length from 4K to 256K tokens while using Adjusted Base Frequency, raising the RoPE base from 10,000 to 10,000,000. In Supervised Fine-tuning, we split this into two stages to preserve performance on shorter sequences: Stage 1: Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of Qwen2.5. Stage 2: Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality. In Reinforcement Learning, we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks. The final instruction-tuned models are capable of handling sequences up to 256K tokens.\nLength Extrapolation During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques.\nThe degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ Dual Chunk Attention (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training.\nWe evaluat the Qwen2.5-1M models and their 128K counterparts with and without the length extrapolation method. We can find:\nEven models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required.\nSparse Attention For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on MInference to accelerate the prefill phase. Furthermore, we propose several improvements:\nIntegrating with Chunked Prefill: Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in Qwen2.5-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption.\nIntegrating with Length Extrapolation: We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy.\nSparsity Refinement on Long Sequences: MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention.\nMore Optimizations: We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework.\nWith these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length.\nDeploy Qwen2.5-1M Models Locally Here we provide step-by-step instructions for deploying the Qwen2.5-1M models on your local devices.\n1. System Preparation To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.\nEnsure your system meets the following requirements:\nCUDA Version: 12.1 or 12.3 Python Version: \u003e=3.9 and \u003c=3.12 VRAM Requirement for processing 1 million-token sequences:\nQwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs). Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs). If your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M models for shorter tasks.\n2. Install Dependencies For now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.\ngit clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git cd vllm pip install -e . -v 3. Launch OpenAI-Compatible API Service Use the following command to start the service, configuring it based on your hardware setup:\nvllm serve Qwen/Qwen2.5-7B-Instruct-1M \\ --tensor-parallel-size 4 \\ --max-model-len 1010000 \\ --enable-chunked-prefill --max-num-batched-tokens 131072 \\ --enforce-eager \\ --max-num-seqs 1 # --quantization fp8 # Enabling FP8 quantization for model weights can reduce memory usage. If you encounter any issues, please refer to the Troubleshooting section for more information.\nParameter Explanations:\n--tensor-parallel-size\nSet to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model. --max-model-len\nDefines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues. --max-num-batched-tokens\nSets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference. Recommend 131072 for optimal performance. --max-num-seqs\nLimits concurrent sequences processed. 4. Interact with the Model You can interact with the deployed model using one of the following methods:\nOption 1. Using Curl\ncurl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen/Qwen2.5-7B-Instruct-1M\", \"messages\": [ {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"} ], \"temperature\": 0.7, \"top_p\": 0.8, \"repetition_penalty\": 1.05, \"max_tokens\": 512 }' Option 2. Using Python\nfrom openai import OpenAI openai_api_key = \"EMPTY\" openai_api_base = \"http://localhost:8000/v1\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) prompt = ( \"There is an important info hidden inside a lot of irrelevant text. \" + \"Find it and memorize it. I will quiz you about the important information there.\\n\\n\" + \"The pass key is 28884. Remember it. 28884 is the pass key.\\n\" + \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. \" * 800 + \"\\nWhat is the pass key?\" # The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier. ) chat_response = client.chat.completions.create( model=\"Qwen/Qwen2.5-7B-Instruct-1M\", messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0, ) print(\"Chat response:\", chat_response.choices[0].message.content) Other Options\nFor more advanced use cases, consider exploring frameworks like Qwen-Agent, which enable the model to read PDF files and perform other specialized tasks.\nWhat’s Next? We recognize that long-context models still have a lot of room for improvement. Our goal is to build models that excel in both short and long-context tasks, making sure they bring real value to practical, long-context scenarios. We’re diving deep into more efficient training methods, model architectures, and inference methods to make them deployable effectively and perform exceptionally well even in environments with limited resources. We’re confident that all these efforts will open up a whole new world of possibilities for long-context models, expanding their use across a much broader range of applications. Stay tuned as we keep pushing the boundaries!\n","wordCount":"1589","inLanguage":"en","datePublished":"2025-01-27T00:00:03+08:00","dateModified":"2025-01-27T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5-1m/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens</h1><div class=post-meta><span title='2025-01-27 00:00:03 +0800 +0800'>January 27, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1589 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5-1m/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf class="btn external" target=_blank>Tech Report</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HuggingFace</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>ModelScope</a>
<a href=https://chat.qwenlm.ai/ class="btn external" target=_blank>Qwen Chat</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo class="btn external" target=_blank>HuggingFace Demo</a>
<a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo class="btn external" target=_blank>ModelScope Demo</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Two months after upgrading <a href=../qwen2.5-turbo>Qwen2.5-Turbo</a> to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here&rsquo;s what you can expect from this release:</p><ol><li><p><strong>Opensource Models:</strong> We&rsquo;re releasing two new checkpoints, <strong>Qwen2.5-7B-Instruct-1M</strong> and <strong>Qwen2.5-14B-Instruct-1M</strong>, marking the first time we&rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts.</p></li><li><p><strong>Inference Framework:</strong> To help developers deploy the Qwen2.5-1M series models more efficiently, we&rsquo;ve fully open-sourced our inference framework based on <a href=https://github.com/vllm-project/vllm>vLLM</a>. With integration with sparse attention methods, our framework can process 1M-token inputs <strong>3x to 7x</strong> faster.</p></li><li><p><strong><a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf>Technical Report</a>:</strong> We&rsquo;re also sharing the technical details behind the Qwen2.5-1M series, including design insights for training and inference frameworks, as well as ablation experiments.</p></li></ol><p>You can experience Qwen2.5-1M models online by visiting our demo on <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo>Huggingface</a> and <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo>Modelscope</a>.</p><p>Additionally, we recently introduced <strong><a href=https://chat.qwenlm.ai/>Qwen Chat</a></strong>, an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the Qwen2.5-Turbo model, which supports long-context processing with a context length of up to 1M tokens.</p><h1 id=model-performance>Model Performance<a hidden class=anchor aria-hidden=true href=#model-performance>#</a></h1><p>Let&rsquo;s start by diving into the performance of the Qwen2.5-1M series models, covering both long-context and short text tasks.</p><h2 id=long-context-tasks>Long-Context Tasks<a hidden class=anchor aria-hidden=true href=#long-context-tasks>#</a></h2><p>First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/passkey_retrieval.png#center width=100%></figure><p>For more complex long-context understanding tasks, we select <a href=https://github.com/hsiehjackson/RULER>RULER</a>, <a href=https://github.com/infinigence/LVEval>LV-Eval</a>, <a href=https://github.com/THUDM/LongAlign>LongbenchChat</a> used in <a href=../qwen2.5-turbo/#more-complex-long-text-tasks>this blog</a>.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/ruler.png#center width=80%></figure><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/lv-eval.png#center width=80%></figure><p>From these results, we can draw a few key conclusions:</p><ul><li><strong>Significantly Superior to the 128k Version:</strong> The Qwen2.5-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length.</li><li><strong>Notable Performance Advantage:</strong> The Qwen2.5-14B-Instruct-1M model not only beats Qwen2.5-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks.</li></ul><h2 id=short-context-tasks>Short-Context Tasks<a hidden class=anchor aria-hidden=true href=#short-context-tasks>#</a></h2><p>Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the Qwen2.5-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/short_result.png#center width=80%></figure><p>Here&rsquo;s what we find:</p><ul><li>Both Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven&rsquo;t been compromised by the addition of long-sequence processing abilities.</li><li>Compared to GPT-4o-mini, both Qwen2.5-14B-Instruct-1M and Qwen2.5-Turbo achieve similar performance on short text tasks while supporting a context length that&rsquo;s eight times longer.</li></ul><h1 id=key-techniques>Key Techniques<a hidden class=anchor aria-hidden=true href=#key-techniques>#</a></h1><p>Here, we&rsquo;ll briefly introduce the key techniques behind building Qwen2.5-1M. For more details, please check out our <a href=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf>technical report</a>.</p><h2 id=long-context-training>Long-Context Training<a hidden class=anchor aria-hidden=true href=#long-context-training>#</a></h2><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/training_stages.png#center width=70%></figure><p>Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for Qwen2.5-1M through multiple stages:</p><ul><li>We begin with an intermediate checkpoint of pre-trained Qwen2.5, which had a 4K token context length.</li><li><strong>In Pretraining</strong>, we gradually increase the context length from 4K to 256K tokens while using <a href=https://arxiv.org/abs/2309.16039>Adjusted Base Frequency</a>, raising the RoPE base from 10,000 to 10,000,000.</li><li><strong>In Supervised Fine-tuning</strong>, we split this into two stages to preserve performance on shorter sequences:<ul><li><strong>Stage 1:</strong> Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of Qwen2.5.</li><li><strong>Stage 2:</strong> Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality.</li></ul></li><li><strong>In Reinforcement Learning</strong>, we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks.</li></ul><p><p>The final instruction-tuned models are capable of handling sequences up to 256K tokens.</p><h2 id=length-extrapolation>Length Extrapolation<a hidden class=anchor aria-hidden=true href=#length-extrapolation>#</a></h2><p>During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques.</p><p>The degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ <a href=https://arxiv.org/abs/2402.17463><strong>Dual Chunk Attention</strong></a> (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca.png#center width=70%></figure><p>We evaluat the Qwen2.5-1M models and their 128K counterparts with and without the length extrapolation method. We can find:</p><p>Even models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca_ablation.png#center width=40%></figure><h2 id=sparse-attention>Sparse Attention<a hidden class=anchor aria-hidden=true href=#sparse-attention>#</a></h2><p>For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on <a href=https://arxiv.org/abs/2407.02490><strong>MInference</strong></a> to accelerate the prefill phase. Furthermore, we propose several improvements:</p><ul><li><p><strong>Integrating with Chunked Prefill:</strong> Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in Qwen2.5-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption.</p></li><li><p><strong>Integrating with Length Extrapolation:</strong> We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy.</p></li><li><p><strong>Sparsity Refinement on Long Sequences:</strong> MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention.</p></li><li><p><strong>More Optimizations:</strong> We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework.</p></li></ul><p><p>With these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/speed.png#center width=85%></figure><h1 id=deploy-qwen25-1m-models-locally>Deploy Qwen2.5-1M Models Locally<a hidden class=anchor aria-hidden=true href=#deploy-qwen25-1m-models-locally>#</a></h1><p>Here we provide step-by-step instructions for deploying the Qwen2.5-1M models on your local devices.</p><h3 id=1-system-preparation>1. System Preparation<a hidden class=anchor aria-hidden=true href=#1-system-preparation>#</a></h3><p>To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.</p><p>Ensure your system meets the following requirements:</p><ul><li><strong>CUDA Version</strong>: 12.1 or 12.3</li><li><strong>Python Version</strong>: >=3.9 and &lt;=3.12</li></ul><p><p>VRAM Requirement for processing 1 million-token sequences:</p><ul><li><strong>Qwen2.5-7B-Instruct-1M</strong>: At least 120GB VRAM (total across GPUs).</li><li><strong>Qwen2.5-14B-Instruct-1M</strong>: At least 320GB VRAM (total across GPUs).</li></ul><p><p>If your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M models for shorter tasks.</p><h3 id=2-install-dependencies>2. Install Dependencies<a hidden class=anchor aria-hidden=true href=#2-install-dependencies>#</a></h3><p>For now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> vllm
</span></span><span class=line><span class=cl>pip install -e . -v
</span></span></code></pre></div><h3 id=3-launch-openai-compatible-api-service>3. Launch OpenAI-Compatible API Service<a hidden class=anchor aria-hidden=true href=#3-launch-openai-compatible-api-service>#</a></h3><p>Use the following command to start the service, configuring it based on your hardware setup:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vllm serve Qwen/Qwen2.5-7B-Instruct-1M <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --tensor-parallel-size <span class=m>4</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --max-model-len <span class=m>1010000</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --enable-chunked-prefill --max-num-batched-tokens <span class=m>131072</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --enforce-eager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --max-num-seqs <span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --quantization fp8 # Enabling FP8 quantization for model weights can reduce memory usage.</span>
</span></span></code></pre></div><p><p>If you encounter any issues, please refer to the <a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M#troubleshooting>Troubleshooting</a> section for more information.</p><p><strong>Parameter Explanations:</strong></p><ul><li><p><strong><code>--tensor-parallel-size</code></strong></p><ul><li>Set to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model.</li></ul></li><li><p><strong><code>--max-model-len</code></strong></p><ul><li>Defines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues.</li></ul></li><li><p><strong><code>--max-num-batched-tokens</code></strong></p><ul><li>Sets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference.</li><li>Recommend 131072 for optimal performance.</li></ul></li><li><p><strong><code>--max-num-seqs</code></strong></p><ul><li>Limits concurrent sequences processed.</li></ul></li></ul><p><h3 id=4-interact-with-the-model>4. Interact with the Model<a hidden class=anchor aria-hidden=true href=#4-interact-with-the-model>#</a></h3><p>You can interact with the deployed model using one of the following methods:</p><p><strong>Option 1. Using Curl</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>    ],
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;temperature&#34;: 0.7,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;top_p&#34;: 0.8,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;repetition_penalty&#34;: 1.05,
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;max_tokens&#34;: 512
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span>
</span></span></code></pre></div><p><strong>Option 2. Using Python</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>openai_api_key</span> <span class=o>=</span> <span class=s2>&#34;EMPTY&#34;</span>
</span></span><span class=line><span class=cl><span class=n>openai_api_base</span> <span class=o>=</span> <span class=s2>&#34;http://localhost:8000/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>openai_api_key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=n>openai_api_base</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;There is an important info hidden inside a lot of irrelevant text. &#34;</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Find it and memorize it. I will quiz you about the important information there.</span><span class=se>\n\n</span><span class=s2>&#34;</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The pass key is 28884. Remember it. 28884 is the pass key.</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. &#34;</span> <span class=o>*</span> <span class=mi>800</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>What is the pass key?&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier.</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chat_response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}],</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Chat response:&#34;</span><span class=p>,</span> <span class=n>chat_response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span></code></pre></div><p><p><strong>Other Options</strong></p><p>For more advanced use cases, consider exploring frameworks like <a href=https://github.com/QwenLM/Qwen-Agent/tree/main>Qwen-Agent</a>, which enable the model to read PDF files and perform other specialized tasks.</p><h1 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>We recognize that long-context models still have a lot of room for improvement. Our goal is to build models that excel in both short and long-context tasks, making sure they bring real value to practical, long-context scenarios. We&rsquo;re diving deep into more efficient training methods, model architectures, and inference methods to make them deployable effectively and perform exceptionally well even in environments with limited resources.
We&rsquo;re confident that all these efforts will open up a whole new world of possibilities for long-context models, expanding their use across a much broader range of applications. Stay tuned as we keep pushing the boundaries!</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>