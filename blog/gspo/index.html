<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GSPO: Towards Scalable Reinforcement Learning for Language Models | Qwen</title><meta name=keywords content><meta name=description content="PAPER DISCORD
Introduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.
To enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/gspo/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/gspo/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/gspo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="GSPO: Towards Scalable Reinforcement Learning for Language Models"><meta property="og:description" content="PAPER DISCORD
Introduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.
To enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/gspo/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-27T15:00:00+08:00"><meta property="article:modified_time" content="2025-07-27T15:00:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="GSPO: Towards Scalable Reinforcement Learning for Language Models"><meta name=twitter:description content="PAPER DISCORD
Introduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.
To enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"GSPO: Towards Scalable Reinforcement Learning for Language Models","item":"https://qwenlm.github.io/blog/gspo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GSPO: Towards Scalable Reinforcement Learning for Language Models","name":"GSPO: Towards Scalable Reinforcement Learning for Language Models","description":"PAPER DISCORD\nIntroduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.\nTo enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm.","keywords":[],"articleBody":"PAPER DISCORD\nIntroduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.\nTo enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm. Unlike previous RL algorithms, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. Compared to GRPO, GSPO demonstrates remarkable advantages in the following aspects:\nPerformant and Efficient: GSPO possesses significantly higher training efficiency and can achieve continuous performance improvements through increasing training compute; Notably Stable: GSPO maintains stable training processes and inherently resolves the stability challenges in the RL training of large Mixture-of-Experts (MoE) models; Infrastructure-Friendly: Due to sequence-level optimization, GSPO is fundamentally more tolerant to precision discrepancies, offering attractive potential for simplifying RL infrastructure. These merits have contributed to the exceptional performance of the latest Qwen3 models (Instruct, Coder, Thinking).\nSequence-Level Optimization Objective Let $x$ be a query, $\\pi_{\\theta_\\mathrm{old}}$ be the old policy that generates responses, $\\{y_i\\}_{i=1}^G$ be the sampled response group, $\\widehat{A}_{i}$ be the group relative advantage of each response, and $\\pi_\\theta$ be the current policy to be optimized. GSPO adopts the following optimization objective:\n$$ \\mathcal{J}_\\text{GSPO} (\\theta) =\\, \\mathbb{E}_{ x \\sim \\mathcal{D},\\, \\{y_i\\}_{i=1}^G \\sim \\pi_{\\theta_\\mathrm{old}}( \\cdot | x) } \\left[ \\frac{1}{G} \\sum_{i=1}^{G} \\min \\left( s_{i}(\\theta) \\widehat{A}_{i}, \\, \\mathrm{clip} \\left( s_{i}(\\theta), 1 - {\\varepsilon}, 1 + {\\varepsilon} \\right) \\widehat{A}_{i} \\right) \\right], $$ where\n$$ s_{i}(\\theta) = \\left( \\frac{ \\pi_{\\theta} (y_i | x) }{ \\pi_{\\theta_\\text{old}} (y_i | x)} \\right)^{\\frac{1}{|y_i|}} = \\exp \\left( \\frac{1}{|y_i|} \\sum_{t=1}^{|y_i|} \\log \\frac{ \\pi_{\\theta} (y_{i,t} | x, y_{i,","wordCount":"916","inLanguage":"en","datePublished":"2025-07-27T15:00:00+08:00","dateModified":"2025-07-27T15:00:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/gspo/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>GSPO: Towards Scalable Reinforcement Learning for Language Models</h1><div class=post-meta><span title='2025-07-27 15:00:00 +0800 +0800'>July 27, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;916 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/gspo/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://huggingface.co/papers/2507.18071 class="btn external" target=_blank>PAPER</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.</p><p>To enable successful RL scaling, we propose the <strong>Group Sequence Policy Optimization (GSPO)</strong> algorithm. Unlike previous RL algorithms, GSPO defines the importance ratio based on sequence likelihood and performs <strong>sequence-level clipping, rewarding, and optimization</strong>. Compared to GRPO, GSPO demonstrates remarkable advantages in the following aspects:</p><ul><li><strong>Performant and Efficient</strong>: GSPO possesses significantly higher training efficiency and can achieve continuous performance improvements through increasing training compute;</li><li><strong>Notably Stable</strong>: GSPO maintains stable training processes and inherently resolves the stability challenges in the RL training of large Mixture-of-Experts (MoE) models;</li><li><strong>Infrastructure-Friendly</strong>: Due to sequence-level optimization, GSPO is fundamentally more tolerant to precision discrepancies, offering attractive potential for simplifying RL infrastructure.</li></ul><p>These merits have contributed to the exceptional performance of the latest Qwen3 models (Instruct, Coder, Thinking).</p><h2 id=sequence-level-optimization-objective>Sequence-Level Optimization Objective<a hidden class=anchor aria-hidden=true href=#sequence-level-optimization-objective>#</a></h2><p>Let $x$ be a query, $\pi_{\theta_\mathrm{old}}$ be the old policy that generates responses, $\{y_i\}_{i=1}^G$ be the sampled response group, $\widehat{A}_{i}$ be the group relative advantage of each response, and $\pi_\theta$ be the current policy to be optimized. GSPO adopts the following optimization objective:</p>$$
\mathcal{J}_\text{GSPO} (\theta)
=\,
\mathbb{E}_{ x \sim \mathcal{D},\, \{y_i\}_{i=1}^G \sim \pi_{\theta_\mathrm{old}}( \cdot | x) }
\left[
\frac{1}{G} \sum_{i=1}^{G}
\min \left( s_{i}(\theta) \widehat{A}_{i}, \, \mathrm{clip} \left( s_{i}(\theta), 1 - {\varepsilon}, 1 + {\varepsilon} \right) \widehat{A}_{i} \right)
\right],
$$<p>where</p>$$
s_{i}(\theta)
=
\left( \frac{ \pi_{\theta} (y_i | x) }{ \pi_{\theta_\text{old}} (y_i | x)} \right)^{\frac{1}{|y_i|}}
=
\exp \left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{ \pi_{\theta} (y_{i,t} | x, y_{i,&lt;t}) }{ \pi_{\theta_\text{old}} (y_{i,t} | x,y_{i,&lt;t})} \right).
$$<p>Here, $s_i(\theta)$ is <strong>the importance ratio defined based on sequence likelihood</strong> in GSPO, where we perform length normalization to reduce variance and unify the numerical range of $s_i(\theta)$.</p><h2 id=training-efficiency-and-performance>Training Efficiency and Performance<a hidden class=anchor aria-hidden=true href=#training-efficiency-and-performance>#</a></h2><p>We experiment with a cold-start model fine-tuned from Qwen3-30B-A3B-Base and report its training reward curves as well as performance curves on the AIME'24, LiveCodeBench, and CodeForces benchmarks. We compare against GRPO as the baseline. Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL (which we will discuss later), while <strong>GSPO has obviated the need for this strategy</strong>.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/results.jpg#center><figcaption><h4>Experimental results</h4></figcaption></figure><p>As shown in the figure above, GSPO demonstrates <strong>significantly higher training efficiency</strong> than GRPO, achieving better performance under the same training cost. Particularly, we observe that <strong>GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length</strong> — this is exactly the <strong>scalability</strong> we expect from an algorithm. Ultimately, we successfully applied GSPO to the large-scale RL training of the latest Qwen3 models, further unleashing the potential of RL scaling!</p><p>An interesting observation is that the fraction of tokens clipped in GSPO is two orders of magnitude higher than that in GRPO (as shown in the figure below), while GSPO still achieves higher training efficiency. This further demonstrates that GRPO&rsquo;s token-level optimization objective is noisy and inefficient, while GSPO&rsquo;s sequence-level approach provides a more reliable and effective learning signal.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/clipping.jpg><figcaption><h4>Fractions of clipped tokens</h4></figcaption></figure><h2 id=benefits-for-moe-rl-and-infrastructure>Benefits for MoE RL and Infrastructure<a hidden class=anchor aria-hidden=true href=#benefits-for-moe-rl-and-infrastructure>#</a></h2><p>We found that when adopting the GRPO algorithm, the expert activation volatility of MoE models prevents RL training from converging properly. To address this challenge, we previously employed the <strong>Routing Replay</strong> training strategy, which caches the activated experts in $\pi_{\theta_\text{old}}$ and &ldquo;replays&rdquo; these routing patterns in $\pi_\theta$ when computing importance ratios. As shown in the figure below, Routing Replay is crucial for normal convergence of GRPO training on MoE models. However, the Routing Replay strategy incurs additional memory and communication overhead and may limit the actual capacity of MoE models.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/routing_replay.jpg><figcaption><h4>Effect of Routing Replay in the GRPO training of MoE models</h4></figcaption></figure><p>The notable advantage of GSPO lies in <strong>completely eliminating the dependency on Routing Replay</strong>. The key insight is that GSPO only focuses on sequence-level likelihood (i.e., $\pi_\theta(y_i|x)$) and is not sensitive to individual token likelihood (i.e., $\pi_\theta(y_{i,t}|x,y_{i,&lt;t})$). Therefore, it does not require infrastructure-heavy workarounds like Routing Replay, both simplifying and stabilizing the training process while allowing models to maximize their capacity.</p><p>Additionally, since GSPO uses only sequence-level rather than token-level likelihoods for optimization, intuitively the former is much more tolerant of precision discrepancies. Therefore, GSPO makes it possible to directly use likelihoods returned by inference engines for optimization, eliminating the need for recomputation with training engines. This is particularly beneficial in scenarios such as partial rollout, multi-turn RL, and training-inference disaggregated frameworks.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>We propose Group Sequence Policy Optimization (GSPO), a new RL algorithm for training language models. GSPO demonstrates notably superior training stability, efficiency, and performance compared to GRPO and exhibits particular efficacy for the large-scale RL training of MoE models, laying the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as our algorithmic cornerstone, we will continue to push the boundaries of RL scaling and look forward to the resulting fundamental advances in intelligence.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find our work helpful, feel free to give us a citation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tex data-lang=tex><span class=line><span class=cl>@article<span class=nb>{</span>gspo,
</span></span><span class=line><span class=cl>  title=<span class=nb>{</span>Group Sequence Policy Optimization<span class=nb>}</span>, 
</span></span><span class=line><span class=cl>  author=<span class=nb>{</span>
</span></span><span class=line><span class=cl>    Chujie Zheng and Shixuan Liu and Mingze Li and Xiong-Hui Chen and Bowen Yu and 
</span></span><span class=line><span class=cl>    Chang Gao and Kai Dang and Yuqiong Liu and Rui Men and An Yang and Jingren Zhou and 
</span></span><span class=line><span class=cl>    Junyang Lin 
</span></span><span class=line><span class=cl>  <span class=nb>}</span>,
</span></span><span class=line><span class=cl>  journal=<span class=nb>{</span>arXiv preprint arXiv:2507.18071<span class=nb>}</span>,
</span></span><span class=line><span class=cl>  year=<span class=nb>{</span>2025<span class=nb>}</span>
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>