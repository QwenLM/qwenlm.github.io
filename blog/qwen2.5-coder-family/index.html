<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD
Introduction Today, we are excited to open source the &ldquo;Powerful&rdquo;, &ldquo;Diverse&rdquo;, and &ldquo;Practical&rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.
Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5-coder-family/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-coder-family/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-coder-family/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5-Coder Series: Powerful, Diverse, Practical."><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD
Introduction Today, we are excited to open source the &ldquo;Powerful&rdquo;, &ldquo;Diverse&rdquo;, and &ldquo;Practical&rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.
Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5-coder-family/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-11-12T00:00:02+08:00"><meta property="article:modified_time" content="2024-11-12T00:00:02+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5-Coder Series: Powerful, Diverse, Practical."><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD
Introduction Today, we are excited to open source the &ldquo;Powerful&rdquo;, &ldquo;Diverse&rdquo;, and &ldquo;Practical&rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.
Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5-Coder Series: Powerful, Diverse, Practical.","item":"https://qwenlm.github.io/blog/qwen2.5-coder-family/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5-Coder Series: Powerful, Diverse, Practical.","name":"Qwen2.5-Coder Series: Powerful, Diverse, Practical.","description":"GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\nIntroduction Today, we are excited to open source the \u0026ldquo;Powerful\u0026rdquo;, \u0026ldquo;Diverse\u0026rdquo;, and \u0026ldquo;Practical\u0026rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.\nPowerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1.","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\nIntroduction Today, we are excited to open source the “Powerful”, “Diverse”, and “Practical” Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.\nPowerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers; Practical: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios; Powerful: Code capabilities reach SOTA for open-source models Code Generation: Qwen2.5-Coder-32B-Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\nCode Repair: Code repair is an important programming skill. Qwen2.5-Coder-32B-Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5-Coder-32B-Instruct scored 73.7, performing comparably to GPT-4o on Aider.\nCode Reasoning: Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5-Coder-7B-Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further. Multiple Programming Languages: An intelligent programming assistant should be familiar with all programming languages. Qwen2.5-Coder-32B-Instruct performs excellently across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket, thanks to our unique data cleaning and balancing during the pre-training phase. Additionally, the multi-language code repair capabilities of Qwen2.5-Coder-32B-Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen2.5-Coder-32B-Instruct scored 75.2, ranking first among all open-source models. Human Preference Alignment: To evaluate the alignment performance of Qwen2.5-Coder-32B-Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‘A vs. B win’ evaluation method, which measures the percentage of instances in the test set where model A’s score exceeds model B’s. The results below demonstrate the advantages of Qwen2.5-Coder-32B-Instruct in preference alignment. Diverse: Rich Model Sizes This time, Qwen2.5-Coder has open-sourced a rich variety of model sizes, including 0.5B/1.5B/3B/7B/14B/32B, which not only meets the needs of developers in different resource scenarios but also provides a good experimental platform for the research community. The following table provides detailed model information:\nModels Params Non-Emb Params Layers Heads (KV) Tie Embedding Context Length License Qwen2.5-Coder-0.5B 0.49B 0.36B 24 14 / 2 Yes 32K Apache 2.0 Qwen2.5-Coder-1.5B 1.54B 1.31B 28 12 / 2 Yes 32K Apache 2.0 Qwen2.5-Coder-3B 3.09B 2.77B 36 16 / 2 Yes 32K Qwen Research Qwen2.5-Coder-7B 7.61B 6.53B 28 28 / 4 No 128K Apache 2.0 Qwen2.5-Coder-14B 14.7B 13.1B 48 40 / 8 No 128K Apache 2.0 Qwen2.5-Coder-32B 32.5B 31.0B 64 40 / 8 No 128K Apache 2.0 We have always believed in the philosophy of Scaling Law. We evaluated the performance of different sizes of Qwen2.5-Coder across all datasets to verify the effectiveness of Scaling in Code LLMs. For each size, we open-sourced both Base and Instruct models, where the Instruct model serves as an official aligned model that can chat directly, and the Base model serves as a foundation for developers to fine-tune their own models.\nHere are the performances of the Base models of different sizes: Here are the performances of the Instruct models of different sizes: We present a comparison of different sizes of Qwen2.5-Coder with other open-source models on core datasets.\nFor the Base model, we chose MBPP-3shot as the evaluation metric. Our extensive experiments show that MBPP-3shot is more suitable for evaluating base models and correlates well with the actual performance of the models. For the Instruct model, we selected the latest 4 months of LiveCodeBench (2024.07 - 2024.11) questions as the evaluation, which are the latest published questions that could not have leaked into the training set, reflecting the model’s OOD capabilities. There is a positive correlation between model size and model performance, and Qwen2.5-Coder has achieved SOTA performance across all sizes, encouraging us to continue exploring larger sizes of Coder.\nPractical: Encountering Cursor and Artifacts A practical Coder has always been our vision, and for this reason, we explored the actual performance of Qwen2.5-Coder in code assistants and Artifacts scenarios.\nQwen2.5-Coder 🤝 Cursor Code assistants have become widely used, but most currently rely on closed-source models. We hope that the emergence of Qwen2.5-Coder can provide developers with a friendly and powerful option. Here is an example of Qwen2.5-Coder in the Cursor.\nExample: Qwen2.5-Coder 🤝 Cursor\rAdditionally, Qwen2.5-Coder-32B has demonstrated strong code completion capabilities on pre-trained models, achieving SOTA performance on a total of 5 benchmarks: Humaneval-Infilling, CrossCodeEval, CrossCodeLongEval, RepoEval, and SAFIM. To maintain a fair comparison, we controlled the maximum sequence length to 8k and used the Fill-in-the-Middle mode for testing. Among the four evaluation sets of CrossCodeEval, CrossCodeLongEval, RepoEval, and Humaneval-Infilling, we evaluated whether the generated content was exactly equal to the true labels (Exact Match). In SAFIM, we used the one-time execution success rate (Pass@1) for evaluation.\nQwen2.5-Coder 🤝 Artifacts Artifacts are an important application of code generation, helping users create visual works. We chose Open WebUI to explore the potential of Qwen2.5-Coder in the Artifacts scenario, and here are some specific examples.\nExample: Three-body Problem Simulation\rNext\rExample: Lissajous Curve\rNext\rExample: Drafting a resume\rNext\rExample: Emoji dancing\rNext\rWe will soon launch the code mode on the Tongyi official website https://tongyi.aliyun.com, supporting one-click generation of websites, mini-games, and data charts, among other visual applications. We welcome everyone to experience it!\nModel License Qwen2.5-Coder 0.5B / 1.5B / 7B / 14B / 32B are licensed under Apache 2.0, while 3B is under Qwen-Research license;\nWhat’s Next for Qwen-Coder? We believe that this release can truly help developers and explore more interesting application scenarios with the community. Additionally, we are delving into powerful reasoning models centered around code, and we believe we will meet everyone soon!\nCitation @article{hui2024qwen2, title={Qwen2. 5-Coder Technical Report}, author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others}, journal={arXiv preprint arXiv:2409.12186}, year={2024} } @article{yang2024qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } ","wordCount":"1158","inLanguage":"en","datePublished":"2024-11-12T00:00:02+08:00","dateModified":"2024-11-12T00:00:02+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5-coder-family/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5-Coder Series: Powerful, Diverse, Practical.</h1><div class=post-meta><span title='2024-11-12 00:00:02 +0800 +0800'>November 12, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1158 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5-coder-family/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-top.jpg#center width=100%></figure><a href=https://github.com/QwenLM/Qwen2.5-Coder class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://www.kaggle.com/models/qwen-lm/qwen2.5-coder class="btn external" target=_blank>KAGGLE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Today, we are excited to open source the &ldquo;Powerful&rdquo;, &ldquo;Diverse&rdquo;, and &ldquo;Practical&rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.</p><ul><li><strong>Powerful</strong>: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills;</li><li><strong>Diverse</strong>: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers;</li><li><strong>Practical</strong>: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios;</li></ul><h2 id=powerful-code-capabilities-reach-sota-for-open-source-models>Powerful: Code capabilities reach SOTA for open-source models<a hidden class=anchor aria-hidden=true href=#powerful-code-capabilities-reach-sota-for-open-source-models>#</a></h2><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-main.png#center width=100%></figure><ul><li><p><strong>Code Generation</strong>: Qwen2.5-Coder-32B-Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.</p></li><li><p><strong>Code Repair</strong>: Code repair is an important programming skill. Qwen2.5-Coder-32B-Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5-Coder-32B-Instruct scored 73.7, performing comparably to GPT-4o on Aider.</p></li><li><p><strong>Code Reasoning</strong>: Code reasoning refers to the model&rsquo;s ability to learn the process of code execution and accurately predict the model&rsquo;s inputs and outputs. The recently released Qwen2.5-Coder-7B-Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-crux.png#center width=80%></figure></p></li><li><p><strong>Multiple Programming Languages</strong>: An intelligent programming assistant should be familiar with all programming languages. Qwen2.5-Coder-32B-Instruct performs excellently across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket, thanks to our unique data cleaning and balancing during the pre-training phase.<br><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-mceval.jpg#center width=80%></figure></p></li></ul><p>Additionally, the multi-language code repair capabilities of Qwen2.5-Coder-32B-Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen2.5-Coder-32B-Instruct scored 75.2, ranking first among all open-source models.<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-mdeval.jpg#center width=80%></figure></p><ul><li><strong>Human Preference Alignment</strong>: To evaluate the alignment performance of Qwen2.5-Coder-32B-Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an &lsquo;A vs. B win&rsquo; evaluation method, which measures the percentage of instances in the test set where model A&rsquo;s score exceeds model B&rsquo;s. The results below demonstrate the advantages of Qwen2.5-Coder-32B-Instruct in preference alignment.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-arena.jpg#center width=80%></figure><h2 id=diverse-rich-model-sizes>Diverse: Rich Model Sizes<a hidden class=anchor aria-hidden=true href=#diverse-rich-model-sizes>#</a></h2><p>This time, Qwen2.5-Coder has open-sourced a rich variety of model sizes, including 0.5B/1.5B/3B/7B/14B/32B, which not only meets the needs of developers in different resource scenarios but also provides a good experimental platform for the research community. The following table provides detailed model information:</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Params</th><th style=text-align:center>Non-Emb Params</th><th style=text-align:center>Layers</th><th style=text-align:center>Heads (KV)</th><th style=text-align:center>Tie Embedding</th><th style=text-align:center>Context Length</th><th style=text-align:center>License</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2.5-Coder-0.5B</td><td style=text-align:center>0.49B</td><td style=text-align:center>0.36B</td><td style=text-align:center>24</td><td style=text-align:center>14 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-1.5B</td><td style=text-align:center>1.54B</td><td style=text-align:center>1.31B</td><td style=text-align:center>28</td><td style=text-align:center>12 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-3B</td><td style=text-align:center>3.09B</td><td style=text-align:center>2.77B</td><td style=text-align:center>36</td><td style=text-align:center>16 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Qwen Research</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-7B</td><td style=text-align:center>7.61B</td><td style=text-align:center>6.53B</td><td style=text-align:center>28</td><td style=text-align:center>28 / 4</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-14B</td><td style=text-align:center>14.7B</td><td style=text-align:center>13.1B</td><td style=text-align:center>48</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-32B</td><td style=text-align:center>32.5B</td><td style=text-align:center>31.0B</td><td style=text-align:center>64</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr></tbody></table><p>We have always believed in the philosophy of Scaling Law. We evaluated the performance of different sizes of Qwen2.5-Coder across all datasets to verify the effectiveness of Scaling in Code LLMs. For each size, we open-sourced both Base and Instruct models, where the Instruct model serves as an official aligned model that can chat directly, and the Base model serves as a foundation for developers to fine-tune their own models.</p><p>Here are the performances of the Base models of different sizes:<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-base.png#center width=100%></figure></p><p>Here are the performances of the Instruct models of different sizes:<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-instruct.png#center width=100%></figure></p><p>We present a comparison of different sizes of Qwen2.5-Coder with other open-source models on core datasets.</p><ul><li>For the Base model, we chose MBPP-3shot as the evaluation metric. Our extensive experiments show that MBPP-3shot is more suitable for evaluating base models and correlates well with the actual performance of the models.</li><li>For the Instruct model, we selected the latest 4 months of LiveCodeBench (2024.07 - 2024.11) questions as the evaluation, which are the latest published questions that could not have leaked into the training set, reflecting the model&rsquo;s OOD capabilities.</li></ul><p>There is a positive correlation between model size and model performance, and Qwen2.5-Coder has achieved SOTA performance across all sizes, encouraging us to continue exploring larger sizes of Coder.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-dual.jpg#center width=100%></figure><h2 id=practical-encountering-cursor-and-artifacts>Practical: Encountering Cursor and Artifacts<a hidden class=anchor aria-hidden=true href=#practical-encountering-cursor-and-artifacts>#</a></h2><p>A practical Coder has always been our vision, and for this reason, we explored the actual performance of Qwen2.5-Coder in code assistants and Artifacts scenarios.</p><h3 id=qwen25-coder--cursor>Qwen2.5-Coder 🤝 Cursor<a hidden class=anchor aria-hidden=true href=#qwen25-coder--cursor>#</a></h3><p>Code assistants have become widely used, but most currently rely on closed-source models. We hope that the emergence of Qwen2.5-Coder can provide developers with a friendly and powerful option. Here is an example of Qwen2.5-Coder in the <a href=https://www.cursor.com/>Cursor</a>.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Qwen2.5-Coder 🤝 Cursor</span></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-game_of_life.mp4></video></figure></div></div></div></div><p>Additionally, Qwen2.5-Coder-32B has demonstrated strong code completion capabilities on pre-trained models, achieving SOTA performance on a total of 5 benchmarks: Humaneval-Infilling, CrossCodeEval, CrossCodeLongEval, RepoEval, and SAFIM. To maintain a fair comparison, we controlled the maximum sequence length to 8k and used the Fill-in-the-Middle mode for testing. Among the four evaluation sets of CrossCodeEval, CrossCodeLongEval, RepoEval, and Humaneval-Infilling, we evaluated whether the generated content was exactly equal to the true labels (Exact Match). In SAFIM, we used the one-time execution success rate (Pass@1) for evaluation.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-fim.jpg#center width=80%></figure><h3 id=qwen25-coder--artifacts>Qwen2.5-Coder 🤝 Artifacts<a hidden class=anchor aria-hidden=true href=#qwen25-coder--artifacts>#</a></h3><p>Artifacts are an important application of code generation, helping users create visual works. We chose <a href=https://openwebui.com/>Open WebUI</a> to explore the potential of Qwen2.5-Coder in the Artifacts scenario, and here are some specific examples.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Three-body Problem Simulation</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-3body.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Lissajous Curve</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-lissajous.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Drafting a resume</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-cv.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Emoji dancing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-dancing.mp4></video></figure></div></div></div></div><p>We will soon launch the code mode on the Tongyi official website <a href=https://tongyi.aliyun.com>https://tongyi.aliyun.com</a>, supporting one-click generation of websites, mini-games, and data charts, among other visual applications. We welcome everyone to experience it!</p><h2 id=model-license>Model License<a hidden class=anchor aria-hidden=true href=#model-license>#</a></h2><p>Qwen2.5-Coder 0.5B / 1.5B / 7B / 14B / 32B are licensed under <strong>Apache 2.0</strong>, while 3B is under Qwen-Research license;</p><h2 id=whats-next-for-qwen-coder>What&rsquo;s Next for Qwen-Coder?<a hidden class=anchor aria-hidden=true href=#whats-next-for-qwen-coder>#</a></h2><p>We believe that this release can truly help developers and explore more interesting application scenarios with the community. Additionally, we are delving into powerful reasoning models centered around code, and we believe we will meet everyone soon!</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><pre tabindex=0><code>@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>