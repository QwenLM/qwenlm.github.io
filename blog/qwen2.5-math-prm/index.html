<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Towards Effective Process Supervision in Mathematical Reasoning | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs&rsquo; reasoning processes."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5-math-prm/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-math-prm/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Towards Effective Process Supervision in Mathematical Reasoning"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs&rsquo; reasoning processes."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5-math-prm/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-14T00:00:03+08:00"><meta property="article:modified_time" content="2025-01-14T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Towards Effective Process Supervision in Mathematical Reasoning"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs&rsquo; reasoning processes."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Towards Effective Process Supervision in Mathematical Reasoning","item":"https://qwenlm.github.io/blog/qwen2.5-math-prm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Towards Effective Process Supervision in Mathematical Reasoning","name":"Towards Effective Process Supervision in Mathematical Reasoning","description":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs\u0026rsquo; reasoning processes.","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMsâ€™ reasoning processes. Hence, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight.\nProcess Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of LLMs, which aim to identify and mitigate intermediate errors in the reasoning processes. In terms of evaluation, previous studies have predominantly relied on the response-level Best-of-N (BoN) evaluation, which selects the highest-scored response from $N$ candidates according to a PRM.\nToday, we release a new state-of-the-art PRM that outperforms existing open-source alternatives for future research in building process supervision models. We also release the step-level benchmark ProcessBench for measuring the model ability to identify erroneous steps in mathematical reasoning.\nOpen-Sourcing ProcessBench ProcessBench aims to measure the model ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. ProcessBench can be used to evaluate two types of models: PRMs and critic models, where for the latter we prompt general language models to critique each solution step by step.\nReleasing Process Reward Models We release two PRMs fine-tuned on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct, namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B respectively. Our trained PRMs exhibit both impressive performance in the BoN evaluation and stronger error identification performance in ProcessBench.\nBest-of-N Evaluation Following Qwen2.5-Math, we sampled eight responses (i.e., $N=8$) from Qwen2.5-Math-7B-Instruct across multiple mathematical benchmarks, including GSM8K, MATH, Minerva Math, GaoKao 2023 En, OlympiadBench, College Math, and MMLU STEM. Each candidate response is scored using the product of all the individual scores of each step within the response. We report the result of majority voting among eight samplings (maj@8) as the baseline, and pass@8 (i.e., the proportion of test samples where any of the eight samplings lead to the correct final answers) as the upper bound.\nAs shown in the following table, Qwen2.5-Math-PRM-7B demonstrates superior performance compared to other PRMs of equivalent model scale. Notably, it outperforms maj@8 across all 7 tasks, achieving an average improvement of 1.4%. Furthermore, Qwen2.5-Math-PRM-72B exhibits slightly better overall performance than Qwen2.5-Math-RM-72B, with particularly significant improvements observed in the Minerva Math and MMLU STEM tasks.\nProcessBench We also evaluate our PRMs on ProcessBench to measure the ability of identify erroneous steps. When compared with LLM-as-judge, Qwen2.5-Math-PRM-7B in smaller model size demonstrates superior performance over all open-source models. For proprietary language models, Qwen2.5-Math-PRM-7B outperforms GPT-4o-0806, while there remains a performance gap compared to o1-mini. Furthermore, in comparison with existing PRMs, both Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B exhibit substantial advantages over their counterparts. An interesting observation worth noting is that the Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B exhibits considerable capability in identifying step errors, even surpassing some open-source PRMs, which validates its potential as a complementary reward beyond solely rule-based mechanism.\nConclusion ProcessBench demonstrate the current challenges of existing PRMs and fills the gap in step-level evaluation of PRMs. Besides open-sourcing PRMs, we also identify critical limitations in current data construction approaches for PRMs and reveal the potential bias in using response-level BoN evaluation alone for PRMs through extensive empirical studies in our paper. We hope that ProcessBench and the best practices in training our own PRMs can foster future research and development for reasoning process supervision. For more details, please check out our papers in the following!\nCitation If you find our work helpful, feel free to give us a citation.\n@article{processbench, title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, author={ Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2412.06559}, year={2024} } @article{prmlessons, title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, author={ Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin }, journal={arXiv preprint arXiv:2501.07301}, year={2025} } ","wordCount":"741","inLanguage":"en","datePublished":"2025-01-14T00:00:03+08:00","dateModified":"2025-01-14T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5-math-prm/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Towards Effective Process Supervision in Mathematical Reasoning</h1><div class=post-meta><span title='2025-01-14 00:00:03 +0800 +0800'>January 14, 2025</span>&nbsp;Â·&nbsp;4 min&nbsp;Â·&nbsp;741 words&nbsp;Â·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5-math-prm/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/Qwen2.5-Math-PRM.png#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2-Math class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen25-math-66eaa240a1b7d5ee65f1da3e class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs&rsquo; reasoning processes. Hence, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight.</p><p>Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of LLMs, which aim to identify and mitigate intermediate errors in the reasoning processes. In terms of evaluation, previous studies have predominantly relied on the response-level Best-of-N (BoN) evaluation, which selects the highest-scored response from $N$ candidates according to a PRM.</p><p>Today, we release a new state-of-the-art PRM that outperforms existing open-source alternatives for future research in building process supervision models. We also release the step-level benchmark ProcessBench for measuring the model ability to identify erroneous steps in mathematical reasoning.</p><h1 id=open-sourcing-processbench>Open-Sourcing ProcessBench<a hidden class=anchor aria-hidden=true href=#open-sourcing-processbench>#</a></h1><p>ProcessBench aims to measure the model ability to identify erroneous steps in mathematical reasoning.
It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts.
Models are required to identify the earliest step that contains an error, or conclude that all steps are correct.
ProcessBench can be used to evaluate two types of models: PRMs and critic models, where for the latter we prompt general language models to critique each solution step by step.</p><h1 id=releasing-process-reward-models>Releasing Process Reward Models<a hidden class=anchor aria-hidden=true href=#releasing-process-reward-models>#</a></h1><p>We release two PRMs fine-tuned on Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct, namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B respectively. Our trained PRMs exhibit both impressive performance in the BoN evaluation and stronger error identification performance in ProcessBench.</p><h2 id=best-of-n-evaluation>Best-of-N Evaluation<a hidden class=anchor aria-hidden=true href=#best-of-n-evaluation>#</a></h2><p>Following Qwen2.5-Math, we sampled eight responses (i.e., $N=8$) from Qwen2.5-Math-7B-Instruct across multiple mathematical benchmarks, including GSM8K, MATH, Minerva Math, GaoKao 2023 En, OlympiadBench, College Math, and MMLU STEM.
Each candidate response is scored using the product of all the individual scores of each step within the response.
We report the result of majority voting among eight samplings (maj@8) as the baseline, and pass@8 (i.e., the proportion of test samples where any of the eight samplings lead to the correct final answers) as the upper bound.</p><p>As shown in the following table, Qwen2.5-Math-PRM-7B demonstrates superior performance compared to other PRMs of equivalent model scale. Notably, it outperforms maj@8 across all 7 tasks, achieving an average improvement of 1.4%.
Furthermore, Qwen2.5-Math-PRM-72B exhibits slightly better overall performance than Qwen2.5-Math-RM-72B, with particularly significant improvements observed in the Minerva Math and MMLU STEM tasks.</p><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/prm_bon.png#center width=100%></figure><h2 id=processbench>ProcessBench<a hidden class=anchor aria-hidden=true href=#processbench>#</a></h2><p>We also evaluate our PRMs on ProcessBench to measure the ability of identify erroneous steps.
When compared with LLM-as-judge, Qwen2.5-Math-PRM-7B in smaller model size demonstrates superior performance over all open-source models. For proprietary language models, Qwen2.5-Math-PRM-7B outperforms GPT-4o-0806, while there remains a performance gap compared to o1-mini.
Furthermore, in comparison with existing PRMs, both Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B exhibit substantial advantages over their counterparts.
An interesting observation worth noting is that the Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B exhibits considerable capability in identifying step errors, even surpassing some open-source PRMs, which validates its potential as a complementary reward beyond solely rule-based mechanism.</p><figure><img src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/Qwen2.5-Math-PRM/prm_processbench.png#center width=100%></figure><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>ProcessBench demonstrate the current challenges of existing PRMs and fills the gap in step-level evaluation of PRMs.
Besides open-sourcing PRMs, we also identify critical limitations in current data construction approaches for PRMs and reveal the potential bias in using response-level BoN evaluation alone for PRMs through extensive empirical studies in our paper.
We hope that ProcessBench and the best practices in training our own PRMs can foster future research and development for reasoning process supervision.
For more details, please check out our papers in the following!</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><p>If you find our work helpful, feel free to give us a citation.</p><pre tabindex=0><code>@article{processbench,
  title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, 
  author={
    Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and
    Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin
  },
  journal={arXiv preprint arXiv:2412.06559},
  year={2024}
}
@article{prmlessons,
  title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, 
  author={
    Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin
  },
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>