<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>QVQ-Max: Think with Evidence | Qwen</title><meta name=keywords content><meta name=description content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only &ldquo;understand&rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qvq-max-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.4f62259998ff7b98600586086c11b90753ca941d3fefd46d058f5df6a9fa05f4.css integrity="sha256-T2IlmZj/e5hgBYYIbBG5B1PKlB0/79RtBY9d9qn6BfQ=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qvq-max-preview/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qvq-max-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="QVQ-Max: Think with Evidence"><meta property="og:description" content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only &ldquo;understand&rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qvq-max-preview/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-03-28T00:00:04+08:00"><meta property="article:modified_time" content="2025-03-28T00:00:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="QVQ-Max: Think with Evidence"><meta name=twitter:description content="QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD
Introduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only &ldquo;understand&rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"QVQ-Max: Think with Evidence","item":"https://qwenlm.github.io/blog/qvq-max-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"QVQ-Max: Think with Evidence","name":"QVQ-Max: Think with Evidence","description":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only \u0026ldquo;understand\u0026rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities.","keywords":[],"articleBody":"\rQWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only “understand” the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities. Though this is just our first version, its potential is already eye-catching.\nMathVision is a benchmark that aggregates various challenging multimodal mathematical problems, and we evaluate a model’s ability to solve complex math problems based on its performance on this benchmark. As shown in the figure, by adjusting the maximum length of the model’s thinking process, we observe a continuous improvement in the model’s accuracy on MathVision, demonstrating the immense potential of the model.\nIn the following sections, we will discuss the design philosophy behind QVQ-Max, its actual capabilities, and what it can do for you.\nWhy Do We Need Visual Reasoning? Traditional AI models mostly rely on text input, such as answering questions, writing articles, or generating code. However, in real life, much of the information isn’t expressed through words but rather through images, charts, or even videos. A single image can contain rich details like colors, shapes, spatial relationships, and more. These elements are often more intuitive, but also more complex than text.\nFor example, if you want to determine whether an architectural blueprint is reasonable, a description alone might not be enough. But if you could see the blueprint and analyze it using professional knowledge, the task becomes much easier. This is the significance of visual reasoning—it allows AI to not just “see,” but also “understand” and “think.”\nOur goal in designing QVQ-Max was simple: to create an assistant that is both “sharp-eyed” and “quick-thinking,” capable of solving various practical problems for users.\nCore Capabilities: From Observation to Reasoning The capabilities of QVQ-Max can be summarized into three areas: detailed observation, deep reasoning, and flexible application. Let’s break down how it performs in each area.\nDetailed Observation: Capturing Every Detail\nQVQ-Max excels at parsing images, whether they’re complex charts or casual snapshots taken in daily life. It can quickly identify key elements in an image. For instance, it can tell you what objects are in a photo, what textual labels exist, and even point out small details that you might overlook.\nDeep Reasoning: Not Just “Seeing,” But Also “Thinking”\nIdentifying content in an image is not enough. QVQ-Max can further analyze this information and combine it with background knowledge to draw conclusions. For example, in a geometry problem, it can derive answers based on the accompanying diagram. In a video clip, it can predict what might happen next based on the current scene.\nFlexible Application: From Problem-Solving to Creation\nBeyond analysis and reasoning, QVQ-Max can also perform interesting tasks like helping you design illustrations, generate short video scripts, or even create role-playing content based on your requirements. If you upload a rough sketch, it might help you refine it into a complete piece. Upload a regular photo, and it can transform into a sharp critic or even a fortune-teller.\nDemo Cases QVQ-Max has a wide range of applications, whether in learning, work, or daily life—it can come in handy in many scenarios.\nWorkplace Tool: At work, QVQ-Max can assist in completing data analysis, organizing information, and even writing code\nLearning Assistant: For students, QVQ-Max can help solve difficult problems in subjects like math and physics, especially those accompanied by diagrams. It can also explain complex concepts in an intuitive way, making learning easier.\nLife Helper: In daily life, QVQ-Max can offer practical advice. For instance, it can recommend outfit combinations based on photos of your wardrobe, or guide you through cooking a new dish based on recipe images.\nMulti-image Recognition\rNext\rQVQ-Max-Preview\rMathematical Reasoning\rNext\rQVQ-Max-Preview\rInterpreting Palm Readings (For Reference Only)\rNext\rQVQ-Max-Preview\rVideo Understanding\rNext\rQVQ-Max-Preview\rLearn to code by watching videos\rNext\rQVQ-Max-Preview\rNext Step The current version of QVQ-Max is just the first iteration, and there’s still much room for improvement. Moving forward, we will focus on several key areas:\nMore Accurate Observations: Enhance recognition accuracy through grounding techniques, which validate observations made from visual content. Visual Agent: Improve the model’s ability to handle multi-step and more complex tasks, such as operating smartphones or computers, and even playing games. Better Interaction: Expand beyond text-based interaction to include more modalities, such as tool verification and visual generation, allowing for richer user experiences. Overall, QVQ-Max is a visual reasoning model that possesses both “vision” and “intellect.” It doesn’t just recognize the content in images; it combines this information to analyze, reason, and even complete creative tasks. Although it’s still in its growth phase, it has already shown great potential. Through continuous optimization, we aim to make QVQ-Max a truly practical visual agent that helps everyone solve real-world problems.\n","wordCount":"829","inLanguage":"en","datePublished":"2025-03-28T00:00:04+08:00","dateModified":"2025-03-28T00:00:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qvq-max-preview/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>QVQ-Max: Think with Evidence</h1><div class=post-meta><span title='2025-03-28 00:00:04 +0800 +0800'>March 28, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;829 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qvq-max-preview/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><video loop src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/head.mov autoplay></video></figure><p><a href=https://chat.qwenlm.ai class="btn external" target=_blank>QWEN CHAT</a>
<a href=https://github.com/QwenLM/Qwen2.5-VL class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5 class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/collections/Qwen25-VL-58fbb5d31f1d47 class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=introduction><strong>Introduction</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only &ldquo;understand&rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities. Though this is just our first version, its potential is already eye-catching.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/test_time.png#center width=80%></figure><p>MathVision is a benchmark that aggregates various challenging multimodal mathematical problems, and we evaluate a model&rsquo;s ability to solve complex math problems based on its performance on this benchmark. As shown in the figure, by adjusting the maximum length of the model’s thinking process, we observe a continuous improvement in the model’s accuracy on MathVision, demonstrating the immense potential of the model.</p><p>In the following sections, we will discuss the design philosophy behind QVQ-Max, its actual capabilities, and what it can do for you.</p><hr><h3 id=why-do-we-need-visual-reasoning><strong>Why Do We Need Visual Reasoning?</strong><a hidden class=anchor aria-hidden=true href=#why-do-we-need-visual-reasoning>#</a></h3><p>Traditional AI models mostly rely on text input, such as answering questions, writing articles, or generating code. However, in real life, much of the information isn&rsquo;t expressed through words but rather through images, charts, or even videos. A single image can contain rich details like colors, shapes, spatial relationships, and more. These elements are often more intuitive, but also more complex than text.</p><p>For example, if you want to determine whether an architectural blueprint is reasonable, a description alone might not be enough. But if you could see the blueprint and analyze it using professional knowledge, the task becomes much easier. This is the significance of visual reasoning—it allows AI to not just &ldquo;see,&rdquo; but also &ldquo;understand&rdquo; and &ldquo;think.&rdquo;</p><p>Our goal in designing QVQ-Max was simple: to create an assistant that is both &ldquo;sharp-eyed&rdquo; and &ldquo;quick-thinking,&rdquo; capable of solving various practical problems for users.</p><hr><h3 id=core-capabilities-from-observation-to-reasoning><strong>Core Capabilities: From Observation to Reasoning</strong><a hidden class=anchor aria-hidden=true href=#core-capabilities-from-observation-to-reasoning>#</a></h3><p>The capabilities of QVQ-Max can be summarized into three areas: detailed observation, deep reasoning, and flexible application. Let&rsquo;s break down how it performs in each area.</p><ol><li><p><strong>Detailed Observation: Capturing Every Detail</strong><br>QVQ-Max excels at parsing images, whether they&rsquo;re complex charts or casual snapshots taken in daily life. It can quickly identify key elements in an image. For instance, it can tell you what objects are in a photo, what textual labels exist, and even point out small details that you might overlook.</p></li><li><p><strong>Deep Reasoning: Not Just &ldquo;Seeing,&rdquo; But Also &ldquo;Thinking&rdquo;</strong><br>Identifying content in an image is not enough. QVQ-Max can further analyze this information and combine it with background knowledge to draw conclusions. For example, in a geometry problem, it can derive answers based on the accompanying diagram. In a video clip, it can predict what might happen next based on the current scene.</p></li><li><p><strong>Flexible Application: From Problem-Solving to Creation</strong><br>Beyond analysis and reasoning, QVQ-Max can also perform interesting tasks like helping you design illustrations, generate short video scripts, or even create role-playing content based on your requirements. If you upload a rough sketch, it might help you refine it into a complete piece. Upload a regular photo, and it can transform into a sharp critic or even a fortune-teller.</p></li></ol><hr><h2 id=demo-cases><strong>Demo Cases</strong><a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h2><p>QVQ-Max has a wide range of applications, whether in learning, work, or daily life—it can come in handy in many scenarios.</p><ul><li><p><strong>Workplace Tool</strong>: At work, QVQ-Max can assist in completing data analysis, organizing information, and even writing code</p></li><li><p><strong>Learning Assistant</strong>: For students, QVQ-Max can help solve difficult problems in subjects like math and physics, especially those accompanied by diagrams. It can also explain complex concepts in an intuitive way, making learning easier.</p></li><li><p><strong>Life Helper</strong>: In daily life, QVQ-Max can offer practical advice. For instance, it can recommend outfit combinations based on photos of your wardrobe, or guide you through cooking a new dish based on recipe images.</p></li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Multi-image Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/travel.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Mathematical Reasoning</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/math.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Interpreting Palm Readings (For Reference Only)</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/Divination.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Video Understanding</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls src=http://qianwen-res.oss-accelerate.aliyuncs.com/QVQ-Max/video.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Learn to code by watching videos</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>QVQ-Max-Preview</div><div class=content><figure><video controls src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ-Max/video_and_code.mov></video></figure></div></div></div></div><hr><h2 id=next-step><strong>Next Step</strong><a hidden class=anchor aria-hidden=true href=#next-step>#</a></h2><p>The current version of QVQ-Max is just the first iteration, and there&rsquo;s still much room for improvement. Moving forward, we will focus on several key areas:</p><ol><li><strong>More Accurate Observations</strong>: Enhance recognition accuracy through grounding techniques, which validate observations made from visual content.</li><li><strong>Visual Agent</strong>: Improve the model’s ability to handle multi-step and more complex tasks, such as operating smartphones or computers, and even playing games.</li><li><strong>Better Interaction</strong>: Expand beyond text-based interaction to include more modalities, such as tool verification and visual generation, allowing for richer user experiences.</li></ol><p>Overall, QVQ-Max is a visual reasoning model that possesses both &ldquo;vision&rdquo; and &ldquo;intellect.&rdquo; It doesn’t just recognize the content in images; it combines this information to analyze, reason, and even complete creative tasks. Although it’s still in its growth phase, it has already shown great potential. Through continuous optimization, we aim to make QVQ-Max a truly practical visual agent that helps everyone solve real-world problems.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>