<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2.5: A Party of Foundation Models! | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history!"><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2.5: A Party of Foundation Models!"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history!"><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-09-19T00:00:04+08:00"><meta property="article:modified_time" content="2024-09-19T00:00:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2.5: A Party of Foundation Models!"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2.5: A Party of Foundation Models!","item":"https://qwenlm.github.io/blog/qwen2.5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2.5: A Party of Foundation Models!","name":"Qwen2.5: A Party of Foundation Models!","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In the past three months since Qwen2\u0026rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history!","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!\nOur latest release features the LLMs Qwen2.5, along with specialized models for coding, Qwen2.5-Coder, and mathematics, Qwen2.5-Math. All open-weight models are dense, decoder-only language models, available in various sizes, including:\nQwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B Qwen2.5-Coder: 1.5B, 7B, and 32B on the way Qwen2.5-Math: 1.5B, 7B, and 72B. All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen-Plus and Qwen-Turbo through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the Qwen2-VL-72B, which features performance enhancements compared to last monthâ€™s release.\nFor more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links:\nQwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math\nGet ready to unlock a world of possibilities with our extensive lineup of models! Weâ€™re excited to share these cutting-edge models with you, and we canâ€™t wait to see the incredible things youâ€™ll achieve with them!\nTakeaways In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.\nThe specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\nPerformance Qwen2.5 To showcase Qwen2.5â€™s capabilities, we benchmark our largest open-source model, Qwen2.5-72B - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences.\nBesides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B.\nFurthermore, we benchmark the latest version of our API-based model, Qwen-Plus, against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plusâ€™s competitive standing in the current landscape of large language models. We show that Qwen-Plus significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plusâ€™s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models.\nA significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, Qwen2.5-14B and Qwen2.5-32B. These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, Qwen-Turbo, offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service.\nIn recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our Qwen2.5-3B stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors.\nIn addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities.\nQwen2.5-Coder Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes.\nWe believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities.\nQwen2.5-Math In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models.\nDevelop with Qwen2.5 The simplest way to use is through Hugging Face Transfomer as demonstrated in the model card:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"Qwen/Qwen2.5-7B-Instruct\" model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) tokenizer = AutoTokenizer.from_pretrained(model_name) prompt = \"Give me a short introduction to large language model.\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) generated_ids = model.generate( **model_inputs, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service:\npython -m vllm.entrypoints.openai.api_server \\ --model Qwen/Qwen2.5-7B-Instruct or use vllm serve if you use vllm\u003e=0.5.3. Then you can communicate with Qwen2.5 via curl:\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"Qwen/Qwen2.5-7B-Instruct\", \"messages\": [ {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"} ], \"temperature\": 0.7, \"top_p\": 0.8, \"repetition_penalty\": 1.05, \"max_tokens\": 512 }' Furthermore, Qwen2.5 supports vllmâ€™s built-in tool calling. This functionality requires vllm\u003e=0.6. If you want to enable this functionality, please start vllmâ€™s OpenAI-compatible service with:\nvllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes You can then use it in the same way you use GPTâ€™s tool calling.\nQwen2.5 also supports Ollamaâ€™s tool calling. You can use it by starting Ollamaâ€™s OpenAI-compatible service and using it in the same way you use GPTâ€™s tool calling.\nQwen2.5â€™s chat template also includes a tool calling template, meaning that you can use Hugging Face transformersâ€™ tool calling support.\nThe vllm / Ollama / transformers tool calling support uses a tool calling template inspired by Nousâ€™ Hermes. Historically, Qwen-Agent provided tool calling support using Qwen2â€™s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2â€™s template and Qwen-Agent as well.\nFriends of Qwen ðŸ’— Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends :\nHugging Face Transformers\nFinetuning: Peft, ChatLearn, Llama-Factory, Axolotl, Firefly, Swift, XTuner, Unsloth, Liger Kernel\nQuantization: AutoGPTQ, AutoAWQ, Neural Compressor\nDeployment: vLLM, SGL, SkyPilot, TensorRT-LLM, OpenVino, TGI, Xinference\nAPI Platforms: Together, Fireworks, OpenRouter, Sillicon Flow\nLocal Run: MLX, Llama.cpp, Ollama, LM Studio, Jan\nAgent and RAG Frameworks: Dify, LlamaIndex, CrewAI\nEvaluation: LMSys, OpenCompass, Open LLM Leaderboard\nModel Training: Arcee AI, Sailor, Dolphin, Openbuddy\nWe would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they havenâ€™t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before.\nWhatâ€™s Next? While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our modelsâ€™ reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments!\nCitation We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog\n@misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} } @article{qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } ","wordCount":"1738","inLanguage":"en","datePublished":"2024-09-19T00:00:04+08:00","dateModified":"2024-09-19T00:00:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2.5: A Party of Foundation Models!</h1><div class=post-meta><span title='2024-09-19 00:00:04 +0800 +0800'>September 19, 2024</span>&nbsp;Â·&nbsp;9 min&nbsp;Â·&nbsp;1738 words&nbsp;Â·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><video width=100% autoplay loop muted playsinline>
<source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2-main-video.m4v type=video/mp4></video><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5 class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: <strong>Qwen2.5</strong>.
We are announcing what might be the largest opensource release in history! Let&rsquo;s get the party started!</p><p>Our latest release features the LLMs <strong>Qwen2.5</strong>, along with specialized models for coding, <strong>Qwen2.5-Coder</strong>, and mathematics, <strong>Qwen2.5-Math</strong>. All open-weight models are dense, decoder-only language models, available in various sizes, including:</p><ul><li>Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B</li><li>Qwen2.5-Coder: 1.5B, 7B, and 32B on the way</li><li>Qwen2.5-Math: 1.5B, 7B, and 72B.</li></ul><br><p>All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: <strong>Qwen-Plus</strong> and <strong>Qwen-Turbo</strong> through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the <strong>Qwen2-VL-72B</strong>, which features performance enhancements compared to last month&rsquo;s release.</p><p>For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links:</p><p><a href=https://qwenlm.github.io/blog/qwen2.5-llm class="btn external" target=_blank>Qwen2.5 LLM</a>
<a href=https://qwenlm.github.io/blog/qwen2.5-coder class="btn external" target=_blank>Qwen2.5-Coder</a>
<a href=https://qwenlm.github.io/blog/qwen2.5-math class="btn external" target=_blank>Qwen2.5-Math</a></p><br><p>Get ready to unlock a world of possibilities with our extensive lineup of models! We&rsquo;re excited to share these cutting-edge models with you, and we can&rsquo;t wait to see the incredible things you&rsquo;ll achieve with them!</p><h1 id=takeaways>Takeaways<a hidden class=anchor aria-hidden=true href=#takeaways>#</a></h1><p>In terms of <strong>Qwen2.5</strong>, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to <strong>18 trillion</strong> tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to <strong>128K</strong> tokens and can generate up to <strong>8K</strong> tokens. They also maintain multilingual support for over <strong>29</strong> languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.</p><p>The specialized expert language models, namely <strong>Qwen2.5-Coder</strong> for coding and <strong>Qwen2.5-Math</strong> for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on <strong>5.5 trillion</strong> tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both <strong>Chinese</strong> and <strong>English</strong> and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5%20modelcard.001.jpeg alt="Qwen2.5 Specification" width=100%></figure><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><h2 id=qwen25>Qwen2.5<a hidden class=anchor aria-hidden=true href=#qwen25>#</a></h2><p>To showcase Qwen2.5&rsquo;s capabilities, we benchmark our largest open-source model, <strong>Qwen2.5-72B</strong> - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-Instruct-Score.jpg alt="Qwen2.5-72B Instruct Performance" width=100%></figure><p>Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-base.001.jpeg alt="Qwen2.5-72B Base Model Performance" width=100%></figure><p>Furthermore, we benchmark the latest version of our API-based model, <strong>Qwen-Plus</strong>, against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus&rsquo;s competitive standing in the current landscape of large language models. We show that <strong>Qwen-Plus</strong> significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plus&rsquo;s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen-plus-instruct.001.jpeg alt="Qwen-Plus Instruct Performance" width=100%></figure><p>A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, <strong>Qwen2.5-14B</strong> and <strong>Qwen2.5-32B</strong>. These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, <strong>Qwen-Turbo</strong>, offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-32B-instruct_wturbo.001.jpeg alt="Qwen2.5-32B Instruct Performance" width=100%></figure><p>In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our <strong>Qwen2.5-3B</strong> stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-small.jpg alt="Qwen2.5 Small Model" width=100%></figure><p>In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities.</p><h2 id=qwen25-coder>Qwen2.5-Coder<a hidden class=anchor aria-hidden=true href=#qwen25-coder>#</a></h2><p>Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/coder-main.png alt="Qwen2.5-Coder Instruct Performance" width=100%></figure><p>We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities.</p><h2 id=qwen25-math>Qwen2.5-Math<a hidden class=anchor aria-hidden=true href=#qwen25-math>#</a></h2><p>In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-allsize.png alt="Qwen2.5 Math Performance Across All Sizes" width=100%></figure><h1 id=develop-with-qwen25>Develop with Qwen2.5<a hidden class=anchor aria-hidden=true href=#develop-with-qwen25>#</a></h1><p>The simplest way to use is through <a href>Hugging Face Transfomer</a> as demonstrated in the <a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct>model card</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;Qwen/Qwen2.5-7B-Instruct&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language model.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>python</span> <span class=o>-</span><span class=n>m</span> <span class=n>vllm</span><span class=o>.</span><span class=n>entrypoints</span><span class=o>.</span><span class=n>openai</span><span class=o>.</span><span class=n>api_server</span> \
</span></span><span class=line><span class=cl>    <span class=o>--</span><span class=n>model</span> <span class=n>Qwen</span><span class=o>/</span><span class=n>Qwen2</span><span class=mf>.5</span><span class=o>-</span><span class=mi>7</span><span class=n>B</span><span class=o>-</span><span class=n>Instruct</span>
</span></span></code></pre></div><p>or use <code>vllm serve</code> if you use <code>vllm>=0.5.3</code>. Then you can communicate with Qwen2.5 via <code>curl</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions -H <span class=s2>&#34;Content-Type: application/json&#34;</span> -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>  ],
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;temperature&#34;: 0.7,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;top_p&#34;: 0.8,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;repetition_penalty&#34;: 1.05,
</span></span></span><span class=line><span class=cl><span class=s1>  &#34;max_tokens&#34;: 512
</span></span></span><span class=line><span class=cl><span class=s1>}&#39;</span>
</span></span></code></pre></div><p>Furthermore, Qwen2.5 supports vllm&rsquo;s built-in tool calling. This functionality requires <code>vllm>=0.6</code>. If you want to enable this functionality, please start vllm&rsquo;s OpenAI-compatible service with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes
</span></span></code></pre></div><p>You can then use it in the same way you use <a href=https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models>GPT&rsquo;s tool calling</a>.</p><p>Qwen2.5 also supports <a href=https://ollama.com/blog/tool-support>Ollama&rsquo;s tool calling</a>. You can use it by starting Ollama&rsquo;s OpenAI-compatible service and using it in the same way you use GPT&rsquo;s tool calling.</p><p>Qwen2.5&rsquo;s chat template also includes a tool calling template, meaning that you can use Hugging Face <a href=https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling>transformers&rsquo; tool calling support</a>.</p><p>The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by <a href=https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B>Nous&rsquo; Hermes</a>. Historically, <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> provided tool calling support using Qwen2&rsquo;s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2&rsquo;s template and Qwen-Agent as well.</p><br><h1 id=friends-of-qwen>Friends of Qwen<a hidden class=anchor aria-hidden=true href=#friends-of-qwen>#</a></h1><p>ðŸ’— Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends :</p><ul><li><p><a href=https://huggingface.co/>Hugging Face Transformers</a></p></li><li><p>Finetuning: <a href=https://github.com/huggingface/peft>Peft</a>, <a href=https://github.com/alibaba/ChatLearn/>ChatLearn</a>, <a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>, <a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>, <a href=https://github.com/yangjianxin1/Firefly>Firefly</a>, <a href=https://github.com/modelscope/swift>Swift</a>, <a href=https://github.com/InternLM/xtuner>XTuner</a>, <a href=https://unsloth.ai/>Unsloth</a>, <a href=https://github.com/linkedin/Liger-Kernel>Liger Kernel</a></p></li><li><p>Quantization: <a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>, <a href=https://github.com/intel/neural-compressor>Neural Compressor</a></p></li><li><p>Deployment: <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/sgl-project/sglang>SGL</a>, <a href=https://github.com/skypilot-org/skypilot>SkyPilot</a>, <a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>, <a href=https://github.com/openvinotoolkit/openvino>OpenVino</a>, <a href=https://github.com/huggingface/text-generation-inference>TGI</a>, <a href=https://inference.readthedocs.io/>Xinference</a></p></li><li><p>API Platforms: <a href=https://www.together.ai/>Together</a>, <a href=https://fireworks.ai/>Fireworks</a>, <a href=https://openrouter.ai/>OpenRouter</a>, <a href=https://siliconflow.cn/>Sillicon Flow</a></p></li><li><p>Local Run: <a href=https://github.com/ml-explore/mlx>MLX</a>, <a href=https://github.com/ggerganov/llama.cpp>Llama.cpp</a>, <a href=https://ollama.com/>Ollama</a>, <a href=https://lmstudio.ai/>LM Studio</a>, <a href=https://jan.ai/>Jan</a></p></li><li><p>Agent and RAG Frameworks: <a href=https://dify.ai/>Dify</a>, <a href=https://www.llamaindex.ai/>LlamaIndex</a>, <a href=https://www.crewai.com/>CrewAI</a></p></li><li><p>Evaluation: <a href=https://chat.lmsys.org/>LMSys</a>, <a href=https://opencompass.org.cn/home>OpenCompass</a>, <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a></p></li><li><p>Model Training: <a href=https://www.arcee.ai/>Arcee AI</a>, <a href=https://sailorllm.github.io/>Sailor</a>, <a href=https://huggingface.co/cognitivecomputations>Dolphin</a>, <a href=https://github.com/OpenBuddy/OpenBuddy>Openbuddy</a></p></li></ul><p>We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven&rsquo;t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before.</p><h1 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models&rsquo; reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments!</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><p>We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog</p><pre tabindex=0><code>@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
</code></pre><pre tabindex=0><code>@article{qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>