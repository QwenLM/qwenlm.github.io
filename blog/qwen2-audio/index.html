<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen2-Audio: Chat with Your Voice! | Qwen</title><meta name=keywords content><meta name=description content="DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2-audio/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2-audio/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2-audio/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen2-Audio: Chat with Your Voice!"><meta property="og:description" content="DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2-audio/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-09T16:18:19+08:00"><meta property="article:modified_time" content="2024-08-09T16:18:19+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen2-Audio: Chat with Your Voice!"><meta name=twitter:description content="DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Qwen2-Audio: Chat with Your Voice!","item":"https://qwenlm.github.io/blog/qwen2-audio/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Qwen2-Audio: Chat with Your Voice!","name":"Qwen2-Audio: Chat with Your Voice!","description":"DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nTo achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs.","keywords":[],"articleBody":"DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nTo achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs. Qwen2-Audio has the following features:\nVoice Chat: for the first time, users can use the voice to give instructions to the audio-language model without ASR modules.\nAudio Analysis: the model is capable of analyzing audio information, including speech, sound, music, etc., with text instructions.\nMultilingual: the model supports more than 8 languages and dialects, e.g., Chinese, English, Cantonese, French, Italian, Spanish, German, and Japanese.\nWe open-weight Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct in Hugging Face and ModelScope, and we have built a demo for users to interact with. Below are some examples to show the model performance:\nVoice Chat Voice Chat: Identify speaker information and respond\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\rYes, the speaker is female and in her twenties. User\rYour browser does not support the audio element. Qwen2-Audio\rI'm sorry to hear that you're feeling stressed about your exam. It's completely normal to feel anxious when you're preparing for an important test. Here are some tips that might help you: 1. Start studying early: Don't leave everything until the last minute. Break up your study sessions into smaller chunks of time, and try to stick to a regular schedule. 2. Find a quiet place to study: A quiet environment can help you concentrate better. If possible, find a library or a study room where you can work without distractions. 3.Take breaks: It's important to take breaks during your study sessions. This will give your brain a chance to rest and recharge. Try taking short breaks every hour or so. 4. Get enough sleep: Lack of sleep can make it harder to focus and retain information. Make sure you're getting enough sleep each night, ideally around 7-8 hours. 5. Practice relaxation techniques: Deep breathing exercises, meditation, or yoga can all help you relax and reduce stress. Remember, it's okay if you're not feeling well-prepared. Just do your best, and trust yourself. Good luck with your exam! Voice Chat: Speech translation\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\r每个人都希望被欣赏，所以如果你欣赏某人，不要把它保密。 User\rYour browser does not support the audio element. Qwen2-Audio\rJeder möchte geschätzt werden, also wenn du jemanden schätzt, lass es nicht als Geheimnis verbergen. User\rYour browser does not support the audio element. Qwen2-Audio\rChacun veut être apprecié, alors si tu apprécies quelqu'un, ne le gardes pas secret. User\rYour browser does not support the audio element. Qwen2-Audio\r1. 每个人都喜欢被赞美，所以如果你喜欢一个人，不要隐藏你的赞赏。 2. 所有人都渴望得到认可，因此如果你欣赏某人，请不要独自享用这种感觉。 3. 人们都希望被欣赏，所以如果你喜欢某人，不要独自保密欣赏他人是一种美德，所以不要让别人知道你对他们的赞赏是秘密的。 4. 人们都希望受到赞扬，所以如果你欣赏某人，不妨公开表达出来。 5. 如果你喜欢某人，不要犹豫告诉他们，因为每个人都有权利享受他人的赞赏。 Voice Chat: Detecting background noise and responding accordingly\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\rUse headphones to block out external noise and find a quiet place to study. Prioritize your tasks and set realistic goals for yourself. Use active learning techniques such as summarizing, note-taking, and quizzing to help you retain information. Take breaks and practice self-care to avoid burnout. User\rYour browser does not support the audio element. Qwen2-Audio\rYes, you can try negotiating with them to reach a mutually beneficial agreement. You could also consider seeking help from a mediator or professional negotiator if needed. Audio Analysis Audio Analysis : Speech transcription and story telling\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\rThe original content of this audio is: 'Stop all the clocks, cut off the telephone, prevent the dog from barking, silence the pianos and with muffled drum bring out the coffin, let the mourners come.' User\rNo less than 50 words and no more than 200 words, add scenes such as rain and thunder to the poem. Finally, give the verses you created in lines. Qwen2-Audio\rIn the midst of an eerie stormy night, With raindrops falling heavy and bright, The world seemed to hush its voice, As winds howled through the darkened space. Stop all the clocks, the time stood still, As the lightning flashed through the sky, The telephone's ring was drowned out, By the roaring thunder that shook the ground. Prevent the dog from barking, its cries were silenced, By the fear that gripped it in the night, Silence the pianos, their keys were muted, As the mournful sound of raindrops beat on wood. With muffled drums, a slow march began, A solemn procession through the storm, The coffin was carried, heavy and solemn, Let the mourners come, to pay their final respects. As the storm slowly passed, the world emerged, A new day dawned, but memories lingered, Of the night when time stood still, And the mourners came, to say goodbye. Audio Analysis: Sound analysis\rNext\rUser\rWhat do you hear? Your browser does not support the audio element. Qwen2-Audio\rI heard the sound of an alarm and a truck's air brake mixed with the noise of a heavy engine running and shifting gears. User\rUnder what circumstances can you hear such sounds? Qwen2-Audio\rThese types of sounds might be commonly heard in industrial settings like construction sites or factories where machinery is operated, or during emergency situations like traffic accidents where sirens are triggered. User\rYour browser does not support the audio element. Qwen2-Audio\rIt's understandable to feel scared under any situation that makes you feel uncomfortable. It's important to remember that it's okay to be scared. Here are some things you can do: 1. Take deep breaths: Inhale deeply through your nose and exhale slowly through your mouth. This can help calm your mind and body. 2. Find a safe place: If possible, move to a safe location away from the source of the fear. If you're in a car, pull over to a parking lot or a safe spot on the side of the road. 3. Stay informed: If you have access to information about the situation, stay informed but don't overdo it. Too much information can create more anxiety. 4. Talk to someone: Talking to someone you trust can help you process your feelings and gain perspective. You can call a friend, family member, or even a professional counselor if needed. 5. Practice relaxation techniques: Listening to music, doing yoga or meditation, or engaging in other relaxing activities can help reduce stress and anxiety. Remember, it's okay to take time for yourself to process your emotions and come up with a plan that works best for you. Audio Analysis: Music analysis\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\rThis is a pop punk music piece. There is a male vocalist singing melodically in the lead. The tune is being played by the guitar with the bass guitar playing in the background. The rhythmic background consists of loud and aggressive sounding drums. The atmosphere is rebellious. This piece could be used in the soundtrack of a high school drama movie or TV show during scenes where the characters are rebelling against authority. User\rYour browser does not support the audio element. Qwen2-Audio\rThe key of this music is F# major. User\rYour browser does not support the audio element. Qwen2-Audio\rThe tempo of this music is 104.17 bpm. User\rYour browser does not support the audio element. Qwen2-Audio\rThe time signature of this music is 4/4. Audio Analysis: Robustness of mixed audio analysis\rNext\rUser\rYour browser does not support the audio element. Qwen2-Audio\rThe old man laid down his hand to light a cigar. User\rYour browser does not support the audio element. Qwen2-Audio\rThe old man laid down his hand to light a cigar. User\rYour browser does not support the audio element. Qwen2-Audio\rThe old man laid down his hand to light a cigar. User\rYour browser does not support the audio element. Qwen2-Audio\rThe old man laid down his hand to light a cigar. User\rYour browser does not support the audio element. Qwen2-Audio\rThe lyrics of the singing are 'Waiting for my favorite song.' User\rYour browser does not support the audio element. Qwen2-Audio\rThe lyrics of the singing are 'Waiting for my favorite song.' Performance We have conducted a series of experiments on benchmark datasets, including LibriSpeech, Common Voice 15, Fleurs, Aishell2, CoVoST2, Meld, Vocalsound, and AIR-Benchmark, to evaluate the performance of Qwen2-Audio in comparison with our previously released Qwen-Audio as well as the state-of-the-art models in each tasks. Below we demonstrate a figure to show how Qwen2-Audio performs against the competitors. In all the tasks, Qwen2-Audio significantly surpasses either previous SOTAs or Qwen-Audio.\nMore specific results on the datasets are listed below in the table.\nArchitecture Below is a demonstration of the training architecture. Specifically, we start from the Qwen language model and an audio encoder as the foundation models. We sequentially apply multi-task pretraining for the audio language alignment, and supervised finetuning as well as direct preference optimization to grasp the capabilities of downstream tasks and model human preference.\nHow to Use Now Qwen2-Audio is officially supported by Hugging Face Transfomers. We advise you install the latest version of transformers from source:\npip install git+https://github.com/huggingface/transformers We demonstrate how to use Qwen2-Audio-7B-Instruct to play with voice chat and audio analysis. Here is an example of voice chat:\nfrom io import BytesIO from urllib.request import urlopen import librosa from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\") model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\") conversation = [ {\"role\": \"user\", \"content\": [ {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"}, ]}, {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"}, {\"role\": \"user\", \"content\": [ {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"}, ]}, ] text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) audios = [] for message in conversation: if isinstance(message[\"content\"], list): for ele in message[\"content\"]: if ele[\"type\"] == \"audio\": audios.append(librosa.load( BytesIO(urlopen(ele['audio_url']).read()), sr=processor.feature_extractor.sampling_rate)[0] ) inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True) inputs.input_ids = inputs.input_ids.to(\"cuda\") generate_ids = model.generate(**inputs, max_length=256) generate_ids = generate_ids[:, inputs.input_ids.size(1):] response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] In voice chat mode, the user input is simply the audio without text, and the instructions from the users are inside the audio. Next is an example of audio analysis:\nfrom io import BytesIO from urllib.request import urlopen import librosa from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\") model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\") conversation = [ {'role': 'system', 'content': 'You are a helpful assistant.'}, {\"role\": \"user\", \"content\": [ {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"}, {\"type\": \"text\", \"text\": \"What's that sound?\"}, ]}, {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"}, {\"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"}, ]}, {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"}, {\"role\": \"user\", \"content\": [ {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"}, {\"type\": \"text\", \"text\": \"What does the person say?\"}, ]}, ] text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) audios = [] for message in conversation: if isinstance(message[\"content\"], list): for ele in message[\"content\"]: if ele[\"type\"] == \"audio\": audios.append( librosa.load( BytesIO(urlopen(ele['audio_url']).read()), sr=processor.feature_extractor.sampling_rate)[0] ) inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True) inputs.input_ids = inputs.input_ids.to(\"cuda\") generate_ids = model.generate(**inputs, max_length=256) generate_ids = generate_ids[:, inputs.input_ids.size(1):] response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] In comparison, in audio analysis mode, there is additional text instructions. However, switching between either mode only needs to modify your user inputs and there is no need to worry about others such as system prompt.\nNext Step This time we bring a new audio language model, Qwen2-Audio, which supports voice chat and audio analysis at the same time and understands more than 8 languages and dialects. In the near future, we plan to train improved Qwen2-Audio models on larger pretraining datasets, enabling the model to support longer audio (over 30s). We also plan to build larger Qwen2-Audio models to explore the scaling laws of audio language models.\n","wordCount":"1999","inLanguage":"en","datePublished":"2024-08-09T16:18:19+08:00","dateModified":"2024-08-09T16:18:19+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2-audio/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Qwen2-Audio: Chat with Your Voice!</h1><div class=post-meta><span title='2024-08-09 16:18:19 +0800 +0800'>August 9, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1999 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2-audio/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo class="btn external" target=_blank>DEMO</a>
<a href=https://arxiv.org/pdf/2407.10759 class="btn external" target=_blank>PAPER</a>
<a href=https://github.com/QwenLM/Qwen2-Audio class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/collections/Qwen/qwen2-audio-66b628d694096020e0c52ff6 class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs. Qwen2-Audio has the following features:</p><ul><li><p>Voice Chat: for the first time, users can use the voice to give instructions to the audio-language model without ASR modules.</p></li><li><p>Audio Analysis: the model is capable of analyzing audio information, including speech, sound, music, etc., with text instructions.</p></li><li><p>Multilingual: the model supports more than 8 languages and dialects, e.g., Chinese, English, Cantonese, French, Italian, Spanish, German, and Japanese.</p></li></ul><br><p>We open-weight Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct in Hugging Face and ModelScope, and we have built a demo for users to interact with. Below are some examples to show the model performance:</p><style>.example-content .grid-layout{display:grid;grid-template-columns:1fr;row-gap:var(--content-gap)}.example-container{width:100%;padding:0;margin:0}.grid-layout{display:flex;flex-direction:column;width:100%;grid-template-columns:1fr}.message{display:flex;flex-direction:column;align-items:flex-start;width:100%}.role{font-weight:700;margin-right:5px;white-space:nowrap}.content{display:flex;align-items:center;white-space:pre-wrap;width:100%;margin-bottom:10px;margin-left:50px}.wide-content{width:100%}.audio-icon{flex-shrink:0;margin-right:10px;margin-bottom:10px}.text-content{width:90%;white-space:pre-wrap;margin-top:10px}audio{vertical-align:middle}</style><h3 id=voice-chat>Voice Chat<a hidden class=anchor aria-hidden=true href=#voice-chat>#</a></h3><div class="full-width-container example-container"><div class=example-content><div class=title><span>Voice Chat: Identify speaker information and respond</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/guess age.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Yes, the speaker is female and in her twenties.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/1718857224593-3efbd24f-f7f8-4f10-abba-24df5d92210b.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>I'm sorry to hear that you're feeling stressed about your exam. It's completely normal to feel anxious when you're preparing for an important test. Here are some tips that might help you:
&nbsp
1. Start studying early: Don't leave everything until the last minute. Break up your study sessions into smaller chunks of time, and try to stick to a regular schedule.
2. Find a quiet place to study: A quiet environment can help you concentrate better. If possible, find a library or a study room where you can work without distractions.
3.Take breaks: It's important to take breaks during your study sessions. This will give your brain a chance to rest and recharge. Try taking short breaks every hour or so.
4. Get enough sleep: Lack of sleep can make it harder to focus and retain information. Make sure you're getting enough sleep each night, ideally around 7-8 hours.
5. Practice relaxation techniques: Deep breathing exercises, meditation, or yoga can all help you relax and reduce stress.
&nbsp
Remember, it's okay if you're not feeling well-prepared. Just do your best, and trust yourself. Good luck with your exam!</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Voice Chat: Speech translation</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/1718864196340-99f0394d-7d0b-4b4b-b86a-4d8f3a2ac09c.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>每个人都希望被欣赏，所以如果你欣赏某人，不要把它保密。</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/translate it into German.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Jeder möchte geschätzt werden, also wenn du jemanden schätzt, lass es nicht als Geheimnis verbergen.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/how about into French.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Chacun veut être apprecié, alors si tu apprécies quelqu'un, ne le gardes pas secret.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/给我生成一些类似这句话意思的，其他说法的句子，用中文生成.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>1. 每个人都喜欢被赞美，所以如果你喜欢一个人，不要隐藏你的赞赏。
2. 所有人都渴望得到认可，因此如果你欣赏某人，请不要独自享用这种感觉。
3. 人们都希望被欣赏，所以如果你喜欢某人，不要独自保密欣赏他人是一种美德，所以不要让别人知道你对他们的赞赏是秘密的。
4. 人们都希望受到赞扬，所以如果你欣赏某人，不妨公开表达出来。
5. 如果你喜欢某人，不要犹豫告诉他们，因为每个人都有权利享受他人的赞赏。</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Voice Chat: Detecting background noise and responding accordingly</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/audio-0-100.wav.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Use headphones to block out external noise and find a quiet place to study. Prioritize your tasks and set realistic goals for yourself. Use active learning techniques such as summarizing, note-taking, and quizzing to help you retain information. Take breaks and practice self-care to avoid burnout.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-noise-2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Yes, you can try negotiating with them to reach a mutually beneficial agreement. You could also consider seeking help from a mediator or professional negotiator if needed.</div></div></div></div></div></div><h3 id=audio-analysis>Audio Analysis<a hidden class=anchor aria-hidden=true href=#audio-analysis>#</a></h3><div class="full-width-container example-container"><div class=example-content><div class=title><span>Audio Analysis : Speech transcription and story telling</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-speech-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The original content of this audio is: 'Stop all the clocks, cut off the telephone, prevent the dog from barking, silence the pianos and with muffled drum bring out the coffin, let the mourners come.'</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>No less than 50 words and no more than 200 words, add scenes such as rain and thunder to the poem. Finally, give the verses you created in lines.</div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>In the midst of an eerie stormy night, With raindrops falling heavy and bright, The world seemed to hush its voice, As winds howled through the darkened space.
&nbsp
Stop all the clocks, the time stood still, As the lightning flashed through the sky, The telephone's ring was drowned out, By the roaring thunder that shook the ground.
&nbsp
Prevent the dog from barking, its cries were silenced, By the fear that gripped it in the night, Silence the pianos, their keys were muted, As the mournful sound of raindrops beat on wood.
&nbsp
With muffled drums, a slow march began, A solemn procession through the storm, The coffin was carried, heavy and solemn, Let the mourners come, to pay their final respects.
&nbsp
As the storm slowly passed, the world emerged, A new day dawned, but memories lingered, Of the night when time stood still, And the mourners came, to say goodbye.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Sound analysis</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>What do you hear?
<audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/sound-3-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>I heard the sound of an alarm and a truck's air brake mixed with the noise of a heavy engine running and shifting gears.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>Under what circumstances can you hear such sounds?</div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>These types of sounds might be commonly heard in industrial settings like construction sites or factories where machinery is operated, or during emergency situations like traffic accidents where sirens are triggered.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-sound-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>It's understandable to feel scared under any situation that makes you feel uncomfortable. It's important to remember that it's okay to be scared. Here are some things you can do:
&nbsp
1. Take deep breaths: Inhale deeply through your nose and exhale slowly through your mouth. This can help calm your mind and body.
2. Find a safe place: If possible, move to a safe location away from the source of the fear. If you're in a car, pull over to a parking lot or a safe spot on the side of the road.
3. Stay informed: If you have access to information about the situation, stay informed but don't overdo it. Too much information can create more anxiety.
4. Talk to someone: Talking to someone you trust can help you process your feelings and gain perspective. You can call a friend, family member, or even a professional counselor if needed.
5. Practice relaxation techniques: Listening to music, doing yoga or meditation, or engaging in other relaxing activities can help reduce stress and anxiety.
&nbsp
Remember, it's okay to take time for yourself to process your emotions and come up with a plan that works best for you.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Music analysis</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/_zVFfYfyyFg.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>This is a pop punk music piece. There is a male vocalist singing melodically in the lead. The tune is being played by the guitar with the bass guitar playing in the background. The rhythmic background consists of loud and aggressive sounding drums. The atmosphere is rebellious. This piece could be used in the soundtrack of a high school drama movie or TV show during scenes where the characters are rebelling against authority.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/key.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The key of this music is F# major.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/tempo.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The tempo of this music is 104.17 bpm.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/timesignature.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The time signature of this music is 4/4.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Robustness of mixed audio analysis</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_0.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_3.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/robust_1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The lyrics of the singing are 'Waiting for my favorite song.'</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/robust_2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The lyrics of the singing are 'Waiting for my favorite song.'</div></div></div></div></div></div><h2 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h2><p>We have conducted a series of experiments on benchmark datasets, including LibriSpeech, Common Voice 15, Fleurs, Aishell2, CoVoST2, Meld, Vocalsound, and AIR-Benchmark, to evaluate the performance of Qwen2-Audio in comparison with our previously released Qwen-Audio as well as the state-of-the-art models in each tasks. Below we demonstrate a figure to show how Qwen2-Audio performs against the competitors. In all the tasks, Qwen2-Audio significantly surpasses either previous SOTAs or Qwen-Audio.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/radar_compare_qwen_audio.png#center width=80%></figure><p>More specific results on the datasets are listed below in the table.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/table.jpg#center width=100%></figure><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>Below is a demonstration of the training architecture. Specifically, we start from the Qwen language model and an audio encoder as the foundation models. We sequentially apply multi-task pretraining for the audio language alignment, and supervised finetuning as well as direct preference optimization to grasp the capabilities of downstream tasks and model human preference.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/framework.png#center width=80%></figure><h2 id=how-to-use>How to Use<a hidden class=anchor aria-hidden=true href=#how-to-use>#</a></h2><p>Now Qwen2-Audio is officially supported by Hugging Face Transfomers. We advise you install the latest version of <code>transformers</code> from source:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install git+https://github.com/huggingface/transformers
</span></span></code></pre></div><p>We demonstrate how to use <code>Qwen2-Audio-7B-Instruct</code> to play with voice chat and audio analysis. Here is an example of voice chat:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>io</span> <span class=kn>import</span> <span class=n>BytesIO</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.request</span> <span class=kn>import</span> <span class=n>urlopen</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>librosa</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>Qwen2AudioForConditionalGeneration</span><span class=p>,</span> <span class=n>AutoProcessor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-Audio-7B-Instruct&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Qwen2AudioForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-Audio-7B-Instruct&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>conversation</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio_url&#34;</span><span class=p>:</span> <span class=s2>&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Yes, the speaker is female and in her twenties.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio_url&#34;</span><span class=p>:</span> <span class=s2>&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>conversation</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>audios</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>message</span> <span class=ow>in</span> <span class=n>conversation</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>message</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>],</span> <span class=nb>list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>ele</span> <span class=ow>in</span> <span class=n>message</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>ele</span><span class=p>[</span><span class=s2>&#34;type&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>audios</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>librosa</span><span class=o>.</span><span class=n>load</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>BytesIO</span><span class=p>(</span><span class=n>urlopen</span><span class=p>(</span><span class=n>ele</span><span class=p>[</span><span class=s1>&#39;audio_url&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>read</span><span class=p>()),</span> 
</span></span><span class=line><span class=cl>                    <span class=n>sr</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>feature_extractor</span><span class=o>.</span><span class=n>sampling_rate</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span><span class=n>text</span><span class=o>=</span><span class=n>text</span><span class=p>,</span> <span class=n>audios</span><span class=o>=</span><span class=n>audios</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generate_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generate_ids</span> <span class=o>=</span> <span class=n>generate_ids</span><span class=p>[:,</span> <span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>):]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generate_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>In voice chat mode, the user input is simply the audio without text, and the instructions from the users are inside the audio. Next is an example of audio analysis:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>io</span> <span class=kn>import</span> <span class=n>BytesIO</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.request</span> <span class=kn>import</span> <span class=n>urlopen</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>librosa</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>Qwen2AudioForConditionalGeneration</span><span class=p>,</span> <span class=n>AutoProcessor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-Audio-7B-Instruct&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Qwen2AudioForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-Audio-7B-Instruct&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>conversation</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;system&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;You are a helpful assistant.&#39;</span><span class=p>},</span> 
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio_url&#34;</span><span class=p>:</span> <span class=s2>&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What&#39;s that sound?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;It is the sound of glass shattering.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What can you do when you hear that?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio_url&#34;</span><span class=p>:</span> <span class=s2>&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What does the person say?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>conversation</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>audios</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>message</span> <span class=ow>in</span> <span class=n>conversation</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>message</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>],</span> <span class=nb>list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>ele</span> <span class=ow>in</span> <span class=n>message</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>ele</span><span class=p>[</span><span class=s2>&#34;type&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>audios</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>librosa</span><span class=o>.</span><span class=n>load</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>BytesIO</span><span class=p>(</span><span class=n>urlopen</span><span class=p>(</span><span class=n>ele</span><span class=p>[</span><span class=s1>&#39;audio_url&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>read</span><span class=p>()),</span> 
</span></span><span class=line><span class=cl>                        <span class=n>sr</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>feature_extractor</span><span class=o>.</span><span class=n>sampling_rate</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span><span class=n>text</span><span class=o>=</span><span class=n>text</span><span class=p>,</span> <span class=n>audios</span><span class=o>=</span><span class=n>audios</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generate_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generate_ids</span> <span class=o>=</span> <span class=n>generate_ids</span><span class=p>[:,</span> <span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>):]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generate_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>In comparison, in audio analysis mode, there is additional text instructions. However, switching between either mode only needs to modify your user inputs and there is no need to worry about others such as system prompt.</p><h2 id=next-step>Next Step<a hidden class=anchor aria-hidden=true href=#next-step>#</a></h2><p>This time we bring a new audio language model, Qwen2-Audio, which supports voice chat and audio analysis at the same time and understands more than 8 languages and dialects. In the near future, we plan to train improved Qwen2-Audio models on larger pretraining datasets, enabling the model to support longer audio (over 30s). We also plan to build larger Qwen2-Audio models to explore the scaling laws of audio language models.</p></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>