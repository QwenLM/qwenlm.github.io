<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Qwen</title>
    <link>http://qwenlm.github.io/blog/</link>
    <description>Recent content in Blog on Qwen</description>
    <image>
      <url>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 28 Mar 2024 11:31:44 +0800</lastBuildDate><atom:link href="http://qwenlm.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters</title>
      <link>http://qwenlm.github.io/blog/qwen-moe/</link>
      <pubDate>Thu, 28 Mar 2024 11:31:44 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-moe/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction Since the surge in interest sparked by Mixtral, research on mixture-of-expert (MoE) models has gained significant momentum. Both researchers and practitioners are keenly interested in understanding how to effectively train such models and assessing their efficiency and effectiveness. Today, we introduce Qwen1.5-MoE-A2.7B, a small MoE model with only 2.7 billion activated parameters yet matching the performance of state-of-the-art 7B models like Mistral 7B and Qwen1.</description>
    </item>
    
    <item>
      <title>Introducing Qwen1.5</title>
      <link>http://qwenlm.github.io/blog/qwen1.5/</link>
      <pubDate>Sun, 04 Feb 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen1.5/</guid>
      <description>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction In recent months, our focus has been on developing a &amp;ldquo;good&amp;rdquo; model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year.
With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B, and also an MoE model (see blog for more information).</description>
    </item>
    
    <item>
      <title>Introducing Qwen-VL</title>
      <link>http://qwenlm.github.io/blog/qwen-vl/</link>
      <pubDate>Thu, 25 Jan 2024 13:33:00 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen-vl/</guid>
      <description>Along with the rapid development of our large language model Qwen, we leveraged Qwenâ€™s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:
Substantially boost in image-related reasoning capabilities; Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein; Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.</description>
    </item>
    
    <item>
      <title>Introducing Qwen</title>
      <link>http://qwenlm.github.io/blog/qwen/</link>
      <pubDate>Tue, 23 Jan 2024 22:13:29 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/qwen/</guid>
      <description>4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.</description>
    </item>
    
    <item>
      <title>OFA: Towards Building a One-For-All Model</title>
      <link>http://qwenlm.github.io/blog/ofa/</link>
      <pubDate>Mon, 14 Nov 2022 16:01:41 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/ofa/</guid>
      <description>2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.</description>
    </item>
    
    <item>
      <title>OFASys: Enabling Multitask Learning with One Line of Code! </title>
      <link>http://qwenlm.github.io/blog/ofasys/</link>
      <pubDate>Wed, 28 Dec 2022 18:01:21 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/ofasys/</guid>
      <description>Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.</description>
    </item>
    
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>http://qwenlm.github.io/blog/chinese-clip/</link>
      <pubDate>Sat, 24 Dec 2022 14:54:19 +0800</pubDate>
      
      <guid>http://qwenlm.github.io/blog/chinese-clip/</guid>
      <description>CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.</description>
    </item>
    
  </channel>
</rss>
