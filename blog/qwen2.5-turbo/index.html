<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Extending the Context Length to 1M Tokens! | Qwen</title><meta name=keywords content><meta name=description content="API Documentation (Chinese) HuggingFace Demo ModelScope Demo
Introduction After the release of Qwen2.5, we heard the community&rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:
Longer Context Support: We have extended the model&rsquo;s context length from 128k to 1M, which is approximately 1 million English words or 1."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2.5-turbo/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2.5-turbo/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2.5-turbo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Extending the Context Length to 1M Tokens!"><meta property="og:description" content="API Documentation (Chinese) HuggingFace Demo ModelScope Demo
Introduction After the release of Qwen2.5, we heard the community&rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:
Longer Context Support: We have extended the model&rsquo;s context length from 128k to 1M, which is approximately 1 million English words or 1."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2.5-turbo/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-11-15T00:00:03+08:00"><meta property="article:modified_time" content="2024-11-15T00:00:03+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Extending the Context Length to 1M Tokens!"><meta name=twitter:description content="API Documentation (Chinese) HuggingFace Demo ModelScope Demo
Introduction After the release of Qwen2.5, we heard the community&rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:
Longer Context Support: We have extended the model&rsquo;s context length from 128k to 1M, which is approximately 1 million English words or 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Extending the Context Length to 1M Tokens!","item":"https://qwenlm.github.io/blog/qwen2.5-turbo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Extending the Context Length to 1M Tokens!","name":"Extending the Context Length to 1M Tokens!","description":"API Documentation (Chinese) HuggingFace Demo ModelScope Demo\nIntroduction After the release of Qwen2.5, we heard the community\u0026rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:\nLonger Context Support: We have extended the model\u0026rsquo;s context length from 128k to 1M, which is approximately 1 million English words or 1.","keywords":[],"articleBody":"API Documentation (Chinese) HuggingFace Demo ModelScope Demo\nIntroduction After the release of Qwen2.5, we heard the community’s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:\nLonger Context Support: We have extended the model’s context length from 128k to 1M, which is approximately 1 million English words or 1.5 million Chinese characters, equivalent to 10 full-length novels, 150 hours of speech transcripts, or 30,000 lines of code. The model achieves 100% accuracy in the 1M length Passkey Retrieval task and scores 93.1 on the long text evaluation benchmark RULER, surpassing GPT-4’s 91.6 and GLM4-9B-1M’s 89.9. Additionally, the model maintains very strong competitiveness in short sequence capabilities, on par with GPT-4o-mini. Faster Inference Speed: Using sparse attention mechanisms, we successfully reduced the time to first token for processing a context of 1M tokens from 4.9 minutes to 68 seconds, achieving a 4.3x speedup. Lower Cost: The price remains ¥0.3 / 1M tokens. At the same cost, Qwen2.5-Turbo can process 3.6 times the number of tokens as GPT-4o-mini. Now, you can use it through the API service of Alibaba Cloud Model Studio [Chinese], or through HuggingFace Demo or ModelScope Demo.\nDemo We have prepared some demos to showcase the new uses that Qwen2.5-Turbo can bring, covering deep understanding of long novels, repository-level code assistant, and reading multiple papers.\nExample: Understanding Long Novels\rNext\rUser\r[Upload the three Chinese novels of The Three-Body Problem, which contain 690k tokens.]\nProvide a summary of the plots of these three novels in English\nQwen2.5-Turbo\rThe three novels you mentioned from the “Earth’s Past” series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.\nFirst Novel: “Earth’s Past” In the first novel, the narrative focuses on the development of a top-secret project called “Red Coast” (红岸) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.\nSecond Novel: “Dark Forest” The second novel, titled “Dark Forest,” delves into the concept of the “Dark Forest Theory.” According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as “water drops” (水滴). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the “Wall-Bearing Plan” (面壁计划), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.\nThird Novel: “Death’s End” In “Death’s End,” Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (执剑人) brings him immense personal struggle and psychological turmoil. As Luo Ji’s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (黑域) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth. The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.\nExample: Understanding Repository\rNext\rUser\r[Upload a text file containing all code files from the Qwen-Agent repository, which has a length of 133k tokens.]\nPlease read the codes carefully and ensure your response is comprehensive. Introduce this repository\nQwen2.5-Turbo\rQwen-Agent is a framework designed for developing LLM (Large Language Model) applications. It is based on the instruction-following, tool usage, planning, and memory capabilities of Qwen. Here are some key points and functionalities:\nAgent Class: Provides a high-level interface for integrating tool calls and LLM interactions. It supports role-playing, automatic planning, and tool invocation. DocQA Agent: Specialized for document question answering, capable of handling long documents and providing context-aware responses. Function Calling: Supports function calling, enabling interaction with tools or APIs directly from the model’s responses. RAG (Retrieval-Augmented Generation): Integrates document retrieval with generation, enhancing the model’s ability to recall and use information from documents. Tool Integration: Includes various built-in and custom tools, such as code interpreters, web extractors, and image generators, which can be used to perform tasks like code execution, web scraping, and image manipulation. Multi-Agent Systems: Supports the creation of multi-agent systems where multiple agents can interact, simulate conversations, and collaborate on tasks. Customization: Highly customizable, allowing developers to tailor the framework to specific needs, such as creating custom agents, defining custom workflows, and integrating additional tools. Examples and Demos: Comes with several examples and demos, such as a Chrome browser extension called BrowserQwen, which demonstrates the practical application of Qwen-Agent in a real-world scenario. Testing and Debugging: Includes testing and debugging utilities, such as test cases and logging mechanisms, to ensure the reliability and effectiveness of the developed applications. Dependencies: Requires certain dependencies to be installed for specific functionalities, such as qwen-agent[gui] for GUI support and qwen-agent[rag] for RAG support. This repository is valuable for researchers, developers, and anyone interested in leveraging large language models for complex tasks and applications. User\rWhat are the subclasses of Agent in this repository, and provide their file paths\rQwen2.5-Turbo\rHere are the subclasses of the Agent class in the Qwen-Agent repository along with their file paths:\nAssistant File Path: qwen_agent/agents/assistant.py ArticleAgent File Path: qwen_agent/agents/article_agent.py BasicDocQA File Path: qwen_agent/agents/doc_qa/basic_doc_qa.py DialogueRetrievalAgent File Path: qwen_agent/agents/dialogue_retrieval_agent.py DialogueSimulator File Path: qwen_agent/agents/dialogue_simulator.py DocQAAgent (Note: The actual implementation of DocQAAgent may vary with each release) File Path: qwen_agent/agents/doc_qa/basic_doc_qa.py GroupChat File Path: qwen_agent/agents/group_chat.py GroupChatAutoRouter File Path: qwen_agent/agents/group_chat_auto_router.py GroupChatCreator File Path: qwen_agent/agents/group_chat_creator.py HumanSimulator File Path: qwen_agent/agents/human_simulator.py ReActChat File Path: qwen_agent/agents/react_chat.py Router File Path: qwen_agent/agents/router.py TIRMathAgent File Path: qwen_agent/agents/tir_agent.py UserAgent File Path: qwen_agent/agents/user_agent.py VisualStorytelling File Path: qwen_agent/agents/visual_storytelling.py MemoAssistant File Path: qwen_agent/agents/memo_assistant.py WriteFromScratch File Path: qwen_agent/agents/writing/write_from_scratch.py ExpandWriting File Path: qwen_agent/agents/writing/expand_writing.py OutlineWriting File Path: qwen_agent/agents/writing/outline_writing.py These subclasses extend the base Agent class to provide specialized functionalities tailored for different use cases within the Qwen-Agent framework.\nExample: Understanding Multiple Papers\rNext\rUser\r[Upload 7 papers about long-context LLM, which have a length of 171k tokens. Sources: 1 2 3 4 5 6 7]\nclassify the above papers into two categories\nQwen2.5-Turbo\rThe papers provided can be classified into two categories based on their focus:\nCategory 1: Long-Context Scaling Techniques\nDCA (Dual Chunk Attention): This paper introduces a new training-free framework to allow large language models to handle long-context sequences by breaking down the attention computation into smaller chunks. (File: 1. DCA.pdf) MInference: This paper proposes a method to accelerate pre-filling for long-context language models by using a dynamic sparse attention mechanism with a context memory module. (File: 2. minference.pdf) InfLLM: This paper presents a training-free memory-based approach to enable large language models to understand extremely long sequences by incorporating an efficient context memory mechanism. (File: 4. infLLM.pdf) SnapKV: This paper introduces a fine-tuning-free approach to efficiently minimize the key-value (KV) cache size of large language models during generation while maintaining comparable performance. (File: 7. SnapKV.pdf) Category 2: Benchmarking Long-Context Capabilities\nLV-Eval: This paper introduces a challenging long-context benchmark designed to evaluate the capabilities of large language models across various lengths and tasks, with a focus on mitigating knowledge leakage and providing a balanced evaluation. (File: 3. lveval.pdf) NeedleBench: This paper presents a benchmark for evaluating the retrieval and reasoning capabilities of large language models in long-context scenarios, including a needle-in-a-haystack task. (File: 5. needlebench.pdf) RULER: This paper proposes a synthetic benchmark for evaluating long-context language models with diverse task categories, including retrieval, multi-hop tracing, aggregation, and question answering. (File: 6. ruler.pdf) User\rOutput a JSON array where each element sequentially contains the paper title and a one-sentence summary\rQwen2.5-Turbo\r[ { \"title\": \"Dual Chunk Attention\", \"summary\": \"DCA introduces a novel training-free framework to enable large language models to handle long-context sequences bydecomposing attention computation into smaller chunks.\" }, { \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\", \"summary\": \"MInference uses a dynamic sparse attention mechanism with a context memory module to significantlyreduce the latency of long-context LLM inference.\" }, { \"title\": \"InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory\", \"summary\": \"InfLLM leverages an efficient context memory to enable large language models to process extremelylong sequences without additional training.\" }, { \"title\": \"SnapKV: LLM Knows What You are Looking for Before Generation\", \"summary\": \"SnapKV is a fine-tuning-free approach that efficiently compresses the KV cache to minimize memory usage while maintaining model performance.\" }, { \"title\": \"LV-EVAL: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K\", \"summary\": \"LV-EVAL is a benchmark designed to evaluate large language models' long-context capabilities across multiple lengths and tasks, focusing on mitigating knowledge leakage.\" }, { \"title\": \"NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?\", \"summary\": \"NeedleBench assesses the retrieval and reasoning capabilities of large language models in long-context scenarios, including tasks like needle-in-a-haystack.\" }, { \"title\": \"RULER: What's the Real Context Size of Your Long-Context Language Models?\", \"summary\": \"RULER is a synthetic benchmark that evaluates long-context language models through tasks like retrieval, multi-hop tracing, aggregation, and question answering.\" } ] How to Use the API The latest Qwen2.5-Turbo supporting 1M tokens is used in the same way as the standard Qwen API and is compatible with the OpenAI API. Below is a simple Python call example (Note: You need to set the environment variable YOUR_API_KEY to your API Key. For more details, please visit Quick Start of Alibaba Cloud Model Studio [Chinese]):\nimport os from openai import OpenAI # Input a long text file with open(\"example.txt\", \"r\", encoding=\"utf-8\") as f: text = f.read() user_input = text + \"\\n\\nSummarize the above text.\" client = OpenAI( api_key=os.getenv(\"YOUR_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", ) completion = client.chat.completions.create( model=\"qwen-turbo-latest\", messages=[ {'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': user_input}, ], ) print(completion.choices[0].message) Model Performance In this section, we evaluate the performance of Qwen2.5-Turbo through various benchmark tests and improvements in inference speed.\nPasskey Retrieval We first conducted experiments on the 1M-token Passkey Retrieval task. The results show that Qwen2.5-Turbo can perfectly capture all hidden numbers in the 1M tokens of irrelevant text, demonstrating the model’s ability to capture detailed information in ultra-long contexts.\nMore Complex Long Text Tasks We select several datasets of long text understanding to test the model, including:\nRULER: An extended benchmark based on Needle in a Haystack, tasks include finding multiple “needles” in irrelevant contexts, answering multiple questions, or finding the most or least frequent words in the context. The maximum context length is 128K. LV-Eval: A benchmark test requiring simultaneous understanding of numerous evidence fragments. We adjust the evaluation metrics in the original version of LV-Eval to avoid false negatives caused by overly strict matching rules. The maximum context length is 256K. LongbenchChat: A dataset evaluating human preference alignment in the tasks of long context. The maximum context length is 100K. The results show that Qwen2.5-Turbo has advantages in various tasks of long context:\nIn the RULER benchmark test, Qwen2.5-Turbo scores 93.1, surpassing GPT-4o-mini and even GPT-4, proving its excellent ability to handle long text tasks. In more tasks of long context understanding like LV-Eval and LongBench-Chat, Qwen2.5-Turbo surpasses GPT-4o-mini in most dimensions and can process tasks with a context of over 128K tokens. Short Text Tasks In addition to performance improvements in tasks of long context, we are also concerned about the model’s performance in tasks of short context. The existing context length extension methods often lead to significant performance degradation when processing short texts. Therefore, we have paid special attention to this issue when building Qwen2.5-Turbo, ensuring that the extension of context length almost does not affect the ability of short text understanding.\nResults on short text benchmarks show that Qwen2.5-Turbo significantly surpasses previous open-source models with a context length of 1M tokens in most tasks; compared to GPT-4o-mini and Qwen2.5-14B-Instruct models, Qwen2.5-Turbo achieves similar performance in short text tasks while supporting 8 times the context length.\nInference Speed We tested the TTFT (time to first token) for inputs of different lengths. On the sequences of 1M tokens, we used sparse attention to compress the computation of the attention by about 12.5 times, achieving a speedup of 3.2 to 4.3 times under different hardware configurations.\nWhat’s Next? While we are pleased to finally extend the context of Qwen2.5-Turbo to 1M tokens, we also recognize that the current model does not always perform satisfactorily when solving long sequence tasks in real applications. There are many unresolved challenges, such as the model’s performance being more unstable in long sequence tasks, and the cost of inference making it difficult to use larger models. However, we will actively explore further alignment of human preferences in long sequences, optimize inference efficiency to reduce computation time, and attempt to launch larger and stronger long-context models. We look forward to sharing new progress in developing long-context models with you soon, so stay tuned!\n","wordCount":"2314","inLanguage":"en","datePublished":"2024-11-15T00:00:03+08:00","dateModified":"2024-11-15T00:00:03+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2.5-turbo/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Extending the Context Length to 1M Tokens!</h1><div class=post-meta><span title='2024-11-15 00:00:03 +0800 +0800'>November 15, 2024</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2314 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2.5-turbo/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen class="btn external" target=_blank>API Documentation (Chinese)</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>HuggingFace Demo</a>
<a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>ModelScope Demo</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>After the release of Qwen2.5, we heard the community&rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:</p><ol><li><strong>Longer Context Support</strong>: We have extended the model&rsquo;s context length from 128k to <strong>1M</strong>, which is approximately <strong>1 million English words or 1.5 million Chinese characters</strong>, equivalent to 10 full-length novels, 150 hours of speech transcripts, or 30,000 lines of code. The model achieves <strong>100%</strong> accuracy in the 1M length Passkey Retrieval task and scores <strong>93.1</strong> on the long text evaluation benchmark RULER, surpassing GPT-4&rsquo;s 91.6 and GLM4-9B-1M&rsquo;s 89.9. Additionally, the model maintains very strong competitiveness in short sequence capabilities, on par with GPT-4o-mini.</li><li><strong>Faster Inference Speed</strong>: Using sparse attention mechanisms, we successfully reduced the time to first token for processing a context of 1M tokens from 4.9 minutes to 68 seconds, <strong>achieving a 4.3x speedup</strong>.</li><li><strong>Lower Cost</strong>: The price remains ¥0.3 / 1M tokens. At the same cost, Qwen2.5-Turbo can process <strong>3.6 times the number of tokens</strong> as GPT-4o-mini.</li></ol><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/cover_en.png#center width=100%></figure><p>Now, you can use it through the API service of <a href=https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio>Alibaba Cloud Model Studio [Chinese]</a>, or through <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo>HuggingFace Demo</a> or <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo>ModelScope Demo</a>.</p><h1 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h1><p>We have prepared some demos to showcase the new uses that Qwen2.5-Turbo can bring, covering deep understanding of long novels, repository-level code assistant, and reading multiple papers.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Understanding Long Novels</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case1_en.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload the three Chinese novels of <code>The Three-Body Problem</code>, which contain 690k tokens.]</p><p>Provide a summary of the plots of these three novels in English</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The three novels you mentioned from the &ldquo;Earth&rsquo;s Past&rdquo; series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.</p><p><strong>First Novel: &ldquo;Earth&rsquo;s Past&rdquo;</strong>
In the first novel, the narrative focuses on the development of a top-secret project called &ldquo;Red Coast&rdquo; (红岸) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.</p><p><strong>Second Novel: &ldquo;Dark Forest&rdquo;</strong>
The second novel, titled &ldquo;Dark Forest,&rdquo; delves into the concept of the &ldquo;Dark Forest Theory.&rdquo; According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as &ldquo;water drops&rdquo; (水滴). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the &ldquo;Wall-Bearing Plan&rdquo; (面壁计划), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.</p><p><strong>Third Novel: &ldquo;Death&rsquo;s End&rdquo;</strong>
In &ldquo;Death&rsquo;s End,&rdquo; Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (执剑人) brings him immense personal struggle and psychological turmoil. As Luo Ji&rsquo;s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (黑域) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth.
The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Repository</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case2.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload a text file containing all code files from the <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> repository, which has a length of 133k tokens.]</p><p>Please read the codes carefully and ensure your response is comprehensive. Introduce this repository</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Qwen-Agent is a framework designed for developing LLM (Large Language Model) applications. It is based on the instruction-following, tool usage, planning, and memory capabilities of Qwen. Here are some key points and functionalities:</p><ol><li><strong>Agent Class</strong>: Provides a high-level interface for integrating tool calls and LLM interactions. It supports role-playing, automatic planning, and tool invocation.</li><li><strong>DocQA Agent</strong>: Specialized for document question answering, capable of handling long documents and providing context-aware responses.</li><li><strong>Function Calling</strong>: Supports function calling, enabling interaction with tools or APIs directly from the model&rsquo;s responses.</li><li><strong>RAG (Retrieval-Augmented Generation)</strong>: Integrates document retrieval with generation, enhancing the model&rsquo;s ability to recall and use information from documents.</li><li><strong>Tool Integration</strong>: Includes various built-in and custom tools, such as code interpreters, web extractors, and image generators, which can be used to perform tasks like code execution, web scraping, and image manipulation.</li><li><strong>Multi-Agent Systems</strong>: Supports the creation of multi-agent systems where multiple agents can interact, simulate conversations, and collaborate on tasks.</li><li><strong>Customization</strong>: Highly customizable, allowing developers to tailor the framework to specific needs, such as creating custom agents, defining custom workflows, and integrating additional tools.</li><li><strong>Examples and Demos</strong>: Comes with several examples and demos, such as a Chrome browser extension called BrowserQwen, which demonstrates the practical application of Qwen-Agent in a real-world scenario.</li><li><strong>Testing and Debugging</strong>: Includes testing and debugging utilities, such as test cases and logging mechanisms, to ensure the reliability and effectiveness of the developed applications.</li><li><strong>Dependencies</strong>: Requires certain dependencies to be installed for specific functionalities, such as <code>qwen-agent[gui]</code> for GUI support and <code>qwen-agent[rag]</code> for RAG support.
This repository is valuable for researchers, developers, and anyone interested in leveraging large language models for complex tasks and applications.</li></ol></div><div class=role>User</div><div class=content>What are the subclasses of Agent in this repository, and provide their file paths</div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Here are the subclasses of the <code>Agent</code> class in the Qwen-Agent repository along with their file paths:</p><ol><li><strong>Assistant</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/assistant.py</code></li></ul><ol start=2><li><strong>ArticleAgent</strong><ul><li>File Path: <code>qwen_agent/agents/article_agent.py</code></li></ul></li><li><strong>BasicDocQA</strong><ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>DialogueRetrievalAgent</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_retrieval_agent.py</code></li></ul></li><li><strong>DialogueSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_simulator.py</code></li></ul></li><li><strong>DocQAAgent</strong> (Note: The actual implementation of DocQAAgent may vary with each release)<ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>GroupChat</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat.py</code></li></ul></li><li><strong>GroupChatAutoRouter</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_auto_router.py</code></li></ul></li><li><strong>GroupChatCreator</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_creator.py</code></li></ul></li><li><strong>HumanSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/human_simulator.py</code></li></ul></li><li><strong>ReActChat</strong><ul><li>File Path: <code>qwen_agent/agents/react_chat.py</code></li></ul></li><li><strong>Router</strong><ul><li>File Path: <code>qwen_agent/agents/router.py</code></li></ul></li><li><strong>TIRMathAgent</strong><ul><li>File Path: <code>qwen_agent/agents/tir_agent.py</code></li></ul></li><li><strong>UserAgent</strong><ul><li>File Path: <code>qwen_agent/agents/user_agent.py</code></li></ul></li><li><strong>VisualStorytelling</strong><ul><li>File Path: <code>qwen_agent/agents/visual_storytelling.py</code></li></ul></li><li><strong>MemoAssistant</strong><ul><li>File Path: <code>qwen_agent/agents/memo_assistant.py</code></li></ul></li><li><strong>WriteFromScratch</strong><ul><li>File Path: <code>qwen_agent/agents/writing/write_from_scratch.py</code></li></ul></li><li><strong>ExpandWriting</strong><ul><li>File Path: <code>qwen_agent/agents/writing/expand_writing.py</code></li></ul></li><li><strong>OutlineWriting</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/writing/outline_writing.py</code></li></ul><p>These subclasses extend the base <code>Agent</code> class to provide specialized functionalities tailored for different use cases within the Qwen-Agent framework.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Multiple Papers</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case3.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload 7 papers about long-context LLM, which have a length of 171k tokens. Sources: <a href=https://arxiv.org/abs/2402.17463>1</a> <a href=https://arxiv.org/abs/2407.02490>2</a> <a href=https://arxiv.org/abs/2402.05136>3</a> <a href=https://arxiv.org/abs/2402.04617>4</a> <a href=https://arxiv.org/abs/2407.11963>5</a> <a href=https://arxiv.org/abs/2404.06654>6</a> <a href=https://arxiv.org/abs/2404.14469>7</a>]</p><p>classify the above papers into two categories</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The papers provided can be classified into two categories based on their focus:</p><p><strong>Category 1: Long-Context Scaling Techniques</strong></p><ul><li><strong>DCA (Dual Chunk Attention)</strong>: This paper introduces a new training-free framework to allow large language models to handle long-context sequences by breaking down the attention computation into smaller chunks. (File: 1. DCA.pdf)</li><li><strong>MInference</strong>: This paper proposes a method to accelerate pre-filling for long-context language models by using a dynamic sparse attention mechanism with a context memory module. (File: 2. minference.pdf)</li><li><strong>InfLLM</strong>: This paper presents a training-free memory-based approach to enable large language models to understand extremely long sequences by incorporating an efficient context memory mechanism. (File: 4. infLLM.pdf)</li><li><strong>SnapKV</strong>: This paper introduces a fine-tuning-free approach to efficiently minimize the key-value (KV) cache size of large language models during generation while maintaining comparable performance. (File: 7. SnapKV.pdf)</li></ul><p><strong>Category 2: Benchmarking Long-Context Capabilities</strong></p><ul><li><strong>LV-Eval</strong>: This paper introduces a challenging long-context benchmark designed to evaluate the capabilities of large language models across various lengths and tasks, with a focus on mitigating knowledge leakage and providing a balanced evaluation. (File: 3. lveval.pdf)</li><li><strong>NeedleBench</strong>: This paper presents a benchmark for evaluating the retrieval and reasoning capabilities of large language models in long-context scenarios, including a needle-in-a-haystack task. (File: 5. needlebench.pdf)</li><li><strong>RULER</strong>: This paper proposes a synthetic benchmark for evaluating long-context language models with diverse task categories, including retrieval, multi-hop tracing, aggregation, and question answering. (File: 6. ruler.pdf)</li></ul></div><div class=role>User</div><div class=content>Output a JSON array where each element sequentially contains the paper title and a one-sentence summary</div><div class=role>Qwen2.5-Turbo</div><div class=content><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Dual Chunk Attention&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;DCA introduces a novel training-free framework to enable large language models to handle 
</span></span></span><span class=line><span class=cl><span class=s2>long-context sequences bydecomposing attention computation into smaller chunks.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;MInference uses a dynamic sparse attention mechanism with a context memory module to
</span></span></span><span class=line><span class=cl><span class=s2>significantlyreduce the latency of long-context LLM inference.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;InfLLM leverages an efficient context memory to enable large language models to process
</span></span></span><span class=line><span class=cl><span class=s2>extremelylong sequences without additional training.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;SnapKV: LLM Knows What You are Looking for Before Generation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;SnapKV is a fine-tuning-free approach that efficiently compresses the KV cache to minimize
</span></span></span><span class=line><span class=cl><span class=s2>memory usage while maintaining model performance.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;LV-EVAL: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;LV-EVAL is a benchmark designed to evaluate large language models&#39; long-context capabilities
</span></span></span><span class=line><span class=cl><span class=s2>across multiple lengths and tasks, focusing on mitigating knowledge leakage.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;NeedleBench assesses the retrieval and reasoning capabilities of large language models in
</span></span></span><span class=line><span class=cl><span class=s2>long-context scenarios, including tasks like needle-in-a-haystack.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;RULER: What&#39;s the Real Context Size of Your Long-Context Language Models?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;summary&#34;</span><span class=p>:</span> <span class=s2>&#34;RULER is a synthetic benchmark that evaluates long-context language models through tasks
</span></span></span><span class=line><span class=cl><span class=s2>like retrieval, multi-hop tracing, aggregation, and question answering.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></div></div></div></div></div><h1 id=how-to-use-the-api>How to Use the API<a hidden class=anchor aria-hidden=true href=#how-to-use-the-api>#</a></h1><p>The latest Qwen2.5-Turbo supporting 1M tokens is used in the same way as the standard Qwen API and is compatible with the OpenAI API. Below is a simple Python call example (Note: You need to set the environment variable <code>YOUR_API_KEY</code> to your API Key. For more details, please visit <a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen>Quick Start of Alibaba Cloud Model Studio [Chinese]</a>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input a long text file</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;example.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>user_input</span> <span class=o>=</span> <span class=n>text</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>Summarize the above text.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;YOUR_API_KEY&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-turbo-latest&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;system&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;You are a helpful assistant.&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=n>user_input</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=model-performance>Model Performance<a hidden class=anchor aria-hidden=true href=#model-performance>#</a></h1><p>In this section, we evaluate the performance of Qwen2.5-Turbo through various benchmark tests and improvements in inference speed.</p><h2 id=passkey-retrieval>Passkey Retrieval<a hidden class=anchor aria-hidden=true href=#passkey-retrieval>#</a></h2><p>We first conducted experiments on the 1M-token Passkey Retrieval task. The results show that Qwen2.5-Turbo can perfectly capture all hidden numbers in the 1M tokens of irrelevant text, demonstrating the model&rsquo;s ability to capture detailed information in ultra-long contexts.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/passkey_retrieval.png#center width=100%></figure><h2 id=more-complex-long-text-tasks>More Complex Long Text Tasks<a hidden class=anchor aria-hidden=true href=#more-complex-long-text-tasks>#</a></h2><p>We select several datasets of long text understanding to test the model, including:</p><ul><li><a href=https://github.com/hsiehjackson/RULER>RULER</a>: An extended benchmark based on Needle in a Haystack, tasks include finding multiple &ldquo;needles&rdquo; in irrelevant contexts, answering multiple questions, or finding the most or least frequent words in the context. The maximum context length is 128K.</li><li><a href=https://github.com/infinigence/LVEval>LV-Eval</a>: A benchmark test requiring simultaneous understanding of numerous evidence fragments. We adjust the evaluation metrics in the original version of LV-Eval to avoid false negatives caused by overly strict matching rules. The maximum context length is 256K.</li><li><a href=https://github.com/THUDM/LongAlign>LongbenchChat</a>: A dataset evaluating human preference alignment in the tasks of long context. The maximum context length is 100K.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/long_result.png#center width=100%></figure><p>The results show that Qwen2.5-Turbo has advantages in various tasks of long context:</p><ul><li>In the RULER benchmark test, Qwen2.5-Turbo scores 93.1, surpassing GPT-4o-mini and even GPT-4, proving its excellent ability to handle long text tasks.</li><li>In more tasks of long context understanding like LV-Eval and LongBench-Chat, Qwen2.5-Turbo surpasses GPT-4o-mini in most dimensions and can process tasks with a context of over 128K tokens.</li></ul><h2 id=short-text-tasks>Short Text Tasks<a hidden class=anchor aria-hidden=true href=#short-text-tasks>#</a></h2><p>In addition to performance improvements in tasks of long context, we are also concerned about the model&rsquo;s performance in tasks of short context. The existing context length extension methods often lead to significant performance degradation when processing short texts. Therefore, we have paid special attention to this issue when building Qwen2.5-Turbo, ensuring that the extension of context length almost does not affect the ability of short text understanding.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/short_result.png#center width=100%></figure><p>Results on short text benchmarks show that Qwen2.5-Turbo significantly surpasses previous open-source models with a context length of 1M tokens in most tasks; compared to GPT-4o-mini and Qwen2.5-14B-Instruct models, Qwen2.5-Turbo achieves similar performance in short text tasks while supporting 8 times the context length.</p><h2 id=inference-speed>Inference Speed<a hidden class=anchor aria-hidden=true href=#inference-speed>#</a></h2><p>We tested the TTFT (time to first token) for inputs of different lengths. On the sequences of 1M tokens, we used sparse attention to compress the computation of the attention by about 12.5 times, achieving a speedup of 3.2 to 4.3 times under different hardware configurations.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/inference_speed.png#center width=70%></figure><h1 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>While we are pleased to finally extend the context of Qwen2.5-Turbo to 1M tokens, we also recognize that the current model does not always perform satisfactorily when solving long sequence tasks in real applications. There are many unresolved challenges, such as the model&rsquo;s performance being more unstable in long sequence tasks, and the cost of inference making it difficult to use larger models. However, we will actively explore further alignment of human preferences in long sequences, optimize inference efficiency to reduce computation time, and attempt to launch larger and stronger long-context models. We look forward to sharing new progress in developing long-context models with you soon, so stay tuned!</p></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>