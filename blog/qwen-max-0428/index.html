<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Qwen-Max-0428 | Qwen</title><meta name=keywords content><meta name=description content="API DEMO DISCORD
Previously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.
Models MT-Bench Arena Qwen1."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen-max-0428/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen-max-0428/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen-max-0428/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Notes on Qwen-Max-0428"><meta property="og:description" content="API DEMO DISCORD
Previously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.
Models MT-Bench Arena Qwen1."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen-max-0428/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-05-11T18:10:00+08:00"><meta property="article:modified_time" content="2024-05-11T18:10:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Notes on Qwen-Max-0428"><meta name=twitter:description content="API DEMO DISCORD
Previously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.
Models MT-Bench Arena Qwen1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Notes on Qwen-Max-0428","item":"https://qwenlm.github.io/blog/qwen-max-0428/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Qwen-Max-0428","name":"Notes on Qwen-Max-0428","description":"API DEMO DISCORD\nPreviously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.\nModels MT-Bench Arena Qwen1.","keywords":[],"articleBody":"API DEMO DISCORD\nPreviously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.\nModels MT-Bench Arena Qwen1.5-110B-Chat 8.88 1172 Qwen-Max-0428 8.96 1186 We provide a demo of chat service of Qwen-Max-0428 (link) in Hugging Face Spaces:\nIt is also accessible through the official DashScope API (link). Additionally, now the DashScope API is compatible with the OpenAI API format. Below is an example of usage:\nfrom openai import OpenAI client = OpenAI( api_key=\"$your-dashscope-api-key\", base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\" ) completion = client.chat.completions.create( model=\"qwen-max\", messages=[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Tell me something about large language models.'}] ) print(completion.choices[0].message) This model is available in our web service and APP (link, only accessible in mainland China). Enjoy!\nCitation @misc{qwen1.5, title = {Introducing Qwen1.5}, url = {https://qwenlm.github.io/blog/qwen1.5/}, author = {Qwen Team}, month = {February}, year = {2024} } ","wordCount":"190","inLanguage":"en","datePublished":"2024-05-11T18:10:00+08:00","dateModified":"2024-05-11T18:10:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen-max-0428/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Notes on Qwen-Max-0428</h1><div class=post-meta><span title='2024-05-11 18:10:00 +0800 +0800'>May 11, 2024</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;190 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen-max-0428/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://dashscope.aliyun.com class="btn external" target=_blank>API</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen-Max-0428 class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>Previously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via <a href=https://chat.lmsys.org/>Chatbot Arena</a> and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/arena_leaderboard.jpg#center width=100%></figure><table><tr><th rowspan=1 align=center>Models</th><th colspan=1 align=center>MT-Bench</th><th colspan=1 align=center>Arena</th></tr><tr><td>Qwen1.5-110B-Chat</td><td align=center>8.88</td><td align=center>1172</td></tr><tr><td>Qwen-Max-0428</td><td align=center>8.96</td><td align=center>1186</td></tr></table><p>We provide a demo of chat service of Qwen-Max-0428 (<a href=https://huggingface.co/spaces/Qwen/Qwen-Max-0428>link</a>) in Hugging Face Spaces:</p><iframe src=https://qwen-qwen-max-0428.hf.space frameborder=0 width=850 height=1000></iframe><p>It is also accessible through the official DashScope API (<a href=https://dashscope.aliyun.com>link</a>). Additionally, now the DashScope API is compatible with the OpenAI API format. Below is an example of usage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;$your-dashscope-api-key&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;qwen-max&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;system&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;You are a helpful assistant.&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>              <span class=p>{</span><span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;Tell me something about large language models.&#39;</span><span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=p>)</span>
</span></span></code></pre></div><p>This model is available in our web service and APP (<a href=https://tongyi.aliyun.com/qianwen/>link</a>, only accessible in mainland China). Enjoy!</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><pre tabindex=0><code>@misc{qwen1.5,
    title = {Introducing Qwen1.5},
    url = {https://qwenlm.github.io/blog/qwen1.5/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>