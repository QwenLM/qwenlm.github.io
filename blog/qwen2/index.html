<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Hello Qwen2 | Qwen</title><meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/qwen2/><link crossorigin=anonymous href=/assets/css/stylesheet.012512d6f1d6f320d85cff7ae2b89d136cc19960a4aa00adf35aaae57e557162.css integrity="sha256-ASUS1vHW8yDYXP964ridE2zBmWCkqgCt81qq5X5VcWI=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/qwen2/><link rel=alternate hreflang=zh href=https://qwenlm.github.io/zh/blog/qwen2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Hello Qwen2"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/qwen2/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-06-07T00:00:00+08:00"><meta property="article:modified_time" content="2024-07-16T00:00:00+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Hello Qwen2"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
Introduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Hello Qwen2","item":"https://qwenlm.github.io/blog/qwen2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Hello Qwen2","name":"Hello Qwen2","description":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:\nPretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.","keywords":[],"articleBody":" GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:\nPretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct. We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!\nModel Information The Qwen2 series include base and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B. We illustrate the key information of the models in the following table:\nModels Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B # Params 0.49B 1.54B 7.07B 57.41B 72.71B # Non-Emb Params 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False Context Length 32K 32K 128K 64K 128K Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.\nIn terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation. However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks. In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the Needle in a Haystack task. Notably, when augmented with YARN, both Qwen2-7B-Instruct and Qwen2-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens.\nSignificant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:\nRegions Languages Western Europe German, French, Spanish, Portuguese, Italian, Dutch Eastern \u0026 Central Europe Russian, Czech, Polish Middle East Arabic, Persian, Hebrew, Turkish Eastern Asia Japanese, Korean South-Eastern Asia Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog Southern Asia Hindi, Bengali, Urdu Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our modelsâ€™ proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.\nPerformance Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model Qwen2-72B. In terms of base language models, Qwen2-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities. Benefiting from meticulously curated datasets and optimized training methods, Qwen2-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.\nAfter extensive large-scale pre-training, we conduct post-training to further enhance Qwenâ€™s intelligence, bringing it closer to human. This process further improves the modelâ€™s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the modelâ€™s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as rejection sampling for math, execution feedback for coding and instruction-following, back-translation for creative writing, scalable oversight for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel Online Merging Optimizer to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.\nWe comprehensively evaluate Qwen2-72B-Instruct on 16 benchmarks across various domains. Qwen2-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, Qwen2-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.1\nIn terms of smaller models, our Qwen2 models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, Qwen2-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.1\nHighlights Coding \u0026 Mathematics We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from CodeQwen1.5, resulting in significant improvements in Qwen2-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, Qwen2-72B-Instruct has reflects stronger capabilities in solving mathematic problems.\nLong Context Understanding In Qwen2, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like YARN or Dual Chunk Attention.\nThe figure below shows our test results on the Needle in a Haystack. Notably, Qwen2-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.\nAdditionally, itâ€™s worth noting the impressive capabilities of other models in the series: Qwen2-7B-Instruct nearly flawlessly handles contexts up to 128k in length, Qwen2-57B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.\nAlongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see our dedicated blog post on this topic.\nSafety and Responsibility The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from Jailbreak and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the Qwen2-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.\nLanguage Illegal Activity Fraud Pornography Privacy Violence GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct zh 0% 13% 0% 0% 17% 0% 43% 47% 53% 0% 10% 0% en 0% 7% 0% 0% 23% 0% 37% 67% 63% 0% 27% 3% ar 0% 13% 0% 0% 7% 0% 15% 26% 15% 3% 13% 0% es 0% 7% 0% 3% 0% 0% 48% 64% 50% 3% 7% 3% fr 0% 3% 0% 3% 3% 7% 3% 19% 7% 0% 27% 0% ko 0% 4% 0% 3% 8% 4% 17% 29% 10% 0% 26% 4% pt 0% 7% 0% 3% 7% 3% 47% 57% 47% 4% 26% 4% th 0% 10% 0% 7% 23% 3% 13% 17% 10% 13% 7% 7% vi 0% 4% 0% 4% 11% 0% 22% 26% 22% 0% 0% 0% Average 0% 8% 0% 3% 11% 2% 27% 39% 31% 3% 16% 2% Developing with Qwen2 Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.\nFor a long time, a lot of friends have been supporting the development of Qwen, including finetuning (Axolotl, Llama-Factory, Firefly, Swift, XTuner), quantization (AutoGPTQ, AutoAWQ, Neural Compressor), deployment (vLLM, SGL, SkyPilot, TensorRT-LLM, OpenVino, TGI), API platforms (Together, Fireworks, OpenRouter), local run (MLX, Llama.cpp, Ollama, LM Studio), Agent and RAG Frameworks (LlamaIndex, CrewAI, OpenDevin) , Evaluation (LMSys, OpenCompass, Open LLM Leaderboard), model training (Dolphin, Openbuddy) etc. For how to use Qwen2 with the third-party frameworks, please refer to the respective documentation as well as our official documentation.\nStill there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.\nLicense This time, we change the licenses of our models to different ones. While Qwen2-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, and Qwen2-57B-A14B, turn to adopt Apache 2.0! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of Qwen2 all around the world.\nWhatâ€™s Next for Qwen2? We are training larger Qwen2 models to further explore model scaling along with our recent data scaling. Additionally, we extend the Qwen2 language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!\nCitation If you find our work helpful, feel free to give us a cite!\n@article{qwen2, title={Qwen2 Technical Report}, author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } Appendix Base Language Model Evaluation The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.\nThe datasets for evaluation include:\nEnglish Tasks: MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)\nCoding Tasks: EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)\nMath Tasks: GSM8K (4-shot), MATH (4-shot)\nChinese Tasks: C-Eval(5-shot), CMMLU (5-shot)\nMultilingual Tasks: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)\nQwen2-72B performance Datasets DeepSeek-V2 Mixtral-8x22B Llama-3-70B Qwen1.5-72B Qwen1.5-110B Qwen2-72B Architecture MoE MoE Dense Dense Dense Dense #Activated Params 21B 39B 70B 72B 110B 72B #Params 236B 140B 70B 72B 110B 72B English MMLU 78.5 77.8 79.5 77.5 80.4 84.2 MMLU-Pro - 49.5 52.8 45.8 49.4 55.6 GPQA - 34.3 36.3 36.3 35.9 37.9 Theorem QA - 35.9 32.3 29.3 34.9 43.1 BBH 78.9 78.9 81.0 65.5 74.8 82.4 HellaSwag 87.8 88.7 88.0 86.0 87.5 87.6 WindoGrande 84.8 85.0 85.3 83.0 83.5 85.1 ARC-C 70.0 70.7 68.8 65.9 69.6 68.9 TruthfulQA 42.2 51.0 45.6 59.6 49.6 54.8 Coding HumanEval 45.7 46.3 48.2 46.3 54.3 64.6 MBPP 73.9 71.7 70.4 66.9 70.9 76.9 EvalPlus 55.0 54.1 54.8 52.9 57.7 65.4 MultiPL-E 44.4 46.7 46.3 41.8 52.7 59.6 Mathematics GSM8K 79.2 83.7 83.0 79.5 85.4 89.5 MATH 43.6 41.7 42.5 34.1 49.6 51.1 Chinese C-Eval 81.7 54.6 65.2 84.1 89.1 91.0 CMMLU 84.0 53.4 67.2 83.5 88.3 90.1 Multilingual Mulit-Exam 67.5 63.5 70.0 66.4 75.6 76.6 Multi-Understanding 77.0 77.7 79.9 78.2 78.2 80.7 Multi-Mathematics 58.8 62.9 67.1 61.7 64.4 76.0 Multi-Translation 36.0 23.3 38.0 35.6 36.2 37.8 Qwen2-57B-A14B Datasets Jamba Mixtral-8x7B Yi-1.5-34B Qwen1.5-32B Qwen2-57B-A14B Architecture MoE MoE Dense Dense MoE #Activated Params 12B 12B 34B 32B 14B #Params 52B 47B 34B 32B 57B English MMLU 67.4 71.8 77.1 74.3 76.5 MMLU-Pro - 41.0 48.3 44.0 43.0 GPQA - 29.2 - 30.8 34.3 Theorem QA - 23.2 - 28.8 33.5 BBH 45.4 50.3 76.4 66.8 67.0 HellaSwag 87.1 86.5 85.9 85.0 85.2 Winogrande 82.5 81.9 84.9 81.5 79.5 ARC-C 64.4 66.0 65.6 63.6 64.1 TruthfulQA 46.4 51.1 53.9 57.4 57.7 Coding HumanEval 29.3 37.2 46.3 43.3 53.0 MBPP - 63.9 65.5 64.2 71.9 EvalPlus - 46.4 51.9 50.4 57.2 MultiPL-E - 39.0 39.5 38.5 49.8 Mathematics GSM8K 59.9 62.5 82.7 76.8 80.7 MATH - 30.8 41.7 36.1 43.0 Chinese C-Eval - - - 83.5 87.7 CMMLU - - 84.8 82.3 88.5 Multilingual Multi-Exam - 56.1 58.3 61.6 65.5 Multi-Understanding - 70.7 73.9 76.5 77.0 Multi-Mathematics - 45.0 49.3 56.1 62.3 Multi-Translation - 29.8 30.0 33.5 34.5 Qwen2-7B Datasets Mistral-7B Gemma-7B Llama-3-8B Qwen1.5-7B Qwen2-7B # Params 7.2B 8.5B 8.0B 7.7B 7.6B # Non-emb Params 7.0B 7.8B 7.0B 6.5B 6.5B English MMLU 64.2 64.6 66.6 61.0 70.3 MMLU-Pro 30.9 33.7 35.4 29.9 40.0 GPQA 24.7 25.7 25.8 26.7 31.8 Theorem QA 19.2 21.5 22.1 14.2 31.1 BBH 56.1 55.1 57.7 40.2 62.6 HellaSwag 83.2 82.2 82.1 78.5 80.7 Winogrande 78.4 79.0 77.4 71.3 77.0 ARC-C 60.0 61.1 59.3 54.2 60.6 TruthfulQA 42.2 44.8 44.0 51.1 54.2 Coding HumanEval 29.3 37.2 33.5 36.0 51.2 MBPP 51.1 50.6 53.9 51.6 65.9 EvalPlus 36.4 39.6 40.3 40.0 54.2 MultiPL-E 29.4 29.7 22.6 28.1 46.3 Mathematics GSM8K 52.2 46.4 56.0 62.5 79.9 MATH 13.1 24.3 20.5 20.3 44.2 Chinese C-Eval 47.4 43.6 49.5 74.1 83.2 CMMLU - - 50.8 73.1 83.9 Multilingual Multi-Exam 47.1 42.7 52.3 47.7 59.2 Multi-Understanding 63.3 58.3 68.6 67.6 72.0 Multi-Mathematics 26.3 39.1 36.3 37.3 57.5 Multi-Translation 23.3 31.2 31.9 28.4 31.5 Qwen2-0.5B \u0026 Qwen2-1.5B Datasets Phi-2 Gemma-2B MiniCPM Qwen1.5-1.8B Qwen2-0.5B Qwen2-1.5B #Non-Emb Params 2.5B 2.0B 2.4B 1.3B 0.35B 1.3B MMLU 52.7 42.3 53.5 46.8 45.4 56.5 MMLU-Pro - 15.9 - - 14.7 21.8 Theorem QA - - - - 8.9 15.0 HumanEval 47.6 22.0 50.0 20.1 22.0 31.1 MBPP 55.0 29.2 47.3 18.0 22.0 37.4 GSM8K 57.2 17.7 53.8 38.4 36.5 58.5 MATH 3.5 11.8 10.2 10.1 10.7 21.7 BBH 43.4 35.2 36.9 24.2 28.4 37.2 HellaSwag 73.1 71.4 68.3 61.4 49.3 66.6 Winogrande 74.4 66.8 - 60.3 56.8 66.2 ARC-C 61.1 48.5 - 37.9 31.5 43.9 TruthfulQA 44.5 33.1 - 39.4 39.7 45.9 C-Eval 23.4 28.0 51.1 59.7 58.2 70.6 CMMLU 24.2 - 51.1 57.8 55.1 70.3 Instruction-tuned Model Evaluation1 Qwen2-72B-Instruct Datasets Llama-3-70B-Instruct Qwen1.5-72B-Chat Qwen2-72B-Instruct English MMLU 82.0 75.6 82.3 MMLU-Pro 56.2 51.7 64.4 GPQA 41.9 39.4 42.4 TheroemQA 42.5 28.8 44.4 MT-Bench 8.95 8.61 9.12 Arena-Hard 41.1 36.1 48.1 IFEval (Prompt Strict-Acc.) 77.3 55.8 77.6 Coding HumanEval 81.7 71.3 86.0 MBPP 82.3 71.9 80.2 MultiPL-E 63.4 48.1 69.2 EvalPlus 75.2 66.9 79.0 LiveCodeBench 29.3 17.9 35.7 Mathematics GSM8K 93.0 82.7 91.1 MATH 50.4 42.5 59.7 Chinese C-Eval 61.6 76.1 83.8 AlignBench 7.42 7.28 8.27 Qwen2-57B-A14B-Instruct Datasets Mixtral-8x7B-Instruct-v0.1 Yi-1.5-34B-Chat Qwen1.5-32B-Chat Qwen2-57B-A14B-Instruct Architecture MoE Dense Dense MoE #Activated Params 12B 34B 32B 14B #Params 47B 34B 32B 57B English MMLU 71.4 76.8 74.8 75.4 MMLU-Pro 43.3 52.3 46.4 52.8 GPQA - - 30.8 34.3 TheroemQA - - 30.9 33.1 MT-Bench 8.30 8.50 8.30 8.55 Coding HumanEval 45.1 75.2 68.3 79.9 MBPP 59.5 74.6 67.9 70.9 MultiPL-E - - 50.7 66.4 EvalPlus 48.5 - 63.6 71.6 LiveCodeBench 12.3 - 15.2 25.5 Mathematics GSM8K 65.7 90.2 83.6 79.6 MATH 30.7 50.1 42.4 49.1 Chinese C-Eval - - 76.7 80.5 AlignBench 5.70 7.20 7.19 7.36 Qwen2-7B-Instruct Datasets Llama-3-8B-Instruct Yi-1.5-9B-Chat GLM-4-9B-Chat Qwen1.5-7B-Chat Qwen2-7B-Instruct English MMLU 68.4 69.5 72.4 59.5 70.5 MMLU-Pro 41.0 - - 29.1 44.1 GPQA 34.2 - - 27.8 25.3 TheroemQA 23.0 - - 14.1 25.3 MT-Bench 8.05 8.20 8.35 7.60 8.41 Coding Humaneval 62.2 66.5 71.8 46.3 79.9 MBPP 67.9 - - 48.9 67.2 MultiPL-E 48.5 - - 27.2 59.1 Evalplus 60.9 - - 44.8 70.3 LiveCodeBench 17.3 - - 6.0 26.6 Mathematics GSM8K 79.6 84.8 79.6 60.3 82.3 MATH 30.0 47.7 50.6 23.2 49.6 Chinese C-Eval 45.9 - 75.6 67.3 77.2 AlignBench 6.20 6.90 7.01 6.20 7.21 Qwen2-0.5B-Instruct \u0026 Qwen2-1.5B-Instruct Datasets Qwen1.5-0.5B-Chat Qwen2-0.5B-Instruct Qwen1.5-1.8B-Chat Qwen2-1.5B-Instruct MMLU 35.0 37.9 43.7 52.4 HumanEval 9.1 17.1 25.0 37.8 GSM8K 11.3 40.1 35.3 61.6 C-Eval 37.2 45.2 55.3 63.8 IFEval (Prompt Strict-Acc.) 14.6 20.0 16.8 29.0 Multilingual capability of instruction-tuned models We compare Qwen2 instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:\nM-MMLU from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh) MGSM: math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn The results are averaged over languages for each benchmark and shown as follows:\nModels M-MMLU (5-shot) MGSM (0-shot, CoT) Proprietary LLMs GPT-4-0613 78.0 87.0 GPT-4-Turbo-0409 79.3 90.5 GPT-4o-0513 83.2 89.6 Claude-3-Opus-20240229 80.1 91.0 Claude-3-Sonnet-20240229 71.0 85.6 Open-source LLMs command-r-plus-110b 65.5 63.5 Qwen1.5-7B-Chat 50.0 37.0 Qwen1.5-32B-Chat 65.0 65.0 Qwen1.5-72B-Chat 68.4 71.7 Qwen2-7B-Instruct 60.0 57.0 Qwen2-57B-A14B-Instruct 68.0 74.0 Qwen2-72B-Instruct 78.0 86.6 For human evaluation, we compare Qwen2-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):\nModels ar es fr ko th vi pt id ja ru Average Claude-3-Opus-20240229 4.15 4.31 4.23 4.23 4.01 3.98 4.09 4.40 3.85 4.25 4.15 GPT-4o-0513 3.55 4.26 4.16 4.40 4.09 4.14 3.89 4.39 3.72 4.32 4.09 GPT-4-Turbo-0409 3.44 4.08 4.19 4.24 4.11 3.84 3.86 4.09 3.68 4.27 3.98 Qwen2-72B-Instruct 3.86 4.10 4.01 4.14 3.75 3.91 3.97 3.83 3.63 4.15 3.93 GPT-4-0613 3.55 3.92 3.94 3.87 3.83 3.95 3.55 3.77 3.06 3.63 3.71 GPT-3.5-Turbo-1106 2.52 4.07 3.47 2.37 3.38 2.90 3.37 3.56 2.75 3.24 3.16 Grouped by task types, the results are shown as follows:\nModels Knowledge Understanding Creation Math Claude-3-Opus-20240229 3.64 4.45 4.42 3.81 GPT-4o-0513 3.76 4.35 4.45 3.53 GPT-4-Turbo-0409 3.42 4.29 4.35 3.58 Qwen2-72B-Instruct 3.41 4.07 4.36 3.61 GPT-4-0613 3.42 4.09 4.10 3.32 GPT-3.5-Turbo-1106 3.37 3.67 3.89 2.97 These results demonstrate the strong multilingual capabilities of Qwen2 instruction-tuned models.\nUpdate on 2024-07-16: The results of instruction-tuned models may differ from those presented in the technical report; in case of any discrepancy, the results documented in the technical report should take precedence.Â â†©ï¸ŽÂ â†©ï¸ŽÂ â†©ï¸Ž\n","wordCount":"3119","inLanguage":"en","datePublished":"2024-06-07T00:00:00+08:00","dateModified":"2024-07-16T00:00:00+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/qwen2/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Hello Qwen2</h1><div class=post-meta><span title='2024-06-07 00:00:00 +0800 +0800'>June 7, 2024</span>&nbsp;Â·&nbsp;15 min&nbsp;Â·&nbsp;3119 words&nbsp;Â·&nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qwenlm.github.io/zh/blog/qwen2/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen.jpg#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2 class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:</p><ul><li>Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and <strong>Qwen2-72B</strong>;</li><li>Having been trained on data in <strong>27</strong> additional languages besides English and Chinese;</li><li>State-of-the-art performance in a large number of benchmark evaluations;</li><li>Significantly improved performance in coding and mathematics;</li><li>Extended context length support up to <strong>128K</strong> tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.<br><br></li></ul><p>We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!</p><h2 id=model-information>Model Information<a hidden class=anchor aria-hidden=true href=#model-information>#</a></h2><p>The Qwen2 series include base and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B. We illustrate the key information of the models in the following table:</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center>Qwen2-1.5B</th><th style=text-align:center>Qwen2-7B</th><th style=text-align:center>Qwen2-57B-A14B</th><th style=text-align:center>Qwen2-72B</th></tr></thead><tbody><tr><td style=text-align:left># Params</td><td style=text-align:center>0.49B</td><td style=text-align:center>1.54B</td><td style=text-align:center>7.07B</td><td style=text-align:center>57.41B</td><td style=text-align:center>72.71B</td></tr><tr><td style=text-align:left># Non-Emb Params</td><td style=text-align:center>0.35B</td><td style=text-align:center>1.31B</td><td style=text-align:center>5.98B</td><td style=text-align:center>56.32B</td><td style=text-align:center>70.21B</td></tr><tr><td style=text-align:left>GQA</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>True</td></tr><tr><td style=text-align:left>Tie Embedding</td><td style=text-align:center>True</td><td style=text-align:center>True</td><td style=text-align:center>False</td><td style=text-align:center>False</td><td style=text-align:center>False</td></tr><tr><td style=text-align:left>Context Length</td><td style=text-align:center>32K</td><td style=text-align:center>32K</td><td style=text-align:center>128K</td><td style=text-align:center>64K</td><td style=text-align:center>128K</td></tr></tbody></table><p>Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.</p><p>In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation.
However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks.
In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the <a href=https://github.com/gkamradt/LLMTest_NeedleInAHaystack>Needle in a Haystack</a> task. Notably, when augmented with YARN, both Qwen2-7B-Instruct and Qwen2-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens.</p><p>Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:</p><table><thead><tr><th style=text-align:left>Regions</th><th style=text-align:center>Languages</th></tr></thead><tbody><tr><td style=text-align:left>Western Europe</td><td style=text-align:center>German, French, Spanish, Portuguese, Italian, Dutch</td></tr><tr><td style=text-align:left>Eastern & Central Europe</td><td style=text-align:center>Russian, Czech, Polish</td></tr><tr><td style=text-align:left>Middle East</td><td style=text-align:center>Arabic, Persian, Hebrew, Turkish</td></tr><tr><td style=text-align:left>Eastern Asia</td><td style=text-align:center>Japanese, Korean</td></tr><tr><td style=text-align:left>South-Eastern Asia</td><td style=text-align:center>Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog</td></tr><tr><td style=text-align:left>Southern Asia</td><td style=text-align:center>Hindi, Bengali, Urdu</td></tr></tbody></table><p>Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models&rsquo; proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.</p><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><p>Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model Qwen2-72B.
In terms of base language models, Qwen2-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities.
Benefiting from meticulously curated datasets and optimized training methods, Qwen2-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b.jpg#center width=100%></figure><p>After extensive large-scale pre-training, we conduct post-training to further enhance Qwen&rsquo;s intelligence, bringing it closer to human. This process further improves the model&rsquo;s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model&rsquo;s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as <a href=https://arxiv.org/pdf/2308.01825>rejection sampling</a> for math, execution feedback for coding and instruction-following, back-translation for creative writing, <a href=https://arxiv.org/pdf/2401.12474>scalable oversight</a> for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel <a href=https://arxiv.org/pdf/2405.17931>Online Merging Optimizer</a> to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b-instruct.jpg#center width=100%></figure><p>We comprehensively evaluate Qwen2-72B-Instruct on 16 benchmarks across various domains. Qwen2-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, Qwen2-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>In terms of smaller models, our Qwen2 models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, Qwen2-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-7b.jpg#center width=100%></figure><h1 id=highlights>Highlights<a hidden class=anchor aria-hidden=true href=#highlights>#</a></h1><h2 id=coding--mathematics>Coding & Mathematics<a hidden class=anchor aria-hidden=true href=#coding--mathematics>#</a></h2><p>We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from <a href=https://qwenlm.github.io/blog/codeqwen1.5/>CodeQwen1.5</a>, resulting in significant improvements in Qwen2-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, Qwen2-72B-Instruct has reflects stronger capabilities in solving mathematic problems.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-code-math.jpg#center width=100%></figure><h2 id=long-context-understanding>Long Context Understanding<a hidden class=anchor aria-hidden=true href=#long-context-understanding>#</a></h2><p>In Qwen2, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like <a href=https://arxiv.org/abs/2309.00071>YARN</a> or <a href=https://arxiv.org/abs/2402.17463>Dual Chunk Attention</a>.</p><p>The figure below shows our test results on the <a href=https://github.com/gkamradt/LLMTest_NeedleInAHaystack>Needle in a Haystack</a>. Notably, Qwen2-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.</p><p>Additionally, it&rsquo;s worth noting the impressive capabilities of other models in the series: Qwen2-7B-Instruct nearly flawlessly handles contexts up to 128k in length, Qwen2-57B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.</p><p>Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see <a href=https://qwenlm.github.io/blog/qwen-agent-2405/>our dedicated blog post on this topic</a>.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2_needle_in_haystack.png#center width=100%></figure><h2 id=safety-and-responsibility>Safety and Responsibility<a hidden class=anchor aria-hidden=true href=#safety-and-responsibility>#</a></h2><p>The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from <a href=https://github.com/verazuo/jailbreak_llms/tree/main>Jailbreak</a> and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the Qwen2-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.</p><table><thead><tr><th>Language</th><th style=text-align:center></th><th style=text-align:center>Illegal Activity</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Fraud</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Pornography</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Privacy Violence</th><th style=text-align:center></th></tr></thead><tbody><tr><td></td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td><td style=text-align:center>GPT-4</td><td style=text-align:center>Mistral-8x22B</td><td style=text-align:center>Qwen2-72B-Instruct</td></tr><tr><td>zh</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>17%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>43%</strong></td><td style=text-align:center>47%</td><td style=text-align:center>53%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>10%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>en</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>23%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>37%</strong></td><td style=text-align:center>67%</td><td style=text-align:center>63%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>27%</td><td style=text-align:center>3%</td></tr><tr><td>ar</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>15%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>15%</strong></td><td style=text-align:center>3%</td><td style=text-align:center>13%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>es</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>48%</strong></td><td style=text-align:center>64%</td><td style=text-align:center>50%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td></tr><tr><td>fr</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>19%</td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>27%</td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>ko</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>8%</td><td style=text-align:center>4%</td><td style=text-align:center>17%</td><td style=text-align:center>29%</td><td style=text-align:center><strong>10%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>26%</td><td style=text-align:center>4%</td></tr><tr><td>pt</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>7%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center><strong>47%</strong></td><td style=text-align:center>57%</td><td style=text-align:center><strong>47%</strong></td><td style=text-align:center><strong>4%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>4%</strong></td></tr><tr><td>th</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>10%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>7%</td><td style=text-align:center>23%</td><td style=text-align:center><strong>3%</strong></td><td style=text-align:center>13%</td><td style=text-align:center>17%</td><td style=text-align:center><strong>10%</strong></td><td style=text-align:center>13%</td><td style=text-align:center><strong>7%</strong></td><td style=text-align:center><strong>7%</strong></td></tr><tr><td>vi</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>4%</td><td style=text-align:center>11%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>22%</strong></td><td style=text-align:center>26%</td><td style=text-align:center><strong>22%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center><strong>0%</strong></td></tr><tr><td>Average</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>8%</td><td style=text-align:center><strong>0%</strong></td><td style=text-align:center>3%</td><td style=text-align:center>11%</td><td style=text-align:center><strong>2%</strong></td><td style=text-align:center><strong>27%</strong></td><td style=text-align:center>39%</td><td style=text-align:center>31%</td><td style=text-align:center>3%</td><td style=text-align:center>16%</td><td style=text-align:center><strong>2%</strong></td></tr></tbody></table><h1 id=developing-with-qwen2>Developing with Qwen2<a hidden class=anchor aria-hidden=true href=#developing-with-qwen2>#</a></h1><p>Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.</p><p>For a long time, a lot of friends have been supporting the development of Qwen, including finetuning (<a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>, <a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>, <a href=https://github.com/yangjianxin1/Firefly>Firefly</a>, <a href=https://github.com/modelscope/swift>Swift</a>, <a href=https://github.com/InternLM/xtuner>XTuner</a>), quantization (<a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>, <a href=https://github.com/intel/neural-compressor>Neural Compressor</a>), deployment (<a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/sgl-project/sglang>SGL</a>, <a href=https://github.com/skypilot-org/skypilot>SkyPilot</a>, <a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>, <a href=https://github.com/openvinotoolkit/openvino>OpenVino</a>, <a href=https://github.com/huggingface/text-generation-inference>TGI</a>), API platforms (<a href=https://www.together.ai/>Together</a>, <a href=https://fireworks.ai/>Fireworks</a>, <a href=https://openrouter.ai/>OpenRouter</a>), local run (<a href=https://github.com/ml-explore/mlx>MLX</a>, <a href=https://github.com/ggerganov/llama.cpp>Llama.cpp</a>, <a href=https://ollama.com/>Ollama</a>, <a href=https://lmstudio.ai/>LM Studio</a>), Agent and RAG Frameworks (<a href=https://www.llamaindex.ai/>LlamaIndex</a>, <a href=https://www.crewai.com/>CrewAI</a>, <a href=https://github.com/OpenDevin/OpenDevin/>OpenDevin</a>) , Evaluation (<a href=https://chat.lmsys.org/>LMSys</a>, <a href=https://opencompass.org.cn/home>OpenCompass</a>, <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a>), model training (<a href=https://huggingface.co/cognitivecomputations>Dolphin</a>, <a href=https://github.com/OpenBuddy/OpenBuddy>Openbuddy</a>) etc. For how to use Qwen2 with the third-party frameworks, please refer to the respective documentation as well as our <a href=https://qwen.readthedocs.io/en/latest/>official documentation</a>.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/logo-v3.jpg#center width=80%></figure><p>Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.</p><h1 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h1><p>This time, we change the licenses of our models to different ones. While Qwen2-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, and Qwen2-57B-A14B, turn to adopt <strong>Apache 2.0</strong>! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of Qwen2 all around the world.</p><h1 id=whats-next-for-qwen2>What&rsquo;s Next for Qwen2?<a hidden class=anchor aria-hidden=true href=#whats-next-for-qwen2>#</a></h1><p>We are training larger Qwen2 models to further explore model scaling along with our recent data scaling. Additionally, we extend the Qwen2 language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><p>If you find our work helpful, feel free to give us a cite!</p><pre tabindex=0><code>@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}
</code></pre><p><br><br></p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><h2 id=base-language-model-evaluation>Base Language Model Evaluation<a hidden class=anchor aria-hidden=true href=#base-language-model-evaluation>#</a></h2><p>The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.</p><p>The datasets for evaluation include:</p><p><strong>English Tasks</strong>: MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)</p><p><strong>Coding Tasks</strong>: EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p><p><strong>Math Tasks</strong>: GSM8K (4-shot), MATH (4-shot)</p><p><strong>Chinese Tasks</strong>: C-Eval(5-shot), CMMLU (5-shot)</p><p><strong>Multilingual Tasks</strong>: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p><h3 id=qwen2-72b-performance>Qwen2-72B performance<a hidden class=anchor aria-hidden=true href=#qwen2-72b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>DeepSeek-V2</th><th style=text-align:center>Mixtral-8x22B</th><th style=text-align:center>Llama-3-70B</th><th style=text-align:center>Qwen1.5-72B</th><th style=text-align:center>Qwen1.5-110B</th><th style=text-align:center><strong>Qwen2-72B</strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>21B</td><td style=text-align:center>39B</td><td style=text-align:center>70B</td><td style=text-align:center>72B</td><td style=text-align:center>110B</td><td style=text-align:center>72B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>236B</td><td style=text-align:center>140B</td><td style=text-align:center>70B</td><td style=text-align:center>72B</td><td style=text-align:center>110B</td><td style=text-align:center>72B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>78.5</td><td style=text-align:center>77.8</td><td style=text-align:center>79.5</td><td style=text-align:center>77.5</td><td style=text-align:center>80.4</td><td style=text-align:center><strong>84.2</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>49.5</td><td style=text-align:center>52.8</td><td style=text-align:center>45.8</td><td style=text-align:center>49.4</td><td style=text-align:center><strong>55.6</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>34.3</td><td style=text-align:center>36.3</td><td style=text-align:center>36.3</td><td style=text-align:center>35.9</td><td style=text-align:center><strong>37.9</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>35.9</td><td style=text-align:center>32.3</td><td style=text-align:center>29.3</td><td style=text-align:center>34.9</td><td style=text-align:center><strong>43.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>78.9</td><td style=text-align:center>78.9</td><td style=text-align:center>81.0</td><td style=text-align:center>65.5</td><td style=text-align:center>74.8</td><td style=text-align:center><strong>82.4</strong></td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center>87.8</td><td style=text-align:center><strong>88.7</strong></td><td style=text-align:center>88.0</td><td style=text-align:center>86.0</td><td style=text-align:center>87.5</td><td style=text-align:center>87.6</td></tr><tr><td style=text-align:left>WindoGrande</td><td style=text-align:center>84.8</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>85.3</strong></td><td style=text-align:center>83.0</td><td style=text-align:center>83.5</td><td style=text-align:center>85.1</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>70.0</td><td style=text-align:center><strong>70.7</strong></td><td style=text-align:center>68.8</td><td style=text-align:center>65.9</td><td style=text-align:center>69.6</td><td style=text-align:center>68.9</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>42.2</td><td style=text-align:center>51.0</td><td style=text-align:center>45.6</td><td style=text-align:center><strong>59.6</strong></td><td style=text-align:center>49.6</td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>45.7</td><td style=text-align:center>46.3</td><td style=text-align:center>48.2</td><td style=text-align:center>46.3</td><td style=text-align:center>54.3</td><td style=text-align:center><strong>64.6</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>73.9</td><td style=text-align:center>71.7</td><td style=text-align:center>70.4</td><td style=text-align:center>66.9</td><td style=text-align:center>70.9</td><td style=text-align:center><strong>76.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>55.0</td><td style=text-align:center>54.1</td><td style=text-align:center>54.8</td><td style=text-align:center>52.9</td><td style=text-align:center>57.7</td><td style=text-align:center><strong>65.4</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>44.4</td><td style=text-align:center>46.7</td><td style=text-align:center>46.3</td><td style=text-align:center>41.8</td><td style=text-align:center>52.7</td><td style=text-align:center><strong>59.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>79.2</td><td style=text-align:center>83.7</td><td style=text-align:center>83.0</td><td style=text-align:center>79.5</td><td style=text-align:center>85.4</td><td style=text-align:center><strong>89.5</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>43.6</td><td style=text-align:center>41.7</td><td style=text-align:center>42.5</td><td style=text-align:center>34.1</td><td style=text-align:center>49.6</td><td style=text-align:center><strong>51.1</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>81.7</td><td style=text-align:center>54.6</td><td style=text-align:center>65.2</td><td style=text-align:center>84.1</td><td style=text-align:center>89.1</td><td style=text-align:center><strong>91.0</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>84.0</td><td style=text-align:center>53.4</td><td style=text-align:center>67.2</td><td style=text-align:center>83.5</td><td style=text-align:center>88.3</td><td style=text-align:center><strong>90.1</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Mulit-Exam</td><td style=text-align:center>67.5</td><td style=text-align:center>63.5</td><td style=text-align:center>70.0</td><td style=text-align:center>66.4</td><td style=text-align:center>75.6</td><td style=text-align:center><strong>76.6</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>77.0</td><td style=text-align:center>77.7</td><td style=text-align:center>79.9</td><td style=text-align:center>78.2</td><td style=text-align:center>78.2</td><td style=text-align:center><strong>80.7</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>58.8</td><td style=text-align:center>62.9</td><td style=text-align:center>67.1</td><td style=text-align:center>61.7</td><td style=text-align:center>64.4</td><td style=text-align:center><strong>76.0</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>36.0</td><td style=text-align:center>23.3</td><td style=text-align:center><strong>38.0</strong></td><td style=text-align:center>35.6</td><td style=text-align:center>36.2</td><td style=text-align:center>37.8</td></tr></tbody></table><h3 id=qwen2-57b-a14b>Qwen2-57B-A14B<a hidden class=anchor aria-hidden=true href=#qwen2-57b-a14b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Jamba</th><th style=text-align:center>Mixtral-8x7B</th><th style=text-align:center>Yi-1.5-34B</th><th style=text-align:center>Qwen1.5-32B</th><th style=text-align:center><strong><strong>Qwen2-57B-A14B</strong></strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>MoE</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>12B</td><td style=text-align:center>12B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>14B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>52B</td><td style=text-align:center>47B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>57B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>67.4</td><td style=text-align:center>71.8</td><td style=text-align:center><strong>77.1</strong></td><td style=text-align:center>74.3</td><td style=text-align:center>76.5</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>41.0</td><td style=text-align:center><strong>48.3</strong></td><td style=text-align:center>44.0</td><td style=text-align:center>43.0</td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>29.2</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>34.3</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>23.2</td><td style=text-align:center>-</td><td style=text-align:center>28.8</td><td style=text-align:center><strong>33.5</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>45.4</td><td style=text-align:center>50.3</td><td style=text-align:center><strong>76.4</strong></td><td style=text-align:center>66.8</td><td style=text-align:center>67.0</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>87.1</strong></td><td style=text-align:center>86.5</td><td style=text-align:center>85.9</td><td style=text-align:center>85.0</td><td style=text-align:center>85.2</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>82.5</td><td style=text-align:center>81.9</td><td style=text-align:center><strong>84.9</strong></td><td style=text-align:center>81.5</td><td style=text-align:center>79.5</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>64.4</td><td style=text-align:center><strong>66.0</strong></td><td style=text-align:center>65.6</td><td style=text-align:center>63.6</td><td style=text-align:center>64.1</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>46.4</td><td style=text-align:center>51.1</td><td style=text-align:center>53.9</td><td style=text-align:center>57.4</td><td style=text-align:center><strong>57.7</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>37.2</td><td style=text-align:center>46.3</td><td style=text-align:center>43.3</td><td style=text-align:center><strong>53.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>-</td><td style=text-align:center>63.9</td><td style=text-align:center>65.5</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>71.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>-</td><td style=text-align:center>46.4</td><td style=text-align:center>51.9</td><td style=text-align:center>50.4</td><td style=text-align:center><strong>57.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>-</td><td style=text-align:center>39.0</td><td style=text-align:center>39.5</td><td style=text-align:center>38.5</td><td style=text-align:center><strong>49.8</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>59.9</td><td style=text-align:center>62.5</td><td style=text-align:center><strong>82.7</strong></td><td style=text-align:center>76.8</td><td style=text-align:center>80.7</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center>41.7</td><td style=text-align:center>36.1</td><td style=text-align:center><strong>43.0</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>83.5</td><td style=text-align:center><strong>87.7</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>84.8</td><td style=text-align:center>82.3</td><td style=text-align:center><strong>88.5</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>-</td><td style=text-align:center>56.1</td><td style=text-align:center>58.3</td><td style=text-align:center>61.6</td><td style=text-align:center><strong>65.5</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>-</td><td style=text-align:center>70.7</td><td style=text-align:center>73.9</td><td style=text-align:center>76.5</td><td style=text-align:center><strong>77.0</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>-</td><td style=text-align:center>45.0</td><td style=text-align:center>49.3</td><td style=text-align:center>56.1</td><td style=text-align:center><strong>62.3</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>-</td><td style=text-align:center>29.8</td><td style=text-align:center>30.0</td><td style=text-align:center>33.5</td><td style=text-align:center><strong>34.5</strong></td></tr></tbody></table><h3 id=qwen2-7b>Qwen2-7B<a hidden class=anchor aria-hidden=true href=#qwen2-7b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mistral-7B</th><th style=text-align:center>Gemma-7B</th><th style=text-align:center>Llama-3-8B</th><th style=text-align:center>Qwen1.5-7B</th><th style=text-align:center>Qwen2-7B</th></tr></thead><tbody><tr><td style=text-align:left># Params</td><td style=text-align:center>7.2B</td><td style=text-align:center>8.5B</td><td style=text-align:center>8.0B</td><td style=text-align:center>7.7B</td><td style=text-align:center>7.6B</td></tr><tr><td style=text-align:left># Non-emb Params</td><td style=text-align:center>7.0B</td><td style=text-align:center>7.8B</td><td style=text-align:center>7.0B</td><td style=text-align:center>6.5B</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>64.2</td><td style=text-align:center>64.6</td><td style=text-align:center>66.6</td><td style=text-align:center>61.0</td><td style=text-align:center><strong>70.3</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>30.9</td><td style=text-align:center>33.7</td><td style=text-align:center>35.4</td><td style=text-align:center>29.9</td><td style=text-align:center><strong>40.0</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>24.7</td><td style=text-align:center>25.7</td><td style=text-align:center>25.8</td><td style=text-align:center>26.7</td><td style=text-align:center><strong>31.8</strong></td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>19.2</td><td style=text-align:center>21.5</td><td style=text-align:center>22.1</td><td style=text-align:center>14.2</td><td style=text-align:center><strong>31.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>56.1</td><td style=text-align:center>55.1</td><td style=text-align:center>57.7</td><td style=text-align:center>40.2</td><td style=text-align:center><strong>62.6</strong></td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>83.2</strong></td><td style=text-align:center>82.2</td><td style=text-align:center>82.1</td><td style=text-align:center>78.5</td><td style=text-align:center>80.7</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>78.4</td><td style=text-align:center><strong>79.0</strong></td><td style=text-align:center>77.4</td><td style=text-align:center>71.3</td><td style=text-align:center>77.0</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>60.0</td><td style=text-align:center><strong>61.1</strong></td><td style=text-align:center>59.3</td><td style=text-align:center>54.2</td><td style=text-align:center>60.6</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>42.2</td><td style=text-align:center>44.8</td><td style=text-align:center>44.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>54.2</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>37.2</td><td style=text-align:center>33.5</td><td style=text-align:center>36.0</td><td style=text-align:center><strong>51.2</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>51.1</td><td style=text-align:center>50.6</td><td style=text-align:center>53.9</td><td style=text-align:center>51.6</td><td style=text-align:center><strong>65.9</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>36.4</td><td style=text-align:center>39.6</td><td style=text-align:center>40.3</td><td style=text-align:center>40.0</td><td style=text-align:center><strong>54.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>29.4</td><td style=text-align:center>29.7</td><td style=text-align:center>22.6</td><td style=text-align:center>28.1</td><td style=text-align:center><strong>46.3</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>52.2</td><td style=text-align:center>46.4</td><td style=text-align:center>56.0</td><td style=text-align:center>62.5</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>13.1</td><td style=text-align:center>24.3</td><td style=text-align:center>20.5</td><td style=text-align:center>20.3</td><td style=text-align:center><strong>44.2</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>47.4</td><td style=text-align:center>43.6</td><td style=text-align:center>49.5</td><td style=text-align:center>74.1</td><td style=text-align:center><strong>83.2</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>50.8</td><td style=text-align:center>73.1</td><td style=text-align:center><strong>83.9</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>47.1</td><td style=text-align:center>42.7</td><td style=text-align:center>52.3</td><td style=text-align:center>47.7</td><td style=text-align:center><strong>59.2</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>63.3</td><td style=text-align:center>58.3</td><td style=text-align:center>68.6</td><td style=text-align:center>67.6</td><td style=text-align:center><strong>72.0</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>26.3</td><td style=text-align:center>39.1</td><td style=text-align:center>36.3</td><td style=text-align:center>37.3</td><td style=text-align:center><strong>57.5</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>23.3</td><td style=text-align:center>31.2</td><td style=text-align:center><strong>31.9</strong></td><td style=text-align:center>28.4</td><td style=text-align:center>31.5</td></tr></tbody></table><h3 id=qwen2-05b--qwen2-15b>Qwen2-0.5B & Qwen2-1.5B<a hidden class=anchor aria-hidden=true href=#qwen2-05b--qwen2-15b>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Phi-2</th><th style=text-align:center>Gemma-2B</th><th style=text-align:center>MiniCPM</th><th style=text-align:center>Qwen1.5-1.8B</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center>Qwen2-1.5B</th></tr></thead><tbody><tr><td style=text-align:left>#Non-Emb Params</td><td style=text-align:center>2.5B</td><td style=text-align:center>2.0B</td><td style=text-align:center>2.4B</td><td style=text-align:center>1.3B</td><td style=text-align:center>0.35B</td><td style=text-align:center>1.3B</td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>52.7</td><td style=text-align:center>42.3</td><td style=text-align:center>53.5</td><td style=text-align:center>46.8</td><td style=text-align:center>45.4</td><td style=text-align:center><strong>56.5</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>-</td><td style=text-align:center>15.9</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>14.7</td><td style=text-align:center>21.8</td></tr><tr><td style=text-align:left>Theorem QA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>8.9</td><td style=text-align:center><strong>15.0</strong></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>47.6</td><td style=text-align:center>22.0</td><td style=text-align:center><strong>50.0</strong></td><td style=text-align:center>20.1</td><td style=text-align:center>22.0</td><td style=text-align:center>31.1</td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>55.0</strong></td><td style=text-align:center>29.2</td><td style=text-align:center>47.3</td><td style=text-align:center>18.0</td><td style=text-align:center>22.0</td><td style=text-align:center>37.4</td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>57.2</td><td style=text-align:center>17.7</td><td style=text-align:center>53.8</td><td style=text-align:center>38.4</td><td style=text-align:center>36.5</td><td style=text-align:center><strong>58.5</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>3.5</td><td style=text-align:center>11.8</td><td style=text-align:center>10.2</td><td style=text-align:center>10.1</td><td style=text-align:center>10.7</td><td style=text-align:center><strong>21.7</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center><strong>43.4</strong></td><td style=text-align:center>35.2</td><td style=text-align:center>36.9</td><td style=text-align:center>24.2</td><td style=text-align:center>28.4</td><td style=text-align:center>37.2</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center><strong>73.1</strong></td><td style=text-align:center>71.4</td><td style=text-align:center>68.3</td><td style=text-align:center>61.4</td><td style=text-align:center>49.3</td><td style=text-align:center>66.6</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center><strong>74.4</strong></td><td style=text-align:center>66.8</td><td style=text-align:center>-</td><td style=text-align:center>60.3</td><td style=text-align:center>56.8</td><td style=text-align:center>66.2</td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center><strong>61.1</strong></td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>37.9</td><td style=text-align:center>31.5</td><td style=text-align:center>43.9</td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>44.5</td><td style=text-align:center>33.1</td><td style=text-align:center>-</td><td style=text-align:center>39.4</td><td style=text-align:center>39.7</td><td style=text-align:center><strong>45.9</strong></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>23.4</td><td style=text-align:center>28.0</td><td style=text-align:center>51.1</td><td style=text-align:center>59.7</td><td style=text-align:center>58.2</td><td style=text-align:center><strong>70.6</strong></td></tr><tr><td style=text-align:left>CMMLU</td><td style=text-align:center>24.2</td><td style=text-align:center>-</td><td style=text-align:center>51.1</td><td style=text-align:center>57.8</td><td style=text-align:center>55.1</td><td style=text-align:center><strong>70.3</strong></td></tr></tbody></table><h2 id=instruction-tuned-model-evaluation1>Instruction-tuned Model Evaluation<sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><a hidden class=anchor aria-hidden=true href=#instruction-tuned-model-evaluation1>#</a></h2><h3 id=qwen2-72b-instruct>Qwen2-72B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-72b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-70B-Instruct</th><th style=text-align:center>Qwen1.5-72B-Chat</th><th style=text-align:center><strong>Qwen2-72B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>82.0</td><td style=text-align:center>75.6</td><td style=text-align:center><strong>82.3</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>56.2</td><td style=text-align:center>51.7</td><td style=text-align:center><strong>64.4</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>41.9</td><td style=text-align:center>39.4</td><td style=text-align:center><strong>42.4</strong></td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>42.5</td><td style=text-align:center>28.8</td><td style=text-align:center><strong>44.4</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.95</td><td style=text-align:center>8.61</td><td style=text-align:center><strong>9.12</strong></td></tr><tr><td style=text-align:left>Arena-Hard</td><td style=text-align:center>41.1</td><td style=text-align:center>36.1</td><td style=text-align:center><strong>48.1</strong></td></tr><tr><td style=text-align:left>IFEval (Prompt Strict-Acc.)</td><td style=text-align:center>77.3</td><td style=text-align:center>55.8</td><td style=text-align:center><strong>77.6</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>81.7</td><td style=text-align:center>71.3</td><td style=text-align:center><strong>86.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>82.3</strong></td><td style=text-align:center>71.9</td><td style=text-align:center>80.2</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>63.4</td><td style=text-align:center>48.1</td><td style=text-align:center><strong>69.2</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>75.2</td><td style=text-align:center>66.9</td><td style=text-align:center><strong>79.0</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>29.3</td><td style=text-align:center>17.9</td><td style=text-align:center><strong>35.7</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center><strong>93.0</strong></td><td style=text-align:center>82.7</td><td style=text-align:center>91.1</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>50.4</td><td style=text-align:center>42.5</td><td style=text-align:center><strong>59.7</strong></td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>61.6</td><td style=text-align:center>76.1</td><td style=text-align:center><strong>83.8</strong></td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>7.42</td><td style=text-align:center>7.28</td><td style=text-align:center><strong>8.27</strong></td></tr></tbody></table><h3 id=qwen2-57b-a14b-instruct>Qwen2-57B-A14B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-57b-a14b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mixtral-8x7B-Instruct-v0.1</th><th style=text-align:center>Yi-1.5-34B-Chat</th><th style=text-align:center>Qwen1.5-32B-Chat</th><th style=text-align:center><strong>Qwen2-57B-A14B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left>Architecture</td><td style=text-align:center>MoE</td><td style=text-align:center>Dense</td><td style=text-align:center>Dense</td><td style=text-align:center>MoE</td></tr><tr><td style=text-align:left>#Activated Params</td><td style=text-align:center>12B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>14B</td></tr><tr><td style=text-align:left>#Params</td><td style=text-align:center>47B</td><td style=text-align:center>34B</td><td style=text-align:center>32B</td><td style=text-align:center>57B</td></tr><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>71.4</td><td style=text-align:center><strong>76.8</strong></td><td style=text-align:center>74.8</td><td style=text-align:center>75.4</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>43.3</td><td style=text-align:center>52.3</td><td style=text-align:center>46.4</td><td style=text-align:center><strong>52.8</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>34.3</strong></td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>30.9</td><td style=text-align:center><strong>33.1</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.30</td><td style=text-align:center>8.50</td><td style=text-align:center>8.30</td><td style=text-align:center><strong>8.55</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>45.1</td><td style=text-align:center>75.2</td><td style=text-align:center>68.3</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>59.5</td><td style=text-align:center><strong>74.6</strong></td><td style=text-align:center>67.9</td><td style=text-align:center>70.9</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>50.7</td><td style=text-align:center><strong>66.4</strong></td></tr><tr><td style=text-align:left>EvalPlus</td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>63.6</td><td style=text-align:center><strong>71.6</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>12.3</td><td style=text-align:center>-</td><td style=text-align:center>15.2</td><td style=text-align:center><strong>25.5</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>65.7</td><td style=text-align:center><strong>90.2</strong></td><td style=text-align:center>83.6</td><td style=text-align:center>79.6</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>30.7</td><td style=text-align:center><strong>50.1</strong></td><td style=text-align:center>42.4</td><td style=text-align:center>49.1</td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>76.7</td><td style=text-align:center>80.5</td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>5.70</td><td style=text-align:center>7.20</td><td style=text-align:center>7.19</td><td style=text-align:center><strong>7.36</strong></td></tr></tbody></table><h3 id=qwen2-7b-instruct>Qwen2-7B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-7b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-8B-Instruct</th><th style=text-align:center>Yi-1.5-9B-Chat</th><th style=text-align:center>GLM-4-9B-Chat</th><th style=text-align:center>Qwen1.5-7B-Chat</th><th style=text-align:center>Qwen2-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left><em><strong>English</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>68.4</td><td style=text-align:center>69.5</td><td style=text-align:center><strong>72.4</strong></td><td style=text-align:center>59.5</td><td style=text-align:center>70.5</td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>41.0</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>29.1</td><td style=text-align:center><strong>44.1</strong></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center><strong>34.2</strong></td><td style=text-align:center>-</td><td style=text-align:center><strong>-</strong></td><td style=text-align:center>27.8</td><td style=text-align:center>25.3</td></tr><tr><td style=text-align:left>TheroemQA</td><td style=text-align:center>23.0</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>14.1</td><td style=text-align:center><strong>25.3</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>8.05</td><td style=text-align:center>8.20</td><td style=text-align:center>8.35</td><td style=text-align:center>7.60</td><td style=text-align:center><strong>8.41</strong></td></tr><tr><td style=text-align:left><em><strong>Coding</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Humaneval</td><td style=text-align:center>62.2</td><td style=text-align:center>66.5</td><td style=text-align:center>71.8</td><td style=text-align:center>46.3</td><td style=text-align:center><strong>79.9</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center><strong>67.9</strong></td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>48.9</td><td style=text-align:center>67.2</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>48.5</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>27.2</td><td style=text-align:center><strong>59.1</strong></td></tr><tr><td style=text-align:left>Evalplus</td><td style=text-align:center>60.9</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>44.8</td><td style=text-align:center><strong>70.3</strong></td></tr><tr><td style=text-align:left>LiveCodeBench</td><td style=text-align:center>17.3</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>6.0</td><td style=text-align:center><strong>26.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>79.6</td><td style=text-align:center><strong>84.8</strong></td><td style=text-align:center>79.6</td><td style=text-align:center>60.3</td><td style=text-align:center>82.3</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>30.0</td><td style=text-align:center>47.7</td><td style=text-align:center><strong>50.6</strong></td><td style=text-align:center>23.2</td><td style=text-align:center>49.6</td></tr><tr><td style=text-align:left><em><strong>Chinese</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>45.9</td><td style=text-align:center>-</td><td style=text-align:center>75.6</td><td style=text-align:center>67.3</td><td style=text-align:center><strong>77.2</strong></td></tr><tr><td style=text-align:left>AlignBench</td><td style=text-align:center>6.20</td><td style=text-align:center>6.90</td><td style=text-align:center>7.01</td><td style=text-align:center>6.20</td><td style=text-align:center><strong>7.21</strong></td></tr></tbody></table><h3 id=qwen2-05b-instruct--qwen2-15b-instruct>Qwen2-0.5B-Instruct & Qwen2-1.5B-Instruct<a hidden class=anchor aria-hidden=true href=#qwen2-05b-instruct--qwen2-15b-instruct>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen1.5-0.5B-Chat</th><th style=text-align:center><strong>Qwen2-0.5B-Instruct</strong></th><th style=text-align:center>Qwen1.5-1.8B-Chat</th><th style=text-align:center><strong>Qwen2-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td style=text-align:left>MMLU</td><td style=text-align:center>35.0</td><td style=text-align:center><strong>37.9</strong></td><td style=text-align:center>43.7</td><td style=text-align:center><strong>52.4</strong></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>9.1</td><td style=text-align:center><strong>17.1</strong></td><td style=text-align:center>25.0</td><td style=text-align:center><strong>37.8</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>11.3</td><td style=text-align:center><strong>40.1</strong></td><td style=text-align:center>35.3</td><td style=text-align:center><strong>61.6</strong></td></tr><tr><td style=text-align:left>C-Eval</td><td style=text-align:center>37.2</td><td style=text-align:center><strong>45.2</strong></td><td style=text-align:center>55.3</td><td style=text-align:center><strong>63.8</strong></td></tr><tr><td style=text-align:left>IFEval (Prompt Strict-Acc.)</td><td style=text-align:center>14.6</td><td style=text-align:center><strong>20.0</strong></td><td style=text-align:center>16.8</td><td style=text-align:center><strong>29.0</strong></td></tr></tbody></table><h2 id=multilingual-capability-of-instruction-tuned-models>Multilingual capability of instruction-tuned models<a hidden class=anchor aria-hidden=true href=#multilingual-capability-of-instruction-tuned-models>#</a></h2><p>We compare Qwen2 instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:</p><ul><li><a href=https://github.com/nlp-uoregon/mlmm-evaluation>M-MMLU</a> from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh)</li><li><a href=https://arxiv.org/abs/2210.03057>MGSM</a>: math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn</li></ul><p>The results are averaged over languages for each benchmark and shown as follows:</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>M-MMLU (5-shot)</th><th style=text-align:center>MGSM (0-shot, CoT)</th></tr></thead><tbody><tr><td style=text-align:left><strong><em>Proprietary LLMs</em></strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>78.0</td><td style=text-align:center>87.0</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>79.3</td><td style=text-align:center>90.5</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>83.2</td><td style=text-align:center>89.6</td></tr><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>80.1</td><td style=text-align:center>91.0</td></tr><tr><td style=text-align:left>Claude-3-Sonnet-20240229</td><td style=text-align:center>71.0</td><td style=text-align:center>85.6</td></tr><tr><td style=text-align:left><strong><em>Open-source LLMs</em></strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>command-r-plus-110b</td><td style=text-align:center>65.5</td><td style=text-align:center>63.5</td></tr><tr><td style=text-align:left>Qwen1.5-7B-Chat</td><td style=text-align:center>50.0</td><td style=text-align:center>37.0</td></tr><tr><td style=text-align:left>Qwen1.5-32B-Chat</td><td style=text-align:center>65.0</td><td style=text-align:center>65.0</td></tr><tr><td style=text-align:left>Qwen1.5-72B-Chat</td><td style=text-align:center>68.4</td><td style=text-align:center>71.7</td></tr><tr><td style=text-align:left><strong>Qwen2-7B-Instruct</strong></td><td style=text-align:center><strong>60.0</strong></td><td style=text-align:center><strong>57.0</strong></td></tr><tr><td style=text-align:left><strong>Qwen2-57B-A14B-Instruct</strong></td><td style=text-align:center><strong>68.0</strong></td><td style=text-align:center><strong>74.0</strong></td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center><strong>78.0</strong></td><td style=text-align:center><strong>86.6</strong></td></tr></tbody></table><p>For human evaluation, we compare Qwen2-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>ar</th><th style=text-align:center>es</th><th style=text-align:center>fr</th><th style=text-align:center>ko</th><th style=text-align:center>th</th><th style=text-align:center>vi</th><th style=text-align:center>pt</th><th style=text-align:center>id</th><th style=text-align:center>ja</th><th style=text-align:center>ru</th><th style=text-align:center>Average</th></tr></thead><tbody><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>4.15</td><td style=text-align:center>4.31</td><td style=text-align:center>4.23</td><td style=text-align:center>4.23</td><td style=text-align:center>4.01</td><td style=text-align:center>3.98</td><td style=text-align:center>4.09</td><td style=text-align:center>4.40</td><td style=text-align:center>3.85</td><td style=text-align:center>4.25</td><td style=text-align:center>4.15</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>3.55</td><td style=text-align:center>4.26</td><td style=text-align:center>4.16</td><td style=text-align:center>4.40</td><td style=text-align:center>4.09</td><td style=text-align:center>4.14</td><td style=text-align:center>3.89</td><td style=text-align:center>4.39</td><td style=text-align:center>3.72</td><td style=text-align:center>4.32</td><td style=text-align:center>4.09</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>3.44</td><td style=text-align:center>4.08</td><td style=text-align:center>4.19</td><td style=text-align:center>4.24</td><td style=text-align:center>4.11</td><td style=text-align:center>3.84</td><td style=text-align:center>3.86</td><td style=text-align:center>4.09</td><td style=text-align:center>3.68</td><td style=text-align:center>4.27</td><td style=text-align:center>3.98</td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center>3.86</td><td style=text-align:center>4.10</td><td style=text-align:center>4.01</td><td style=text-align:center>4.14</td><td style=text-align:center>3.75</td><td style=text-align:center>3.91</td><td style=text-align:center>3.97</td><td style=text-align:center>3.83</td><td style=text-align:center>3.63</td><td style=text-align:center>4.15</td><td style=text-align:center>3.93</td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>3.55</td><td style=text-align:center>3.92</td><td style=text-align:center>3.94</td><td style=text-align:center>3.87</td><td style=text-align:center>3.83</td><td style=text-align:center>3.95</td><td style=text-align:center>3.55</td><td style=text-align:center>3.77</td><td style=text-align:center>3.06</td><td style=text-align:center>3.63</td><td style=text-align:center>3.71</td></tr><tr><td style=text-align:left>GPT-3.5-Turbo-1106</td><td style=text-align:center>2.52</td><td style=text-align:center>4.07</td><td style=text-align:center>3.47</td><td style=text-align:center>2.37</td><td style=text-align:center>3.38</td><td style=text-align:center>2.90</td><td style=text-align:center>3.37</td><td style=text-align:center>3.56</td><td style=text-align:center>2.75</td><td style=text-align:center>3.24</td><td style=text-align:center>3.16</td></tr></tbody></table><p>Grouped by task types, the results are shown as follows:</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Knowledge</th><th style=text-align:center>Understanding</th><th style=text-align:center>Creation</th><th style=text-align:center>Math</th></tr></thead><tbody><tr><td style=text-align:left>Claude-3-Opus-20240229</td><td style=text-align:center>3.64</td><td style=text-align:center>4.45</td><td style=text-align:center>4.42</td><td style=text-align:center>3.81</td></tr><tr><td style=text-align:left>GPT-4o-0513</td><td style=text-align:center>3.76</td><td style=text-align:center>4.35</td><td style=text-align:center>4.45</td><td style=text-align:center>3.53</td></tr><tr><td style=text-align:left>GPT-4-Turbo-0409</td><td style=text-align:center>3.42</td><td style=text-align:center>4.29</td><td style=text-align:center>4.35</td><td style=text-align:center>3.58</td></tr><tr><td style=text-align:left><strong>Qwen2-72B-Instruct</strong></td><td style=text-align:center>3.41</td><td style=text-align:center>4.07</td><td style=text-align:center>4.36</td><td style=text-align:center>3.61</td></tr><tr><td style=text-align:left>GPT-4-0613</td><td style=text-align:center>3.42</td><td style=text-align:center>4.09</td><td style=text-align:center>4.10</td><td style=text-align:center>3.32</td></tr><tr><td style=text-align:left>GPT-3.5-Turbo-1106</td><td style=text-align:center>3.37</td><td style=text-align:center>3.67</td><td style=text-align:center>3.89</td><td style=text-align:center>2.97</td></tr></tbody></table><p>These results demonstrate the strong multilingual capabilities of Qwen2 instruction-tuned models.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Update on 2024-07-16: The results of instruction-tuned models may differ from those presented in the technical report; in case of any discrepancy, the results documented in the technical report should take precedence.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>